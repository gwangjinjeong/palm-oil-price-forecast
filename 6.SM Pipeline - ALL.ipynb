{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58d54f1-ccf2-445f-96f0-256d76fbd162",
   "metadata": {},
   "source": [
    "# SageMaker 모델 빌드 파이프라인을 이용한 모델 빌드 오케스트레이션\n",
    "Amazon SageMaker Model building pipeline은 머신러닝 워크플로우를 개발하는 데이터 과학자, 엔지니어들에게 SageMaker작업과 재생산가능한 머신러닝 파이프라인을 오케스트레이션하는 기능을 제공합니다. 또한 커스텀빌드된 모델을 실시간 추론환경이나 배치변환을 통한 추론 실행환경으로 배포하거나, 생성된 아티팩트의 계보(lineage)를 추적하는 기능을 제공합니다. 이 기능들을 통해 모델 아티팩트를 배포하고, 업무환경에서의 워크플로우를 배포/모니터링하고, 간단한 인터페이스를 통해 아티팩트의 계보 추적하고, 머신러닝 애플리케이션 개발의 베스트 프렉티스를 도입하여, 보다 안정적인 머신러닝 애플리케이션 운영환경을 구현할 수 있습니다.\n",
    "\n",
    "SageMaker pipeline 서비스는 JSON 선언으로 구현된 SageMaker Pipeline DSL(Domain Specific Language, 도메인종속언어)를 지원합니다. 이 DSL은 파이프라인 파라마터와 SageMaker 작업단계의 DAG(Directed Acyclic Graph)를 정의합니다. SageMaker Python SDK를 이용하면 이 파이프라인 DSL의 생성을 보다 간편하게 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc761a70-090c-416d-a0f6-1b7847bc2314",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. 사용 코드들 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e04e20-2211-4506-842f-1eb694f83a48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca544d62-143c-4523-a2b6-263294caf00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY=','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCHI','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'FKLIc1', 'FKLIc2', 'FKLIc3','FTSE','GCc1', 'GCc2', 'GCc3','GDAXI','GENMKL','HSI','IOIBKL','IXIC','JNIc1','JNIc2','JNIc3','KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2', 'MCCc3','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL','SSEC', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  ric,\n",
    "                  s3_resource,\n",
    "                  BUCKET_NAME_USECASE,\n",
    "                  S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(scaler, fp)\n",
    "        fp.seek(0)\n",
    "        s3_resource.put_object(Body = fp.read(),\n",
    "                               Bucket = BUCKET_NAME_USECASE,\n",
    "                               Key = f\"{S3_PATH_GOLDEN}/{KST.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    split_date_default = dt.today() + relativedelta(hours = 9) - relativedelta(months=1)\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--split_date', type=str, default=split_date_default.strftime('%Y-%m-%d'))       \n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = 1, help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "        \n",
    "    # parse arguments\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_date: {args.split_date}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")   \n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_date = args.split_date    \n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    stage_dir = f'{base_output_dir}/stage/stage.csv\"'\n",
    "    logger.info(f\"\\n### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    for file in path_list:\n",
    "        df_tmp= pd.read_csv(file, encoding='utf-8') \n",
    "        df_sum = pd.concat([df_sum, df_tmp])\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{base_output_dir}/stage/stage.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    \n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= '2014-07-02'].reset_index(drop = True)\n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if int(scaler_switch) == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value, col, name, s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    \n",
    "    # train 데이터 나누기\n",
    "    df_train = df_golden[df_golden['ds'] < split_date]\n",
    "    df_train.to_csv(f\"{base_output_dir}/train/train.csv\", index = False)\n",
    "    \n",
    "    df_test = df_golden[df_golden['ds'] >= split_date]\n",
    "    df_test.to_csv(f\"{base_output_dir}/test/test.csv\", index = False)\n",
    "    \n",
    "    logger.info(f\"\\n ### Final result for train dataset \")\n",
    "    logger.info(f\"\\n ####preprocessed train shape \\n {df_train.shape}\")        \n",
    "    logger.info(f\"preprocessed train sample: head(2) \\n {df_train.head(2)}\")\n",
    "    logger.info(f\"preprocessed train sample: tail(2) \\n {df_train.tail(2)}\")\n",
    "    \n",
    "    logger.info(f\"\\n ####preprocessed test shape \\n {df_test.shape}\")            \n",
    "    logger.info(f\"preprocessed test sample: head(2) \\n {df_test.head(2)}\")\n",
    "    logger.info(f\"preprocessed test sample: tail(2) \\n {df_test.tail(2)}\")\n",
    "\n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eefb11-6bb6-4e1b-8db1-8e59f2ffa528",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-2. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe56ec5-af8e-44d1-a2d9-06438a136ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import joblib # from sklearn.externals import joblib\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime as dt\n",
    "\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ###################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ###################################        \n",
    "\n",
    "    logger.info(f\"### start training code\")    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output_dir', type = str, default = os.environ.get('SM_OUTPUT_DIR'))\n",
    "    parser.add_argument('--output_data_dir', type = str, default = os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type = str, default = os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train_dir', type = str, default = os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test_dir', type = str, default = os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--item', type = str, default = 'FCPOc3')\n",
    "    parser.add_argument('--target', type = str, default = 'y')\n",
    "    parser.add_argument('--metric', type = str, default = 'MAPE')    \n",
    "    parser.add_argument('--quality', type = str, default = 'low_quality')\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.output_data_dir: {args.output_data_dir}\")    \n",
    "    logger.info(f\"args.model_dir: {args.model_dir}\")        \n",
    "    logger.info(f\"args.train_dir: {args.train_dir}\")   \n",
    "    logger.info(f\"args.test_dir: {args.test_dir}\")   \n",
    "    logger.info(f\"args.item: {args.item}\")   \n",
    "    logger.info(f\"args.target: {args.target}\")    \n",
    "    logger.info(f\"args.metric: {args.metric}\")   \n",
    "    logger.info(f\"args.quality: {args.quality}\")   \n",
    "    \n",
    "    output_dir = args.output_dir\n",
    "    output_data_dir = args.output_data_dir\n",
    "    model_dir = args.model_dir\n",
    "    train_dir = args.train_dir\n",
    "    test_dir = args.test_dir\n",
    "    item = args.item\n",
    "    target = args.target\n",
    "    metric = args.metric\n",
    "    quality = args.quality\n",
    "    \n",
    "    logger.info(\"### Reading input data\")\n",
    "    df_train= pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(test_dir, 'test.csv'))        \n",
    "    \n",
    "    logger.info(\"### Convert TimeSeriesDataFrame\")\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"### Show the range of date for training and test\")    \n",
    "    logger.info('Item:\\t', item)\n",
    "    logger.info('Target:\\t', target)   \n",
    "    logger.info('Train:\\t',tdf_train.loc[item][target].index.min(),'~',tdf_train.loc[item][target].index.max())\n",
    "    logger.info('Test:\\t',tdf_test.loc[item][target].index.min(),'~',tdf_test.loc[item][target].index.max())\n",
    "    logger.info('The number of test data:',len(tdf_test.loc[item][target]))\n",
    "    \n",
    "    logger.info(\"### Training AutoGluon Model\")    \n",
    "    predictor = TimeSeriesPredictor(\n",
    "        path = model_dir,\n",
    "        target = target,\n",
    "        prediction_length = len(tdf_test.loc[item][target]),\n",
    "        eval_metric = metric,\n",
    "    )\n",
    "    predictor.fit(\n",
    "        train_data = tdf_train,\n",
    "        presets = quality\n",
    "    )    \n",
    "    logger.info(\"Saving model to {}\".format(model_dir))\n",
    "    \n",
    "    # 원래라면 Validation dataset이 input으로 들어와서 leaderboard와 prediction을 해야한다.\n",
    "    # 근데, 여기서는 아니다. 이번 사이클에서는 test data까지 모두 산출한다음에 넣는것으로 진행하자.\n",
    "    predictor_leaderboard = predictor.leaderboard(tdf_test, silent = True)\n",
    "    predictor_leaderboard.to_csv(os.path.join(output_data_dir,'leaderboard.csv'), index = False)\n",
    "    \n",
    "    predictions = predictor.predict(train_data)\n",
    "    predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69914f-0aef-4d13-8d6b-5d1ef94aa75c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-3. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de56531e-8546-4a3a-8890-c6e6495354b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/model_validation.py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import argparse\n",
    "import json\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import joblib\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import tarfile\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "def convert_series_to_description(leaderboard : pd.Series):\n",
    "    return ','.join(leaderboard.loc[0,['model','score_test','score_val']].to_string().split())\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_path', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--s3_model_uri', type=str, default=\"/opt/ml/processing/model\")   \n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_path: {args.base_input_path}\")\n",
    "    logger.info(f\"args.s3_model_uri: {args.s3_model_uri}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "    \n",
    "    base_input_path = args.base_input_path\n",
    "    s3_model_uri = args.s3_model_uri\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ############################################\n",
    "    ##### Model, Leaderboard 파일 가져오기 #####\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Model, Leaderboard zip files \")\n",
    "    logger.info(f\"\\n#### Extract output.tar.gz and Read a Leaderboard \")\n",
    "    ## 22.11.29 추가: 이전 step인, step_train에서 model.tar.gz의 uri는 가져올 수 있었지만, output.tar.gz는 못가져왔다. 이를 model.tar.gz에서 output.tar.gz으로 바꾸는방식으로 우회하자\n",
    "    leaderboard_uri = s3_model_uri.replace('model.tar.gz','output.tar.gz')#,f'{base_input_path}/output.tar.gz'\n",
    "    logger.info(f\"\\n#### output.tar.gz uri : {leaderboard_uri}\")\n",
    "    output_bucket, output_key = get_bucket_key_from_uri(leaderboard_uri)  \n",
    "    output_obj = s3_client.get_object(Bucket = output_bucket, Key = output_key)\n",
    "   \n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=output_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(base_input_path)    \n",
    "    logger.info(f\"file list in {base_input_path}: {os.listdir(base_input_path)}\")        \n",
    "    \n",
    "    # if leaderboard_path.endswith(\"tar.gz\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:gz\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "    # elif leaderboard_path.endswith(\"tar\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "\n",
    "    leaderboard = pd.read_csv(f'{base_input_path}/leaderboard.csv').sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                                ascending = False)\n",
    "    logger.info(f\"leaderboard train sample: head(5) \\n {leaderboard.head()}\")\n",
    "    logger.info(f\"\\n#### Set  \")\n",
    "    model_package_group_name = model_package_group_name\n",
    "    modelpackage_inference_specification =  {\n",
    "        \"InferenceSpecification\": {\n",
    "            \"Containers\": [\n",
    "                {\n",
    "                    \"Image\": '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/autogluon-inference:0.4-cpu-py38',\n",
    "                    \"ModelDataUrl\": s3_model_uri#'#args.model_path_uri\n",
    "                }\n",
    "            ],\n",
    "            \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "            \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "        }\n",
    "    }\n",
    "    if len(leaderboard[leaderboard['score_val'] > -0.13]) > 0:\n",
    "        logger.info(f\"\\n#### Pass the first performance filtering\")\n",
    "        \n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Passed ModelPackage Version ARN : {}'.format(model_package_arn))\n",
    "        \n",
    "    else:\n",
    "        logger.info(f\"\\n#### None of them passed the filtering\")\n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"Rejected\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Rejected ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491c493-b830-47df-8afd-8585041f4bba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-4. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1224ccca-32e7-4b42-a3e1-e2e3ed6fb6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/prediction.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'autogluon==0.6.0'])\n",
    "    from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_dir', type=str, default=\"/opt/ml/processing/input\", help='train,testset 불러오는곳')\n",
    "    parser.add_argument('--output_dir', type = str, default = \"/opt/ml/processing/output\", help='예측 결과값이 저장되는 곳, test dataset과 prediction 결과가 merge되서 저장된다.')\n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "    logger.info(\"\\n######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_dir: {args.base_input_dir}\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "\n",
    "    base_input_dir = args.base_input_dir\n",
    "    output_dir = args.output_dir\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    model_dir = '/opt/ml/model'\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 적합한 모델의 URI 찾고, 탑 성능 모델 이름 가져오기 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Finding suitable model uri ####################################\")\n",
    "    logger.info(f\"Model Group name: {model_package_group_name}\")\n",
    "    model_registry_list = sm_client.list_model_packages(ModelPackageGroupName = model_package_group_name)['ModelPackageSummaryList']\n",
    "    for model in model_registry_list:\n",
    "        if (model['ModelPackageGroupName'] == model_package_group_name and\n",
    "            model['ModelApprovalStatus'] == 'Approved'):\n",
    "            mr_arn = model['ModelPackageArn']\n",
    "            break\n",
    "    describe_model = sm_client.describe_model_package(ModelPackageName=mr_arn)\n",
    "    s3_model_uri = describe_model['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "    top_model_name = describe_model['ModelPackageDescription'].split(',')[1]\n",
    "\n",
    "    logger.info(f\"Found suitable model uri: {s3_model_uri}\")\n",
    "    logger.info(f\"And top model name: {top_model_name}\")\n",
    "    \n",
    "    logger.info(\"\\n#########Download suitable model file  ####################################\")\n",
    "    model_bucket, model_key = get_bucket_key_from_uri(s3_model_uri)  \n",
    "    model_obj = s3_client.get_object(Bucket = model_bucket, Key = model_key)\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 모델 압축 풀고 TimeseriesDataFrame으로 변환 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=model_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(model_dir)    \n",
    "    logger.info(f\"list in /opt/ml/model: {os.listdir(model_dir)}\")        \n",
    "    \n",
    "    logger.info(\"\\n######### Convert df_test dataframe into TimeSeriesDataFrame  ###########\")        \n",
    "    df_train = pd.read_csv(os.path.join(f'{base_input_dir}/train/train.csv'))\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    df_test = pd.read_csv(f\"{base_input_dir}/test/test.csv\")\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    logger.info(f\"df_test sample: tail(2) \\n {tdf_train.tail(2)}\")\n",
    "    logger.info(f\"df_test sample: head(2) \\n {tdf_test.head(2)}\")\n",
    "    \n",
    "    ################################\n",
    "    ###### Prediction 시작 ##########\n",
    "    ###############################\n",
    "    logger.info(\"\\n######### Start prediction  ###########\")        \n",
    "    loaded_trainer = pickle.load(open(f\"{model_dir}/models/trainer.pkl\", 'rb'))\n",
    "    logger.info(f\"loaded_trainer: {loaded_trainer}\")\n",
    "    prediction_ag_model = loaded_trainer.predict(data = tdf_train,\n",
    "                                                 model = top_model_name)\n",
    "    logger.info(f\"prediction_ag_model sample: head(2) \\n {prediction_ag_model.head(2)}\")\n",
    "\n",
    "    prediction_result = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model.loc['FCPOc3'],\n",
    "                                 left_index = True, right_index = True, how = 'left')\n",
    "    prediction_result.to_csv(f'{output_dir}/prediction_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0174ac3-8374-41a2-b29c-6ed0e36a41bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. 사용 코드들 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e809d-1584-4a57-a69a-ecb2182063fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a076e479-a18b-4a8a-baca-6e3fbc67c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY=','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCHI','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'FKLIc1', 'FKLIc2', 'FKLIc3','FTSE','GCc1', 'GCc2', 'GCc3','GDAXI','GENMKL','HSI','IOIBKL','IXIC','JNIc1','JNIc2','JNIc3','KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2', 'MCCc3','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL','SSEC', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  ric,\n",
    "                  s3_resource,\n",
    "                  BUCKET_NAME_USECASE,\n",
    "                  S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(scaler, fp)\n",
    "        fp.seek(0)\n",
    "        s3_resource.put_object(Body = fp.read(),\n",
    "                               Bucket = BUCKET_NAME_USECASE,\n",
    "                               Key = f\"{S3_PATH_GOLDEN}/{KST.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--split_date', type=str, default=KST.strftime('%Y-%m-%d'))\n",
    "    parser.add_argument('--num_fold', type=str, default='5')       \n",
    "\n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = 1, help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "        \n",
    "    # parse arguments\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_date: {args.split_date}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")\n",
    "    logger.info(f\"args.num_fold: {args.num_fold}\")\n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_date = args.split_date\n",
    "    num_fold = int(args.num_fold)\n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    stage_dir = f'{base_output_dir}/stage'\n",
    "    logger.info(f\"\\n### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    for file in path_list:\n",
    "        df_tmp= pd.read_csv(file, encoding='utf-8') \n",
    "        df_sum = pd.concat([df_sum, df_tmp])\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{stage_dir}/stage_integrated.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    \n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    \n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= '2014-07-02'].reset_index(drop = True)\n",
    "        \n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if int(scaler_switch) == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value, col, name, s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "        \n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    \n",
    "    df_train_fold0 = df_golden[df_golden['ds'] < split_date]\n",
    "    df_train_fold0.to_csv(f\"{base_output_dir}/train/train_fold1.csv\", index = False)\n",
    "    df_test_fold1 = df_golden[df_golden['ds'] >= split_date]\n",
    "    df_test_fold1.to_csv(f\"{base_output_dir}/test/test_fold1.csv\", index = False)\n",
    "\n",
    "    # train 데이터 나누기\n",
    "    for cnt in range(num_fold):\n",
    "        split_date = (dt.strptime(split_date, '%Y-%m-%d') - relativedelta(days=30)).strftime('%Y-%m-%d')\n",
    "    \n",
    "        logger.info(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < {split_date}]\")\n",
    "        exec(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < split_date]\")\n",
    "        exec(f\"df_train_fold{cnt+1}.to_csv('{base_output_dir}/train/train_fold{cnt+1}.csv', index = False)\")\n",
    "\n",
    "        logger.info(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= {split_date}]\")\n",
    "        exec(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= split_date]\")\n",
    "        exec(f\"df_test_fold{cnt+1}.to_csv('{base_output_dir}/test/test_fold{cnt+1}.csv', index = False)\")\n",
    "    \n",
    "    logger.info(f\"\\n ### Final result for train dataset \")\n",
    "    logger.info(f\"\\n ####preprocessed train shape \\n {df_train.shape}\")        \n",
    "    logger.info(f\"preprocessed train sample: head(2) \\n {df_train.head(2)}\")\n",
    "    logger.info(f\"preprocessed train sample: tail(2) \\n {df_train.tail(2)}\")\n",
    "    \n",
    "    logger.info(f\"\\n ####preprocessed test shape \\n {df_test.shape}\")            \n",
    "    logger.info(f\"preprocessed test sample: head(2) \\n {df_test.head(2)}\")\n",
    "    logger.info(f\"preprocessed test sample: tail(2) \\n {df_test.tail(2)}\")\n",
    "\n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae82de-9794-4955-9e71-fbeedc0a76ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-2. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fcadea-8ce1-48ca-a12f-3ce989c77772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import joblib # from sklearn.externals import joblib\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime as dt\n",
    "\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ###################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ###################################        \n",
    "\n",
    "    logger.info(f\"### start training code\")    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output_dir', type = str, default = os.environ.get('SM_OUTPUT_DIR'))\n",
    "    parser.add_argument('--output_data_dir', type = str, default = os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type = str, default = os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train_dir', type = str, default = os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test_dir', type = str, default = os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--item', type = str, default = 'FCPOc3')\n",
    "    parser.add_argument('--target', type = str, default = 'y')\n",
    "    parser.add_argument('--metric', type = str, default = 'MAPE')    \n",
    "    parser.add_argument('--quality', type = str, default = 'low_quality')\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.output_data_dir: {args.output_data_dir}\")    \n",
    "    logger.info(f\"args.model_dir: {args.model_dir}\")        \n",
    "    logger.info(f\"args.train_dir: {args.train_dir}\")   \n",
    "    logger.info(f\"args.test_dir: {args.test_dir}\")   \n",
    "    logger.info(f\"args.item: {args.item}\")   \n",
    "    logger.info(f\"args.target: {args.target}\")    \n",
    "    logger.info(f\"args.metric: {args.metric}\")   \n",
    "    logger.info(f\"args.quality: {args.quality}\")   \n",
    "    \n",
    "    output_dir = args.output_dir\n",
    "    output_data_dir = args.output_data_dir\n",
    "    model_dir = args.model_dir\n",
    "    train_dir = args.train_dir\n",
    "    test_dir = args.test_dir\n",
    "    item = args.item\n",
    "    target = args.target\n",
    "    metric = args.metric\n",
    "    quality = args.quality\n",
    "    \n",
    "    logger.info(\"### Reading input data\")\n",
    "    df_train= pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(test_dir, 'test.csv'))        \n",
    "    \n",
    "    logger.info(\"### Convert TimeSeriesDataFrame\")\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"### Show the range of date for training and test\")    \n",
    "    logger.info('Item:\\t', item)\n",
    "    logger.info('Target:\\t', target)   \n",
    "    logger.info('Train:\\t',tdf_train.loc[item][target].index.min(),'~',tdf_train.loc[item][target].index.max())\n",
    "    logger.info('Test:\\t',tdf_test.loc[item][target].index.min(),'~',tdf_test.loc[item][target].index.max())\n",
    "    logger.info('The number of test data:',len(tdf_test.loc[item][target]))\n",
    "    \n",
    "    logger.info(\"### Training AutoGluon Model\")    \n",
    "    predictor = TimeSeriesPredictor(\n",
    "        path = model_dir,\n",
    "        target = target,\n",
    "        prediction_length = len(tdf_test.loc[item][target]),\n",
    "        eval_metric = metric,\n",
    "    )\n",
    "    predictor.fit(\n",
    "        train_data = tdf_train,\n",
    "        presets = quality\n",
    "    )    \n",
    "    logger.info(\"Saving model to {}\".format(model_dir))\n",
    "    \n",
    "    # 원래라면 Validation dataset이 input으로 들어와서 leaderboard와 prediction을 해야한다.\n",
    "    # 근데, 여기서는 아니다. 이번 사이클에서는 test data까지 모두 산출한다음에 넣는것으로 진행하자.\n",
    "    predictor_leaderboard = predictor.leaderboard(tdf_test, silent = True)\n",
    "    predictor_leaderboard.to_csv(os.path.join(output_data_dir,'leaderboard.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b38b7-f505-4656-9efa-fa9296e6b459",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-3. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bb17db-627e-4d76-b576-8b6869385867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/model_validation.py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import argparse\n",
    "import json\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import joblib\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import tarfile\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "def convert_series_to_description(leaderboard : pd.Series):\n",
    "    return ','.join(leaderboard.loc[0,['model','score_test','score_val']].to_string().split())\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_path', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--s3_model_uri', type=str, default=\"/opt/ml/processing/model\")   \n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_path: {args.base_input_path}\")\n",
    "    logger.info(f\"args.s3_model_uri: {args.s3_model_uri}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "    \n",
    "    base_input_path = args.base_input_path\n",
    "    s3_model_uri = args.s3_model_uri\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ############################################\n",
    "    ##### Model, Leaderboard 파일 가져오기 #####\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Model, Leaderboard zip files \")\n",
    "    logger.info(f\"\\n#### Extract output.tar.gz and Read a Leaderboard \")\n",
    "    ## 22.11.29 추가: 이전 step인, step_train에서 model.tar.gz의 uri는 가져올 수 있었지만, output.tar.gz는 못가져왔다. 이를 model.tar.gz에서 output.tar.gz으로 바꾸는방식으로 우회하자\n",
    "    leaderboard_uri = s3_model_uri.replace('model.tar.gz','output.tar.gz')#,f'{base_input_path}/output.tar.gz'\n",
    "    logger.info(f\"\\n#### output.tar.gz uri : {leaderboard_uri}\")\n",
    "    output_bucket, output_key = get_bucket_key_from_uri(leaderboard_uri)  \n",
    "    output_obj = s3_client.get_object(Bucket = output_bucket, Key = output_key)\n",
    "   \n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=output_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(base_input_path)    \n",
    "    logger.info(f\"file list in {base_input_path}: {os.listdir(base_input_path)}\")        \n",
    "    \n",
    "    # if leaderboard_path.endswith(\"tar.gz\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:gz\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "    # elif leaderboard_path.endswith(\"tar\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "\n",
    "    leaderboard = pd.read_csv(f'{base_input_path}/leaderboard.csv').sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                                ascending = False)\n",
    "    logger.info(f\"leaderboard train sample: head(5) \\n {leaderboard.head()}\")\n",
    "    logger.info(f\"\\n#### Set  \")\n",
    "    model_package_group_name = model_package_group_name\n",
    "    modelpackage_inference_specification =  {\n",
    "        \"InferenceSpecification\": {\n",
    "            \"Containers\": [\n",
    "                {\n",
    "                    \"Image\": '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/autogluon-inference:0.4-cpu-py38',\n",
    "                    \"ModelDataUrl\": s3_model_uri#'#args.model_path_uri\n",
    "                }\n",
    "            ],\n",
    "            \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "            \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "        }\n",
    "    }\n",
    "    if len(leaderboard[leaderboard['score_val'] > -0.13]) > 0:\n",
    "        logger.info(f\"\\n#### Pass the first performance filtering\")\n",
    "        \n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Passed ModelPackage Version ARN : {}'.format(model_package_arn))\n",
    "        \n",
    "    else:\n",
    "        logger.info(f\"\\n#### None of them passed the filtering\")\n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"Rejected\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Rejected ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbc6dd-ac0d-4ee6-b307-a55378d5ba63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-4. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd60cfd8-e444-46e8-87f0-f44ad9d2af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/prediction.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'autogluon==0.6.0'])\n",
    "    from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_dir', type=str, default=\"/opt/ml/processing/input\", help='train,testset 불러오는곳')\n",
    "    parser.add_argument('--output_dir', type = str, default = \"/opt/ml/processing/output\", help='예측 결과값이 저장되는 곳, test dataset과 prediction 결과가 merge되서 저장된다.')\n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "    logger.info(\"\\n######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_dir: {args.base_input_dir}\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "\n",
    "    base_input_dir = args.base_input_dir\n",
    "    output_dir = args.output_dir\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    model_dir = '/opt/ml/model'\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 적합한 모델의 URI 찾고, 탑 성능 모델 이름 가져오기 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Finding suitable model uri ####################################\")\n",
    "    logger.info(f\"Model Group name: {model_package_group_name}\")\n",
    "    model_registry_list = sm_client.list_model_packages(ModelPackageGroupName = model_package_group_name)['ModelPackageSummaryList']\n",
    "    for model in model_registry_list:\n",
    "        if (model['ModelPackageGroupName'] == model_package_group_name and\n",
    "            model['ModelApprovalStatus'] == 'Approved'):\n",
    "            mr_arn = model['ModelPackageArn']\n",
    "            break\n",
    "    describe_model = sm_client.describe_model_package(ModelPackageName=mr_arn)\n",
    "    s3_model_uri = describe_model['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "    top_model_name = describe_model['ModelPackageDescription'].split(',')[1]\n",
    "\n",
    "    logger.info(f\"Found suitable model uri: {s3_model_uri}\")\n",
    "    logger.info(f\"And top model name: {top_model_name}\")\n",
    "    \n",
    "    logger.info(\"\\n#########Download suitable model file  ####################################\")\n",
    "    model_bucket, model_key = get_bucket_key_from_uri(s3_model_uri)  \n",
    "    model_obj = s3_client.get_object(Bucket = model_bucket, Key = model_key)\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 모델 압축 풀고 TimeseriesDataFrame으로 변환 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=model_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(model_dir)    \n",
    "    logger.info(f\"list in /opt/ml/model: {os.listdir(model_dir)}\")        \n",
    "    \n",
    "    logger.info(\"\\n######### Convert df_test dataframe into TimeSeriesDataFrame  ###########\")        \n",
    "    df_train = pd.read_csv(os.path.join(f'{base_input_dir}/train/train.csv'))\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    df_test = pd.read_csv(f\"{base_input_dir}/test/test.csv\")\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    logger.info(f\"df_test sample: tail(2) \\n {tdf_train.tail(2)}\")\n",
    "    logger.info(f\"df_test sample: head(2) \\n {tdf_test.head(2)}\")\n",
    "    \n",
    "    ################################\n",
    "    ###### Prediction 시작 ##########\n",
    "    ###############################\n",
    "    logger.info(\"\\n######### Start prediction  ###########\")        \n",
    "    loaded_trainer = pickle.load(open(f\"{model_dir}/models/trainer.pkl\", 'rb'))\n",
    "    logger.info(f\"loaded_trainer: {loaded_trainer}\")\n",
    "    prediction_ag_model = loaded_trainer.predict(data = tdf_train,\n",
    "                                                 model = top_model_name)\n",
    "    logger.info(f\"prediction_ag_model sample: head(2) \\n {prediction_ag_model.head(2)}\")\n",
    "\n",
    "    prediction_result = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model.loc['FCPOc3'],\n",
    "                                 left_index = True, right_index = True, how = 'left')\n",
    "    prediction_result.to_csv(f'{output_dir}/prediction_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43ea47-dcc7-4973-8303-956c65f5f7c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce81ec-fdd6-41b0-8a82-2465c664cc31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.1 라이브러리 및 변수 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d65ec2-afc2-4954-ad35-7f59fd097b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import sagemaker\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be1cb17-7559-4e20-b593-7f850b3daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2022-12-20 21:00:44.486344\n"
     ]
    }
   ],
   "source": [
    "# 한국 시간\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "print(f\"Start job time: {KST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017b0400-5458-4c22-86b2-dd494f5a215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code version: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 코드 버전\n",
    "code_version = '1.0'\n",
    "print(f\"Code version: {code_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c32c0e4-17bb-44f1-944a-c78ade63795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "keychain = json.loads(get_secret())\n",
    "ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "S3_PATH_FORECAST = keychain['S3_PATH_FORECAST']\n",
    "\n",
    "boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "sm_session = sagemaker.Session(boto_session = boto3_session)\n",
    "region = boto3_session.region_name\n",
    "\n",
    "s3_resource = boto3_session.resource('s3')\n",
    "bucket = s3_resource.Bucket(BUCKET_NAME_USECASE)\n",
    "s3_client = boto3_session.client('s3')\n",
    "sm_client = boto3.client('sagemaker',\n",
    "                         aws_access_key_id = ACCESS_KEY_ID,\n",
    "                         aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                         region_name = 'ap-northeast-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231675a-a0e4-49ad-bb2c-3bf40ccff01d",
   "metadata": {},
   "source": [
    "노트북에 저장된 변수를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11129c21-3790-4c21-94f5-4f3e424a6258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimator_output_path:  s3://palm-oil-price-forecast/trained-model/2022/12/20\n",
      "prediction_output_path:  s3://palm-oil-price-forecast/forecasted-data/2022/12/20\n"
     ]
    }
   ],
   "source": [
    "estimator_output_path = f\"s3://{BUCKET_NAME_USECASE}/{S3_PATH_TRAIN}/{KST.strftime('%Y/%m/%d')}\"\n",
    "prediction_output_path = f\"s3://{BUCKET_NAME_USECASE}/{S3_PATH_FORECAST}/{KST.strftime('%Y/%m/%d')}\"\n",
    "print(\"estimator_output_path: \", estimator_output_path)\n",
    "print(\"prediction_output_path: \", prediction_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab711bf3-438e-4921-8131-5b845694f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_data_uri = f\"{base}/{keychain['S3_PATH_STAGE']}\"\n",
    "# train_data_uri = f\"{preproc_data_dir}/train.csv\"\n",
    "# test_data_uri = f\"{preproc_data_dir}/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06536e11-9548-4803-b87a-a5ae93dbc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store stage_data_uri\n",
    "# %store train_data_uri\n",
    "# %store test_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "068027ba-2abc-4cf9-b9bb-904c82966116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stage_data_uri)\n",
    "# print(train_data_uri)\n",
    "# print(test_data_uri)\n",
    "# print(train_model_uri)\n",
    "# print(leaderboard_uri)\n",
    "# print(project_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3af0d45-e6f2-40f8-9422-797c35f67618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 한국 시간\n",
    "# KST = dt.today() + relativedelta(hours=9)\n",
    "# print(f\"Start job time: {KST}\")\n",
    "# # 프로젝트 변수\n",
    "# project_prefix = bucket\n",
    "# base = f\"s3://{bucket}\"\n",
    "\n",
    "# # 전처리 결과 데이터 위치(Golden data path)\n",
    "# preproc_data_dir = f\"{base}/{keychain['S3_PATH_GOLDEN']}/{KST.strftime('%Y/%m/%d')}\"\n",
    "\n",
    "# # stage_data_uri= f\"{preproc_data_dir}/stage.csv\"\n",
    "# stage_data_uri = f\"{base}/{keychain['S3_PATH_STAGE']}\"\n",
    "# train_data_uri = f\"{preproc_data_dir}/train.csv\"\n",
    "# test_data_uri = f\"{preproc_data_dir}/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e18774d8-f046-4de6-84fe-7a70c40a564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket                       -> 'palm-oil-price-forecast'\n",
      "preproc_data_dir             -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n",
      "project_prefix               -> 'palm-oil-price-forecast'\n",
      "stage_data_uri               -> 's3://palm-oil-price-forecast/staged-data'\n",
      "test_data_uri                -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n",
      "train_data_uri               -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4315f4a-fcbc-4796-8d98-6c2e5abd87e4",
   "metadata": {},
   "source": [
    "노트북에 저장된 변수를 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21ee5e9-7f3f-4c06-be03-d538fbf781fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store stage_data_uri)\n",
    "# %store train_data_uri)\n",
    "# %store test_data_uri)\n",
    "# %store train_model_uri)\n",
    "# %store leaderboard_uri)\n",
    "# %store project_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591b978f-ca1d-47da-8f99-ca849e167ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba1eda-81d5-4df9-9e5c-b6682982b8fa",
   "metadata": {},
   "source": [
    "# 2. 모델 빌딩 파이프라인 의 스텝(Step) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9d664-d5a1-44a1-a249-cbd1aa294a7e",
   "metadata": {},
   "source": [
    "## 2.1 모델 빌딩 파이프라인 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d15ee3-98aa-4c58-819b-5ea7479715aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_version: 1.0\n",
      "preprocessing_code: src/v1.0/preprocessing.py\n",
      "training_code: src/v1.0/train.py\n",
      "model_validation_code: src/v1.0/model_validation.py\n",
      "model_prediction_code: src/v1.0/prediction.py\n"
     ]
    }
   ],
   "source": [
    "preprocessing_code = f'src/v{code_version}/preprocessing.py'\n",
    "training_code = f'src/v{code_version}/train.py'\n",
    "model_validation_code = f'src/v{code_version}/model_validation.py'\n",
    "model_prediction_code = f'src/v{code_version}/prediction.py'\n",
    "print('code_version:',code_version)\n",
    "print('preprocessing_code:',preprocessing_code)\n",
    "print('training_code:',training_code)\n",
    "print('model_validation_code:',model_validation_code)\n",
    "print('model_prediction_code:',model_prediction_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3eaa7cf-ebeb-4438-9a18-d5fe1bf12971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2022-12-20 21:23:26.227101\n",
      "code verison: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "if code_version == '1.0':\n",
    "    ###################\n",
    "    # 0) 변수 선언 ###\n",
    "    ##################\n",
    "    project_prefix = BUCKET_NAME_USECASE # 프로젝트 변수\n",
    "    base = f\"s3://{BUCKET_NAME_USECASE}\"\n",
    "    KST = dt.today() + relativedelta(hours=9) # 한국 시간\n",
    "\n",
    "    stage_data_dir = f\"{base}/{keychain['S3_PATH_STAGE']}\" # Staging data path\n",
    "    preproc_data_dir = f\"{base}/{keychain['S3_PATH_GOLDEN']}/{KST.strftime('%Y/%m/%d')}\" # Golden data path\n",
    "    train_data_uri = f\"{preproc_data_dir}/train.csv\" \n",
    "    test_data_uri = f\"{preproc_data_dir}/test.csv\"\n",
    "    train_data_dir = f\"{base}/{keychain['S3_PATH_TRAIN']}/{KST.strftime('%Y/%m/%d')}\"\n",
    "    train_model_uri = f\"{train_data_dir}/output/output.tar.gz\" # 이건 전 단계에서 path를 가지고 와야함\n",
    "    leaderboard_uri = f\"{train_data_dir}/output/model.tar.gz\"\n",
    "\n",
    "    print(f\"Start job time: {KST}\")\n",
    "    print(f\"code verison: {code_version}\")\n",
    "\n",
    "    ###################\n",
    "    ## 1) 데이터 전처리를 위한 파이프라인 변수  ####################################\n",
    "    ###################\n",
    "    processing_instance_type = ParameterString(\n",
    "        name = \"ProcessingInstanceType\",\n",
    "        default_value = \"ml.m5.xlarge\"\n",
    "    )\n",
    "    processing_instance_count = ParameterInteger(\n",
    "        name = \"ProcessingInstanceCount\",\n",
    "        default_value = 1\n",
    "    )\n",
    "    input_stage_data = ParameterString(\n",
    "        name = \"InputStageData\",\n",
    "        default_value = stage_data_dir,\n",
    "    )\n",
    "    ###################\n",
    "    ## 2) 데이터 학습을 위한 파이프라인 변수  ####################################\n",
    "    ###################\n",
    "    train_instance_type = ParameterString(\n",
    "        name = \"TrainingInstanceType\",\n",
    "        default_value = \"ml.m5.xlarge\"\n",
    "    )\n",
    "    train_instance_count = ParameterInteger(\n",
    "        name = \"TrainInstanceCount\",\n",
    "        default_value = 1\n",
    "    )\n",
    "    input_train_data = ParameterString(\n",
    "        name = \"InputTrainData\",\n",
    "        default_value = train_data_uri,\n",
    "    )\n",
    "    input_test_data = ParameterString(\n",
    "        name = \"InputTestData\",\n",
    "        default_value = test_data_uri,\n",
    "    )\n",
    "    ###################\n",
    "    ## 3) 모델 검증을 위한 파이프라인 변수  ####################################\n",
    "    ###################\n",
    "    model_validation_instance_type = ParameterString(\n",
    "        name=\"ModelValidationInstanceType\",\n",
    "        default_value='ml.c5.xlarge'\n",
    "    )\n",
    "    model_validation_instance_count = ParameterInteger(\n",
    "        name=\"ModelValidationInstanceCount\",\n",
    "        default_value=1\n",
    "    )\n",
    "    input_model_data = ParameterString(\n",
    "        name=\"InputModelData\",\n",
    "        default_value = train_model_uri,\n",
    "    )\n",
    "    input_leaderboard_data = ParameterString(\n",
    "        name=\"InputLeaderboardData\",\n",
    "        default_value = leaderboard_uri,\n",
    "    )\n",
    "    ###################\n",
    "    ## 4) 모델 예측을 위한 파이프라인 변수  ####################################\n",
    "    ###################\n",
    "    prediction_instance_type = ParameterString(\n",
    "        name = \"PredctionInstanceType\",\n",
    "        default_value = \"ml.m5.xlarge\"\n",
    "    )\n",
    "    prediction_instance_count = ParameterInteger(\n",
    "        name = \"PredctionInstanceCount\",\n",
    "        default_value = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9497ace7-b074-449f-b652-2ec25023acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "# cache_config = CacheConfig(enable_caching=True, \n",
    "#                            expire_after=\"7d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa50b16-6c49-4b1f-b0d4-b2ebfb36683c",
   "metadata": {},
   "source": [
    "## 2.3 프로세서 단계 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6177b3b-24cb-4690-81e2-1aa8ee5747a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "skframework_version = \"0.23-1\"\n",
    "image_uri = retrieve(framework='mxnet',\n",
    "                     region='ap-northeast-2',\n",
    "                     version='1.9.0',\n",
    "                     py_version='py38',\n",
    "                     image_scope='training',\n",
    "                     instance_type=prediction_instance_type)\n",
    "\n",
    "###################\n",
    "# 1) 데이터 전처리 ###\n",
    "##################\n",
    "skprocessor_preprocessing = SKLearnProcessor(\n",
    "    framework_version = skframework_version,\n",
    "    instance_type = processing_instance_type,\n",
    "    instance_count = processing_instance_count,\n",
    "    base_job_name = f\"{project_prefix}-preprocessing\",\n",
    "    role = role,\n",
    ")\n",
    "\n",
    "################\n",
    "# 2) 모델 학습 ###\n",
    "###############\n",
    "mxnet_estimator = MXNet(\n",
    "    base_job_name = f\"{project_prefix}-training-autogluon060\", # prefix\n",
    "    entry_point = 'train.py',\n",
    "    source_dir = \"src\",\n",
    "    code_location = estimator_output_path,\n",
    "    output_path = estimator_output_path,\n",
    "    instance_type = train_instance_type,\n",
    "    instance_count = train_instance_count,\n",
    "    framework_version = '1.9.0',\n",
    "    py_version = 'py38',\n",
    "    role = role,\n",
    ")\n",
    "################\n",
    "# 3) 모델 검증 ###\n",
    "###############\n",
    "skprocessor_model_validation = SKLearnProcessor(\n",
    "    framework_version = skframework_version,\n",
    "    instance_type = model_validation_instance_type,\n",
    "    instance_count = model_validation_instance_count,\n",
    "    base_job_name = f\"{project_prefix}-model_validation\",\n",
    "    role = role,\n",
    ")\n",
    "###############\n",
    "# 4) 모델 예측 ##\n",
    "##############\n",
    "script_processor_prediction = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=image_uri,\n",
    "    instance_type = prediction_instance_type,\n",
    "    instance_count = prediction_instance_count,\n",
    "    base_job_name = f\"{project_prefix}-prediction\",\n",
    "    role = role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557e393-90b6-49a5-9a1e-89218ee04fbf",
   "metadata": {},
   "source": [
    "## 2.4 파이프라인 스텝 단계 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5f3bb63-c9f0-4fe1-b676-34ed600d5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "if code_version == '1.0':\n",
    "    split_date = '2022-10-31'\n",
    "\n",
    "    ###################\n",
    "    # 1) 데이터 전처리 ###\n",
    "    ##################\n",
    "    step_preprocessing = ProcessingStep(\n",
    "        name = f\"{project_prefix}-processing\",\n",
    "        processor = skprocessor_preprocessing,\n",
    "        inputs = [\n",
    "            ProcessingInput(source = stage_data_dir,\n",
    "                            destination = '/opt/ml/processing/input'),\n",
    "        ],\n",
    "        outputs = [\n",
    "            ProcessingOutput(output_name = \"stage\",\n",
    "                             source = '/opt/ml/processing/output/stage',\n",
    "                             destination = preproc_data_dir),\n",
    "            ProcessingOutput(output_name = \"train\",\n",
    "                             source = '/opt/ml/processing/output/train',\n",
    "                             destination = preproc_data_dir),\n",
    "            ProcessingOutput(output_name = \"test\",\n",
    "                             source = '/opt/ml/processing/output/test',\n",
    "                             destination = preproc_data_dir),\n",
    "        ],\n",
    "        job_arguments=[\"--split_date\", split_date,\n",
    "                      ],    \n",
    "        code = preprocessing_code\n",
    "    )\n",
    "    ################\n",
    "    # 2) 모델 학습 ###\n",
    "    ###############\n",
    "    step_train = TrainingStep(\n",
    "        name = f\"{project_prefix}-train-autogluon060\",\n",
    "        estimator = mxnet_estimator,\n",
    "        inputs = {\n",
    "            \"train\" : TrainingInput(\n",
    "                s3_data = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type = \"text/csv\"\n",
    "            ),\n",
    "            \"test\" : TrainingInput(\n",
    "                s3_data = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type = \"text/csv\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # 3) 모델 검증 ###\n",
    "    ###############\n",
    "    # 참조:https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html#API_DescribeTrainingJob_ResponseSyntax\n",
    "    step_model_validaion = ProcessingStep(\n",
    "        name = f\"{project_prefix}-model_validation\",\n",
    "        processor = skprocessor_model_validation,\n",
    "        job_arguments=[\"--s3_model_uri\", step_train.properties.ModelArtifacts.S3ModelArtifacts],    \n",
    "        code = model_validation_code\n",
    "    )\n",
    "    ###############\n",
    "    # 4) 모델 예측 ##\n",
    "    ##############\n",
    "    step_prediction = ProcessingStep(\n",
    "        name = f\"{project_prefix}-prediction\",\n",
    "        processor = script_processor_prediction,\n",
    "        inputs=[\n",
    "            ProcessingInput(source = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                            destination = \"/opt/ml/processing/input/train\"),\n",
    "            ProcessingInput(source = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                            destination = \"/opt/ml/processing/input/test\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name = \"prediction_data\",\n",
    "                             source = \"/opt/ml/processing/output\",\n",
    "                             destination = prediction_output_path)\n",
    "            ],\n",
    "        code = model_prediction_code\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b079639-15f9-4a02-93eb-da899709bf5a",
   "metadata": {},
   "source": [
    "## 2.5 최종 파이프라인 정의 및 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89624b5-bc17-4677-b1e6-660fd328c49e",
   "metadata": {},
   "source": [
    "### 최종 파이프라인 정의\n",
    "1. Processing: staged data내에서 데이터를 추출하여 데이터 통합 그리고 데이터 전처리 진행\n",
    "2. Training: \n",
    "3. Model Validation:\n",
    "4. Model Prediction(Infernece):\n",
    "     - 굳이 Endpoint를 생성할 필요가 없다.\n",
    "     \n",
    "참조하자: [MLOps Pipeline](https://aws.amazon.com/ko/blogs/machine-learning/deploy-an-mlops-solution-that-hosts-your-model-endpoints-in-aws-lambda/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1ce443-0600-42ad-b997-b3acf5bace2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "if code_version == '1.0':\n",
    "    pipeline_name = project_prefix\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            # 1) preprocessing's parameters \n",
    "            processing_instance_type, \n",
    "            processing_instance_count,\n",
    "            input_stage_data,\n",
    "            # 2) training's parameters        \n",
    "            train_instance_type,        \n",
    "            train_instance_count,   \n",
    "            input_train_data,\n",
    "            input_test_data,\n",
    "            # 3) model validating's parameters\n",
    "            model_validation_instance_type,\n",
    "            model_validation_instance_count,\n",
    "            # 4) predicion's parameters(inference)\n",
    "            prediction_instance_type,\n",
    "            prediction_instance_count,\n",
    "        ],\n",
    "       steps=[step_preprocessing,\n",
    "              step_train,\n",
    "              step_model_validaion,\n",
    "              step_prediction]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221059ed-17dc-475e-a866-1f4f5ff627f9",
   "metadata": {},
   "source": [
    "### 파이프라인 정의 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5f889ea-c4ba-4984-b2e6-8ed8139a1c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'InputStageData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://palm-oil-price-forecast/staged-data'},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'TrainInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'InputTrainData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://palm-oil-price-forecast/golden-data/2022/12/20/train.csv'},\n",
       "  {'Name': 'InputTestData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://palm-oil-price-forecast/golden-data/2022/12/20/test.csv'},\n",
       "  {'Name': 'ModelValidationInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'ModelValidationInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'PredctionInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'PredctionInstanceCount', 'Type': 'Integer', 'DefaultValue': 1}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'palm-oil-price-forecast-processing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--split_date', '2022-10-31'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::276114397529:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://palm-oil-price-forecast/staged-data',\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-276114397529/palm-oil-price-forecast-processing-ae1b05f7de586b37c79709c48b18c9ea/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'stage',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/20',\n",
       "        'LocalPath': '/opt/ml/processing/output/stage',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/20',\n",
       "        'LocalPath': '/opt/ml/processing/output/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/20',\n",
       "        'LocalPath': '/opt/ml/processing/output/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'palm-oil-price-forecast-train-autogluon060',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/mxnet-training:1.9.0-cpu-py38',\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://palm-oil-price-forecast/trained-model/2022/12/20'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': {'Get': 'Parameters.TrainInstanceCount'},\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'}},\n",
       "    'RoleArn': 'arn:aws:iam::276114397529:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.palm-oil-price-forecast-processing.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.palm-oil-price-forecast-processing.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'sagemaker_submit_directory': '\"s3://palm-oil-price-forecast/trained-model/2022/12/20/palm-oil-price-forecast-train-autogluon060-03e9489d87b3457108cd2777ac4b8bd2/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"ap-northeast-2\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://palm-oil-price-forecast/trained-model/2022/12/20',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1671539647',\n",
       "      'RuleEvaluatorImage': '578805364391.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://palm-oil-price-forecast/trained-model/2022/12/20'}}},\n",
       "  {'Name': 'palm-oil-price-forecast-model_validation',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ModelValidationInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ModelValidationInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--s3_model_uri',\n",
       "      {'Get': 'Steps.palm-oil-price-forecast-train-autogluon060.ModelArtifacts.S3ModelArtifacts'}],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/model_validation.py']},\n",
       "    'RoleArn': 'arn:aws:iam::276114397529:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-276114397529/palm-oil-price-forecast-model_validation-6738fdc53b3df55cbda9a691228410c3/input/code/model_validation.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}]}},\n",
       "  {'Name': 'palm-oil-price-forecast-prediction',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.PredctionInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.PredctionInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/mxnet-training:1.9.0-cpu-py38',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/prediction.py']},\n",
       "    'RoleArn': 'arn:aws:iam::276114397529:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.palm-oil-price-forecast-processing.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/train',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.palm-oil-price-forecast-processing.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-276114397529/palm-oil-price-forecast-prediction-85c0b927b25a6b8ba4be6c832dd255fc/input/code/prediction.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'prediction_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/forecasted-data/2022/12/20',\n",
       "        'LocalPath': '/opt/ml/processing/output',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f77f33-f76e-442d-aaef-88f5e3956b9e",
   "metadata": {},
   "source": [
    "### 파이프라인 정의를 제출하고 실행하기\n",
    "- 요청만 하고 기다리진 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24d24151-cf61-4b65-ad29-4d4c3cd52479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Async IO in Python\n",
    "# pipeline.upsert(role_arn=role)\n",
    "# execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d22ff4-d0cb-4178-8d59-7b0e6838797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "pipeline.upsert(role_arn = sagemaker.get_execution_role())\n",
    "execution = pipeline.start()\n",
    "#실행이 완료될 때까지 기다린다.\n",
    "execution.wait() \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ead1a434-b86c-4397-904a-2b3b37cb9ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리~모델 검증시간 : 1749.8 sec\n",
      "데이터 전처리~모델 검증시간 : 29.2 min\n"
     ]
    }
   ],
   "source": [
    "print(f\"데이터 전처리~모델 검증시간 : {end - start:.1f} sec\")\n",
    "print(f\"데이터 전처리~모델 검증시간 : {((end - start)/60):.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9753aed-e16b-44c0-9178-e17467b5e710",
   "metadata": {},
   "source": [
    "[22년 11월 30일 1차 테스트]    \n",
    "- 데이터 전처리~모델 검증 시간(초) : 1688.9 sec\n",
    "- 데이터 전처리~모델 검증 시간(분) : 28.1 min   \n",
    "\n",
    "[22년 11월 30일 2차 테스트]    \n",
    "- 데이터 전처리~모델 검증 시간(초) : 1719.9 sec\n",
    "- 데이터 전처리~모델 검증 시간(분) : 28.7 min\n",
    "\n",
    "[22년 12월 20일 1차 테스트]    \n",
    "- 데이터 전처리~모델 검증 시간(초) : 1749.8 sec\n",
    "- 데이터 전처리~모델 검증 시간(분) : 29.2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93222763-e88e-4a9f-910d-fea657f9a945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "041cb8c9-62f1-41b1-909c-832285bdcb6c",
   "metadata": {},
   "source": [
    "### Debug Model Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1efc9-3f08-4d28-8d8c-9b8e0faf8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4b365-3adc-4c86-8146-64ca2d4ea645",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0457a0-6262-4771-a7a3-8b2ac0c7fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68827aa7-9645-4b1c-8270-2162d1ba4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response['ModelArtifacts']['S3ModelArtifacts'].replace('model','output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e7329-d8c9-4c85-a2ab-3d584299c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response = execution.list_steps()[2]\n",
    "train_arn = train_response['Metadata']['TrainingJob']['Arn'] # index -1은 가장 처음 실행 step\n",
    "train_job_name = train_arn.split('/')[-1] # Processing job name만 추출\n",
    "train_response = sm_client.describe_training_job(TrainingJobName = train_job_name)\n",
    "train_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6ec5e-1baf-4c1e-b058-07408327f6a8",
   "metadata": {},
   "source": [
    "# 3. Future Works\n",
    "참고사항: 앞으로 얼마나 예측을 할것인지에 대해서 split_date를 설정하여 앞으로 몇일을 예측할 수 있다.(현재 분기점은 '2022-10-31' 기준으로 되어있다.)\n",
    "\n",
    "- 1 iter때 Prediction까지 진행을 하고 Model Registry 내 PendingManualApproval 모델이 저장이되면, 예측값과 실제값을 Quicksight 내 보여주고 Approve시 PendingManualApproval -> Approve 상태 전환\n",
    "    => 사내에서는 보안 이슈로 인해서 data 업로드가 어려움\n",
    "- Model Registry 내 model 상태가 업데이트 되면, Deployment 파이프라인 수행\n",
    "- CodeCommit을 통한 소스코드 관리\n",
    "    => 이거 첫번째로 찍먹해보자\n",
    "\n",
    "- Multi-model을 적용하는 파이프라인\n",
    "    => "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ed4e8-6c88-4495-9afb-62fc3dd1d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(prediction_output_path+'/prediction_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4de8ed-19a5-44f6-9b11-83c2e06d1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('_mpl-gallery')\n",
    "\n",
    "# make data\n",
    "np.random.seed(1)\n",
    "x = np.linspace(0, 8, 16)\n",
    "y1 = 3 + 4*x/8 + np.random.uniform(0.0, 0.5, len(x))\n",
    "y2 = 1 + 2*x/8 + np.random.uniform(0.0, 0.5, len(x))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.fill_between(x, y1, y2, alpha=.5, linewidth=0)\n",
    "ax.plot(x, (y1 + y2)/2, linewidth=2)\n",
    "\n",
    "ax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n",
    "       ylim=(0, 8), yticks=np.arange(1, 8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0fa4c-b944-43c3-8514-3e99b1cc41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N = 1000\n",
    "x = np.linspace(0, 10, N)\n",
    "y = x**2\n",
    "ones = np.ones(N)\n",
    "\n",
    "vals = [30, 20, 10] # Values to iterate over and add/subtract from y.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, val in enumerate(vals):\n",
    "    alpha = 0.5*(i+1)/len(vals) # Modify the alpha value for each iteration.\n",
    "    ax.fill_between(x, y+ones*val, y-ones*val, color='red', alpha=alpha)\n",
    "\n",
    "ax.plot(x, y, color='red') # Plot the original signal\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
