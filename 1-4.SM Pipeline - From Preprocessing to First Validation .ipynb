{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58d54f1-ccf2-445f-96f0-256d76fbd162",
   "metadata": {},
   "source": [
    "# SageMaker 모델 빌드 파이프라인을 이용한 모델 빌드 오케스트레이션\n",
    "Amazon SageMaker Model building pipeline은 머신러닝 워크플로우를 개발하는 데이터 과학자, 엔지니어들에게 SageMaker작업과 재생산가능한 머신러닝 파이프라인을 오케스트레이션하는 기능을 제공합니다. 또한 커스텀빌드된 모델을 실시간 추론환경이나 배치변환을 통한 추론 실행환경으로 배포하거나, 생성된 아티팩트의 계보(lineage)를 추적하는 기능을 제공합니다. 이 기능들을 통해 모델 아티팩트를 배포하고, 업무환경에서의 워크플로우를 배포/모니터링하고, 간단한 인터페이스를 통해 아티팩트의 계보 추적하고, 머신러닝 애플리케이션 개발의 베스트 프렉티스를 도입하여, 보다 안정적인 머신러닝 애플리케이션 운영환경을 구현할 수 있습니다.\n",
    "\n",
    "SageMaker pipeline 서비스는 JSON 선언으로 구현된 SageMaker Pipeline DSL(Domain Specific Language, 도메인종속언어)를 지원합니다. 이 DSL은 파이프라인 파라마터와 SageMaker 작업단계의 DAG(Directed Acyclic Graph)를 정의합니다. SageMaker Python SDK를 이용하면 이 파이프라인 DSL의 생성을 보다 간편하게 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2c2f1d-ceb9-40f1-b4ef-2089575788ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "model_validation_code             -> 's3://crude-palm-oil-prices-forecast/src/model_val\n",
      "prediction_code                   -> 's3://crude-palm-oil-prices-forecast/src/predictio\n",
      "visualization_code                -> 's3://crude-palm-oil-prices-forecast/src/visualiza\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d108d0-bba1-4ad2-a141-1df0f59577b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Miscel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc761a70-090c-416d-a0f6-1b7847bc2314",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 사용 코드 v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e04e20-2211-4506-842f-1eb694f83a48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca544d62-143c-4523-a2b6-263294caf00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY=','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCHI','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'FKLIc1', 'FKLIc2', 'FKLIc3','FTSE','GCc1', 'GCc2', 'GCc3','GDAXI','GENMKL','HSI','IOIBKL','IXIC','JNIc1','JNIc2','JNIc3','KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2', 'MCCc3','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL','SSEC', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  ric,\n",
    "                  s3_resource,\n",
    "                  BUCKET_NAME_USECASE,\n",
    "                  S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(scaler, fp)\n",
    "        fp.seek(0)\n",
    "        s3_resource.put_object(Body = fp.read(),\n",
    "                               Bucket = BUCKET_NAME_USECASE,\n",
    "                               Key = f\"{S3_PATH_GOLDEN}/{KST.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    split_date_default = dt.today() + relativedelta(hours = 9) - relativedelta(months=1)\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--split_date', type=str, default=split_date_default.strftime('%Y-%m-%d'))       \n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = 1, help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "        \n",
    "    # parse arguments\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_date: {args.split_date}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")   \n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_date = args.split_date    \n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    stage_dir = f'{base_output_dir}/stage/stage.csv\"'\n",
    "    logger.info(f\"\\n### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    for file in path_list:\n",
    "        df_tmp= pd.read_csv(file, encoding='utf-8') \n",
    "        df_sum = pd.concat([df_sum, df_tmp])\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{base_output_dir}/stage/stage.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    \n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= '2014-07-02'].reset_index(drop = True)\n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if int(scaler_switch) == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value, col, name, s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    \n",
    "    # train 데이터 나누기\n",
    "    df_train = df_golden[df_golden['ds'] < split_date]\n",
    "    df_train.to_csv(f\"{base_output_dir}/train/train.csv\", index = False)\n",
    "    \n",
    "    df_test = df_golden[df_golden['ds'] >= split_date]\n",
    "    df_test.to_csv(f\"{base_output_dir}/test/test.csv\", index = False)\n",
    "    \n",
    "    logger.info(f\"\\n ### Final result for train dataset \")\n",
    "    logger.info(f\"\\n ####preprocessed train shape \\n {df_train.shape}\")        \n",
    "    logger.info(f\"preprocessed train sample: head(2) \\n {df_train.head(2)}\")\n",
    "    logger.info(f\"preprocessed train sample: tail(2) \\n {df_train.tail(2)}\")\n",
    "    \n",
    "    logger.info(f\"\\n ####preprocessed test shape \\n {df_test.shape}\")            \n",
    "    logger.info(f\"preprocessed test sample: head(2) \\n {df_test.head(2)}\")\n",
    "    logger.info(f\"preprocessed test sample: tail(2) \\n {df_test.tail(2)}\")\n",
    "\n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eefb11-6bb6-4e1b-8db1-8e59f2ffa528",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-2. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe56ec5-af8e-44d1-a2d9-06438a136ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import joblib # from sklearn.externals import joblib\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime as dt\n",
    "\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ###################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ###################################        \n",
    "\n",
    "    logger.info(f\"### start training code\")    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output_dir', type = str, default = os.environ.get('SM_OUTPUT_DIR'))\n",
    "    parser.add_argument('--output_data_dir', type = str, default = os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type = str, default = os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train_dir', type = str, default = os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test_dir', type = str, default = os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--item', type = str, default = 'FCPOc3')\n",
    "    parser.add_argument('--target', type = str, default = 'y')\n",
    "    parser.add_argument('--metric', type = str, default = 'MAPE')    \n",
    "    parser.add_argument('--quality', type = str, default = 'low_quality')\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.output_data_dir: {args.output_data_dir}\")    \n",
    "    logger.info(f\"args.model_dir: {args.model_dir}\")        \n",
    "    logger.info(f\"args.train_dir: {args.train_dir}\")   \n",
    "    logger.info(f\"args.test_dir: {args.test_dir}\")   \n",
    "    logger.info(f\"args.item: {args.item}\")   \n",
    "    logger.info(f\"args.target: {args.target}\")    \n",
    "    logger.info(f\"args.metric: {args.metric}\")   \n",
    "    logger.info(f\"args.quality: {args.quality}\")   \n",
    "    \n",
    "    output_dir = args.output_dir\n",
    "    output_data_dir = args.output_data_dir\n",
    "    model_dir = args.model_dir\n",
    "    train_dir = args.train_dir\n",
    "    test_dir = args.test_dir\n",
    "    item = args.item\n",
    "    target = args.target\n",
    "    metric = args.metric\n",
    "    quality = args.quality\n",
    "    \n",
    "    logger.info(\"### Reading input data\")\n",
    "    df_train= pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(test_dir, 'test.csv'))        \n",
    "    \n",
    "    logger.info(\"### Convert TimeSeriesDataFrame\")\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"### Show the range of date for training and test\")    \n",
    "    logger.info('Item:\\t', item)\n",
    "    logger.info('Target:\\t', target)   \n",
    "    logger.info('Train:\\t',tdf_train.loc[item][target].index.min(),'~',tdf_train.loc[item][target].index.max())\n",
    "    logger.info('Test:\\t',tdf_test.loc[item][target].index.min(),'~',tdf_test.loc[item][target].index.max())\n",
    "    logger.info('The number of test data:',len(tdf_test.loc[item][target]))\n",
    "    \n",
    "    logger.info(\"### Training AutoGluon Model\")    \n",
    "    predictor = TimeSeriesPredictor(\n",
    "        path = model_dir,\n",
    "        target = target,\n",
    "        prediction_length = len(tdf_test.loc[item][target]),\n",
    "        eval_metric = metric,\n",
    "    )\n",
    "    predictor.fit(\n",
    "        train_data = tdf_train,\n",
    "        presets = quality\n",
    "    )    \n",
    "    logger.info(\"Saving model to {}\".format(model_dir))\n",
    "    \n",
    "    # 원래라면 Validation dataset이 input으로 들어와서 leaderboard와 prediction을 해야한다.\n",
    "    # 근데, 여기서는 아니다. 이번 사이클에서는 test data까지 모두 산출한다음에 넣는것으로 진행하자.\n",
    "    predictor_leaderboard = predictor.leaderboard(tdf_test, silent = True)\n",
    "    predictor_leaderboard.to_csv(os.path.join(output_data_dir,'leaderboard.csv'), index = False)\n",
    "    \n",
    "    predictions = predictor.predict(train_data)\n",
    "    predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69914f-0aef-4d13-8d6b-5d1ef94aa75c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-3. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de56531e-8546-4a3a-8890-c6e6495354b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/model_validation.py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import argparse\n",
    "import json\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import joblib\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import tarfile\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "def convert_series_to_description(leaderboard : pd.Series):\n",
    "    return ','.join(leaderboard.loc[0,['model','score_test','score_val']].to_string().split())\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_path', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--s3_model_uri', type=str, default=\"/opt/ml/processing/model\")   \n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_path: {args.base_input_path}\")\n",
    "    logger.info(f\"args.s3_model_uri: {args.s3_model_uri}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "    \n",
    "    base_input_path = args.base_input_path\n",
    "    s3_model_uri = args.s3_model_uri\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ############################################\n",
    "    ##### Model, Leaderboard 파일 가져오기 #####\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Model, Leaderboard zip files \")\n",
    "    logger.info(f\"\\n#### Extract output.tar.gz and Read a Leaderboard \")\n",
    "    ## 22.11.29 추가: 이전 step인, step_train에서 model.tar.gz의 uri는 가져올 수 있었지만, output.tar.gz는 못가져왔다. 이를 model.tar.gz에서 output.tar.gz으로 바꾸는방식으로 우회하자\n",
    "    leaderboard_uri = s3_model_uri.replace('model.tar.gz','output.tar.gz')#,f'{base_input_path}/output.tar.gz'\n",
    "    logger.info(f\"\\n#### output.tar.gz uri : {leaderboard_uri}\")\n",
    "    output_bucket, output_key = get_bucket_key_from_uri(leaderboard_uri)  \n",
    "    output_obj = s3_client.get_object(Bucket = output_bucket, Key = output_key)\n",
    "   \n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=output_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(base_input_path)    \n",
    "    logger.info(f\"file list in {base_input_path}: {os.listdir(base_input_path)}\")        \n",
    "    \n",
    "    # if leaderboard_path.endswith(\"tar.gz\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:gz\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "    # elif leaderboard_path.endswith(\"tar\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "\n",
    "    leaderboard = pd.read_csv(f'{base_input_path}/leaderboard.csv').sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                                ascending = False)\n",
    "    logger.info(f\"leaderboard train sample: head(5) \\n {leaderboard.head()}\")\n",
    "    logger.info(f\"\\n#### Set  \")\n",
    "    model_package_group_name = model_package_group_name\n",
    "    modelpackage_inference_specification =  {\n",
    "        \"InferenceSpecification\": {\n",
    "            \"Containers\": [\n",
    "                {\n",
    "                    \"Image\": '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/autogluon-inference:0.4-cpu-py38',\n",
    "                    \"ModelDataUrl\": s3_model_uri#'#args.model_path_uri\n",
    "                }\n",
    "            ],\n",
    "            \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "            \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "        }\n",
    "    }\n",
    "    if len(leaderboard[leaderboard['score_val'] > -0.13]) > 0:\n",
    "        logger.info(f\"\\n#### Pass the first performance filtering\")\n",
    "        \n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Passed ModelPackage Version ARN : {}'.format(model_package_arn))\n",
    "        \n",
    "    else:\n",
    "        logger.info(f\"\\n#### None of them passed the filtering\")\n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"Rejected\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Rejected ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491c493-b830-47df-8afd-8585041f4bba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-4. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1224ccca-32e7-4b42-a3e1-e2e3ed6fb6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.0/prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.0/prediction.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'autogluon==0.6.0'])\n",
    "    from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_dir', type=str, default=\"/opt/ml/processing/input\", help='train,testset 불러오는곳')\n",
    "    parser.add_argument('--output_dir', type = str, default = \"/opt/ml/processing/output\", help='예측 결과값이 저장되는 곳, test dataset과 prediction 결과가 merge되서 저장된다.')\n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "    logger.info(\"\\n######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_dir: {args.base_input_dir}\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "\n",
    "    base_input_dir = args.base_input_dir\n",
    "    output_dir = args.output_dir\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    model_dir = '/opt/ml/model'\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 적합한 모델의 URI 찾고, 탑 성능 모델 이름 가져오기 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Finding suitable model uri ####################################\")\n",
    "    logger.info(f\"Model Group name: {model_package_group_name}\")\n",
    "    model_registry_list = sm_client.list_model_packages(ModelPackageGroupName = model_package_group_name)['ModelPackageSummaryList']\n",
    "    for model in model_registry_list:\n",
    "        if (model['ModelPackageGroupName'] == model_package_group_name and\n",
    "            model['ModelApprovalStatus'] == 'Approved'):\n",
    "            mr_arn = model['ModelPackageArn']\n",
    "            break\n",
    "    describe_model = sm_client.describe_model_package(ModelPackageName=mr_arn)\n",
    "    s3_model_uri = describe_model['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "    top_model_name = describe_model['ModelPackageDescription'].split(',')[1]\n",
    "\n",
    "    logger.info(f\"Found suitable model uri: {s3_model_uri}\")\n",
    "    logger.info(f\"And top model name: {top_model_name}\")\n",
    "    \n",
    "    logger.info(\"\\n#########Download suitable model file  ####################################\")\n",
    "    model_bucket, model_key = get_bucket_key_from_uri(s3_model_uri)  \n",
    "    model_obj = s3_client.get_object(Bucket = model_bucket, Key = model_key)\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 모델 압축 풀고 TimeseriesDataFrame으로 변환 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=model_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(model_dir)    \n",
    "    logger.info(f\"list in /opt/ml/model: {os.listdir(model_dir)}\")        \n",
    "    \n",
    "    logger.info(\"\\n######### Convert df_test dataframe into TimeSeriesDataFrame  ###########\")        \n",
    "    df_train = pd.read_csv(os.path.join(f'{base_input_dir}/train/train.csv'))\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    df_test = pd.read_csv(f\"{base_input_dir}/test/test.csv\")\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    logger.info(f\"df_test sample: tail(2) \\n {tdf_train.tail(2)}\")\n",
    "    logger.info(f\"df_test sample: head(2) \\n {tdf_test.head(2)}\")\n",
    "    \n",
    "    ################################\n",
    "    ###### Prediction 시작 ##########\n",
    "    ###############################\n",
    "    logger.info(\"\\n######### Start prediction  ###########\")        \n",
    "    loaded_trainer = pickle.load(open(f\"{model_dir}/models/trainer.pkl\", 'rb'))\n",
    "    logger.info(f\"loaded_trainer: {loaded_trainer}\")\n",
    "    prediction_ag_model = loaded_trainer.predict(data = tdf_train,\n",
    "                                                 model = top_model_name)\n",
    "    logger.info(f\"prediction_ag_model sample: head(2) \\n {prediction_ag_model.head(2)}\")\n",
    "\n",
    "    prediction_result = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model.loc['FCPOc3'],\n",
    "                                 left_index = True, right_index = True, how = 'left')\n",
    "    prediction_result.to_csv(f'{output_dir}/prediction_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0174ac3-8374-41a2-b29c-6ed0e36a41bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 사용 코드 v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e809d-1584-4a57-a69a-ecb2182063fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a076e479-a18b-4a8a-baca-6e3fbc67c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.1/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY=','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCHI','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'FKLIc1', 'FKLIc2', 'FKLIc3','FTSE','GCc1', 'GCc2', 'GCc3','GDAXI','GENMKL','HSI','IOIBKL','IXIC','JNIc1','JNIc2','JNIc3','KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2', 'MCCc3','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL','SSEC', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  ric,\n",
    "                  s3_resource,\n",
    "                  BUCKET_NAME_USECASE,\n",
    "                  S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(scaler, fp)\n",
    "        fp.seek(0)\n",
    "        s3_resource.put_object(Body = fp.read(),\n",
    "                               Bucket = BUCKET_NAME_USECASE,\n",
    "                               Key = f\"{S3_PATH_GOLDEN}/{KST.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--split_date', type=str, default=KST.strftime('%Y-%m-%d'))\n",
    "    parser.add_argument('--num_fold', type=str, default='5')       \n",
    "\n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = 1, help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "        \n",
    "    # parse arguments\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_date: {args.split_date}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")\n",
    "    logger.info(f\"args.num_fold: {args.num_fold}\")\n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_date = args.split_date\n",
    "    num_fold = int(args.num_fold)\n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    stage_dir = f'{base_output_dir}/stage'\n",
    "    logger.info(f\"\\n### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    for file in path_list:\n",
    "        df_tmp= pd.read_csv(file, encoding='utf-8') \n",
    "        df_sum = pd.concat([df_sum, df_tmp])\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{stage_dir}/stage_integrated.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    \n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    \n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= '2014-07-02'].reset_index(drop = True)\n",
    "        \n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if int(scaler_switch) == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value, col, name, s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "        \n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    \n",
    "    df_train_fold0 = df_golden[df_golden['ds'] < split_date]\n",
    "    df_train_fold0.to_csv(f\"{base_output_dir}/train/train_fold1.csv\", index = False)\n",
    "    df_test_fold1 = df_golden[df_golden['ds'] >= split_date]\n",
    "    df_test_fold1.to_csv(f\"{base_output_dir}/test/test_fold1.csv\", index = False)\n",
    "\n",
    "    # train 데이터 나누기\n",
    "    for cnt in range(num_fold):\n",
    "        split_date = (dt.strptime(split_date, '%Y-%m-%d') - relativedelta(days=30)).strftime('%Y-%m-%d')\n",
    "    \n",
    "        logger.info(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < {split_date}]\")\n",
    "        exec(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < split_date]\")\n",
    "        exec(f\"df_train_fold{cnt+1}.to_csv('{base_output_dir}/train/train_fold{cnt+1}.csv', index = False)\")\n",
    "\n",
    "        logger.info(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= {split_date}]\")\n",
    "        exec(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= split_date]\")\n",
    "        exec(f\"df_test_fold{cnt+1}.to_csv('{base_output_dir}/test/test_fold{cnt+1}.csv', index = False)\")\n",
    "    \n",
    "    logger.info(f\"\\n ### Final result for train dataset \")\n",
    "    logger.info(f\"\\n ####preprocessed train shape \\n {df_train.shape}\")        \n",
    "    logger.info(f\"preprocessed train sample: head(2) \\n {df_train.head(2)}\")\n",
    "    logger.info(f\"preprocessed train sample: tail(2) \\n {df_train.tail(2)}\")\n",
    "    \n",
    "    logger.info(f\"\\n ####preprocessed test shape \\n {df_test.shape}\")            \n",
    "    logger.info(f\"preprocessed test sample: head(2) \\n {df_test.head(2)}\")\n",
    "    logger.info(f\"preprocessed test sample: tail(2) \\n {df_test.tail(2)}\")\n",
    "\n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae82de-9794-4955-9e71-fbeedc0a76ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-2. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01fcadea-8ce1-48ca-a12f-3ce989c77772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.1/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import joblib # from sklearn.externals import joblib\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime as dt\n",
    "\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ###################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ###################################        \n",
    "\n",
    "    logger.info(f\"### start training code\")    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output_dir', type = str, default = os.environ.get('SM_OUTPUT_DIR'))\n",
    "    parser.add_argument('--output_data_dir', type = str, default = os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type = str, default = os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train_dir', type = str, default = os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test_dir', type = str, default = os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--item', type = str, default = 'FCPOc3')\n",
    "    parser.add_argument('--target', type = str, default = 'y')\n",
    "    parser.add_argument('--metric', type = str, default = 'MAPE')    \n",
    "    parser.add_argument('--quality', type = str, default = 'low_quality')\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.output_data_dir: {args.output_data_dir}\")    \n",
    "    logger.info(f\"args.model_dir: {args.model_dir}\")        \n",
    "    logger.info(f\"args.train_dir: {args.train_dir}\")   \n",
    "    logger.info(f\"args.test_dir: {args.test_dir}\")   \n",
    "    logger.info(f\"args.item: {args.item}\")   \n",
    "    logger.info(f\"args.target: {args.target}\")    \n",
    "    logger.info(f\"args.metric: {args.metric}\")   \n",
    "    logger.info(f\"args.quality: {args.quality}\")   \n",
    "    \n",
    "    output_dir = args.output_dir\n",
    "    output_data_dir = args.output_data_dir\n",
    "    model_dir = args.model_dir\n",
    "    train_dir = args.train_dir\n",
    "    test_dir = args.test_dir\n",
    "    item = args.item\n",
    "    target = args.target\n",
    "    metric = args.metric\n",
    "    quality = args.quality\n",
    "    \n",
    "    logger.info(\"### Reading input data\")\n",
    "    df_train= pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(test_dir, 'test.csv'))        \n",
    "    \n",
    "    logger.info(\"### Convert TimeSeriesDataFrame\")\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"### Show the range of date for training and test\")    \n",
    "    logger.info('Item:\\t', item)\n",
    "    logger.info('Target:\\t', target)   \n",
    "    logger.info('Train:\\t',tdf_train.loc[item][target].index.min(),'~',tdf_train.loc[item][target].index.max())\n",
    "    logger.info('Test:\\t',tdf_test.loc[item][target].index.min(),'~',tdf_test.loc[item][target].index.max())\n",
    "    logger.info('The number of test data:',len(tdf_test.loc[item][target]))\n",
    "    \n",
    "    logger.info(\"### Training AutoGluon Model\")    \n",
    "    predictor = TimeSeriesPredictor(\n",
    "        path = model_dir,\n",
    "        target = target,\n",
    "        prediction_length = len(tdf_test.loc[item][target]),\n",
    "        eval_metric = metric,\n",
    "    )\n",
    "    predictor.fit(\n",
    "        train_data = tdf_train,\n",
    "        presets = quality\n",
    "    )    \n",
    "    logger.info(\"Saving model to {}\".format(model_dir))\n",
    "    \n",
    "    # 원래라면 Validation dataset이 input으로 들어와서 leaderboard와 prediction을 해야한다.\n",
    "    # 근데, 여기서는 아니다. 이번 사이클에서는 test data까지 모두 산출한다음에 넣는것으로 진행하자.\n",
    "    predictor_leaderboard = predictor.leaderboard(tdf_test, silent = True)\n",
    "    predictor_leaderboard.to_csv(os.path.join(output_data_dir,'leaderboard.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b38b7-f505-4656-9efa-fa9296e6b459",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-3. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4bb17db-627e-4d76-b576-8b6869385867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.1/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/model_validation.py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import argparse\n",
    "import json\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import joblib\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import tarfile\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "def convert_series_to_description(leaderboard : pd.Series):\n",
    "    return ','.join(leaderboard.loc[0,['model','score_test','score_val']].to_string().split())\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_path', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--s3_model_uri', type=str, default=\"/opt/ml/processing/model\")   \n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_path: {args.base_input_path}\")\n",
    "    logger.info(f\"args.s3_model_uri: {args.s3_model_uri}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "    \n",
    "    base_input_path = args.base_input_path\n",
    "    s3_model_uri = args.s3_model_uri\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ############################################\n",
    "    ##### Model, Leaderboard 파일 가져오기 #####\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Model, Leaderboard zip files \")\n",
    "    logger.info(f\"\\n#### Extract output.tar.gz and Read a Leaderboard \")\n",
    "    ## 22.11.29 추가: 이전 step인, step_train에서 model.tar.gz의 uri는 가져올 수 있었지만, output.tar.gz는 못가져왔다. 이를 model.tar.gz에서 output.tar.gz으로 바꾸는방식으로 우회하자\n",
    "    leaderboard_uri = s3_model_uri.replace('model.tar.gz','output.tar.gz')#,f'{base_input_path}/output.tar.gz'\n",
    "    logger.info(f\"\\n#### output.tar.gz uri : {leaderboard_uri}\")\n",
    "    output_bucket, output_key = get_bucket_key_from_uri(leaderboard_uri)  \n",
    "    output_obj = s3_client.get_object(Bucket = output_bucket, Key = output_key)\n",
    "   \n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=output_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(base_input_path)    \n",
    "    logger.info(f\"file list in {base_input_path}: {os.listdir(base_input_path)}\")        \n",
    "    \n",
    "    # if leaderboard_path.endswith(\"tar.gz\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:gz\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "    # elif leaderboard_path.endswith(\"tar\"):\n",
    "    #     tar = tarfile.open(leaderboard_path, \"r:\")\n",
    "    #     tar.extractall(base_input_path)\n",
    "    #     tar.close()\n",
    "\n",
    "    leaderboard = pd.read_csv(f'{base_input_path}/leaderboard.csv').sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                                ascending = False)\n",
    "    logger.info(f\"leaderboard train sample: head(5) \\n {leaderboard.head()}\")\n",
    "    logger.info(f\"\\n#### Set  \")\n",
    "    model_package_group_name = model_package_group_name\n",
    "    modelpackage_inference_specification =  {\n",
    "        \"InferenceSpecification\": {\n",
    "            \"Containers\": [\n",
    "                {\n",
    "                    \"Image\": '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/autogluon-inference:0.4-cpu-py38',\n",
    "                    \"ModelDataUrl\": s3_model_uri#'#args.model_path_uri\n",
    "                }\n",
    "            ],\n",
    "            \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "            \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "        }\n",
    "    }\n",
    "    if len(leaderboard[leaderboard['score_val'] > -0.13]) > 0:\n",
    "        logger.info(f\"\\n#### Pass the first performance filtering\")\n",
    "        \n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Passed ModelPackage Version ARN : {}'.format(model_package_arn))\n",
    "        \n",
    "    else:\n",
    "        logger.info(f\"\\n#### None of them passed the filtering\")\n",
    "        create_model_package_input_dict = {\n",
    "            \"ModelPackageGroupName\" : model_package_group_name,\n",
    "            \"ModelPackageDescription\" : convert_series_to_description(leaderboard),\n",
    "            \"ModelApprovalStatus\" : \"Rejected\"\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "        create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "        model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "        logger.info('### Rejected ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbc6dd-ac0d-4ee6-b307-a55378d5ba63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0-4. prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd60cfd8-e444-46e8-87f0-f44ad9d2af83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.1/prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/prediction.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "\n",
    "def get_bucket_key_from_uri(uri):\n",
    "    uri_aws_path = uri.split('//')[1]\n",
    "    uri_bucket = uri_aws_path.rsplit('/')[0]\n",
    "    uri_file_path = '/'.join(uri_aws_path.rsplit('/')[1:])\n",
    "    return uri_bucket, uri_file_path\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'autogluon==0.6.0'])\n",
    "    from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    \n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "    boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "\n",
    "    region = boto3_session.region_name\n",
    "\n",
    "    s3_resource = boto3_session.resource('s3')\n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3.client('sagemaker',\n",
    "                             aws_access_key_id = ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                             region_name = 'ap-northeast-2')\n",
    "    \n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_input_dir', type=str, default=\"/opt/ml/processing/input\", help='train,testset 불러오는곳')\n",
    "    parser.add_argument('--output_dir', type = str, default = \"/opt/ml/processing/output\", help='예측 결과값이 저장되는 곳, test dataset과 prediction 결과가 merge되서 저장된다.')\n",
    "    parser.add_argument('--model_package_group_name', type=str, default='palm-oil-price-forecast')   \n",
    "    args = parser.parse_args()     \n",
    "    logger.info(\"\\n######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_input_dir: {args.base_input_dir}\")\n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "\n",
    "    base_input_dir = args.base_input_dir\n",
    "    output_dir = args.output_dir\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    model_dir = '/opt/ml/model'\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 적합한 모델의 URI 찾고, 탑 성능 모델 이름 가져오기 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Finding suitable model uri ####################################\")\n",
    "    logger.info(f\"Model Group name: {model_package_group_name}\")\n",
    "    model_registry_list = sm_client.list_model_packages(ModelPackageGroupName = model_package_group_name)['ModelPackageSummaryList']\n",
    "    for model in model_registry_list:\n",
    "        if (model['ModelPackageGroupName'] == model_package_group_name and\n",
    "            model['ModelApprovalStatus'] == 'Approved'):\n",
    "            mr_arn = model['ModelPackageArn']\n",
    "            break\n",
    "    describe_model = sm_client.describe_model_package(ModelPackageName=mr_arn)\n",
    "    s3_model_uri = describe_model['InferenceSpecification']['Containers'][0]['ModelDataUrl']\n",
    "    top_model_name = describe_model['ModelPackageDescription'].split(',')[1]\n",
    "\n",
    "    logger.info(f\"Found suitable model uri: {s3_model_uri}\")\n",
    "    logger.info(f\"And top model name: {top_model_name}\")\n",
    "    \n",
    "    logger.info(\"\\n#########Download suitable model file  ####################################\")\n",
    "    model_bucket, model_key = get_bucket_key_from_uri(s3_model_uri)  \n",
    "    model_obj = s3_client.get_object(Bucket = model_bucket, Key = model_key)\n",
    "    \n",
    "    ##########################################################\n",
    "    ###### 모델 압축 풀고 TimeseriesDataFrame으로 변환 ##########\n",
    "    #########################################################\n",
    "    logger.info(\"\\n######### Model zip file extraction ####################################\")\n",
    "    with tarfile.open(fileobj=model_obj['Body'], mode='r|gz') as file:\n",
    "        file.extractall(model_dir)    \n",
    "    logger.info(f\"list in /opt/ml/model: {os.listdir(model_dir)}\")        \n",
    "    \n",
    "    logger.info(\"\\n######### Convert df_test dataframe into TimeSeriesDataFrame  ###########\")        \n",
    "    df_train = pd.read_csv(os.path.join(f'{base_input_dir}/train/train.csv'))\n",
    "    df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "    tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_train,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    df_test = pd.read_csv(f\"{base_input_dir}/test/test.csv\")\n",
    "    df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "    tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "        df_test,\n",
    "        id_column=\"ric\",\n",
    "        timestamp_column=\"ds\",\n",
    "    )\n",
    "    logger.info(f\"df_test sample: tail(2) \\n {tdf_train.tail(2)}\")\n",
    "    logger.info(f\"df_test sample: head(2) \\n {tdf_test.head(2)}\")\n",
    "    \n",
    "    ################################\n",
    "    ###### Prediction 시작 ##########\n",
    "    ###############################\n",
    "    logger.info(\"\\n######### Start prediction  ###########\")        \n",
    "    loaded_trainer = pickle.load(open(f\"{model_dir}/models/trainer.pkl\", 'rb'))\n",
    "    logger.info(f\"loaded_trainer: {loaded_trainer}\")\n",
    "    prediction_ag_model = loaded_trainer.predict(data = tdf_train,\n",
    "                                                 model = top_model_name)\n",
    "    logger.info(f\"prediction_ag_model sample: head(2) \\n {prediction_ag_model.head(2)}\")\n",
    "\n",
    "    prediction_result = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model.loc['FCPOc3'],\n",
    "                                 left_index = True, right_index = True, how = 'left')\n",
    "    prediction_result.to_csv(f'{output_dir}/prediction_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03365354-35af-4bdb-9abc-c8c3a3c5da5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. 사용 코드 v1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad58b1-5eb6-4f31-b2b4-62f96c960ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d8dde1-3036-44bb-953f-c4b8fa25c82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.2/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.2/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "from logging.config import dictConfig\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "KST_aday_before = KST - relativedelta(days=1) \n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD', 'EUR','JPY', 'KRW', 'MYR', 'GBP', 'INR','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'GCc1', 'GCc2', 'GCc3','GENMKL','HSI','IOIBKL', 'KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        l.addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "\n",
    "    secret_name = \"dev/ForecastPalmOilPrice\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    #2023-01-07: Interpolate 했음에도 불구하고, null값인경우 close 값으로 채움\n",
    "    df[col] = df[col].fillna(df['y'])\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  output_dir : str,\n",
    "                  ric,) -> tuple:\n",
    "                  # s3_resource,\n",
    "                  # BUCKET_NAME_USECASE,\n",
    "                  # S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    joblib.dump(scaler, os.path.join(output_dir, f'{ric}_{col_name}_scaler.pkl'))    \n",
    "    # with tempfile.TemporaryFile() as fp:\n",
    "    #     joblib.dump(scaler, fp)\n",
    "    #     fp.seek(0)\n",
    "        # s3_resource.put_object(Body = fp.read(),\n",
    "        #                        Bucket = BUCKET_NAME_USECASE,\n",
    "        #                        Key = f\"{S3_PATH_GOLDEN}/{KST_aday_before.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\") \n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--split_start', type=str, default='2014-07-02')    \n",
    "    parser.add_argument('--split_end', type=str, default=KST.strftime('%Y-%m-%d'))\n",
    "    parser.add_argument('--num_fold', type=str, default='5')\n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = '1', help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "     \n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    \n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "\n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    args = parse_args()\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_start: {args.split_start}\")   \n",
    "    logger.info(f\"args.split_end: {args.split_end}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")\n",
    "    logger.info(f\"args.num_fold: {args.num_fold}\")\n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_start = args.split_start\n",
    "    split_end = args.split_end\n",
    "    num_fold = int(args.num_fold)\n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['AWS_ACCESS_KEY_ID']\n",
    "    ACCESS_SECRET_KEY = keychain['AWS_ACCESS_SECRET_KEY']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['PROJECT_BUCKET_NAME']\n",
    "    DATALAKE_BUCKET_NAME = keychain['DATALAKE_BUCKET_NAME']\n",
    "\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    logger.info(f\"### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    equalSignList = ['CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=', 'CNY=']\n",
    "    for file in path_list:\n",
    "        try:\n",
    "            df_tmp = pd.read_csv(file, encoding='utf-8') \n",
    "            df_tmp['RIC'] = df_tmp['RIC'].apply(lambda x: x[:-1] if x in equalSignList else x)\n",
    "            df_sum = pd.concat([df_sum, df_tmp])\n",
    "        except:\n",
    "            logger.info(f\"{file} is empty \")\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{base_output_dir}/stage/stage_integrated.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    logger.info(f\"Integrated data.info \\n {df_sum.info()}\")\n",
    "\n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    \n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= split_start].reset_index(drop = True)\n",
    "        \n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if scaler_switch == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value,\n",
    "                                                  col,\n",
    "                                                  f'{base_output_dir}/scaler',\n",
    "                                                  name,) #s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Transform data info \\n {df_sum.head(2)}\")\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "        \n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    df_train_fold0 = df_golden[df_golden['ds'] < split_end]\n",
    "\n",
    "    # train 데이터 나누기\n",
    "    for cnt in range(num_fold):\n",
    "        split_end = (dt.strptime(split_end, '%Y-%m-%d') - relativedelta(days=30)).strftime('%Y-%m-%d')\n",
    "    \n",
    "        logger.info(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < {split_end}]\")\n",
    "        exec(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < split_end]\")\n",
    "        exec(f\"df_train_fold{cnt+1}.to_csv('{base_output_dir}/train/train_fold{cnt+1}.csv', index = False)\")\n",
    "\n",
    "        logger.info(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= {split_end}]\")\n",
    "        exec(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= split_end]\")\n",
    "        exec(f\"df_test_fold{cnt+1}.to_csv('{base_output_dir}/test/test_fold{cnt+1}.csv', index = False)\")\n",
    "    \n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    \n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299d9b7-41e1-4f7c-9550-0437e94fa2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-2. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b96b625-2eee-4960-9046-bea2656bb433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.2/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.2/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import joblib # from sklearn.externals import joblib\n",
    "import pickle\n",
    "import tarfile # model registry에는 uri만 등록된다.\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "from logging.config import dictConfig\n",
    "\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_dir\", type=str, default='/opt/ml/processing/input/train')\n",
    "    parser.add_argument(\"--test_dir\", type=str, default='/opt/ml/processing/input/test')\n",
    "    parser.add_argument('--output_dir', type = str, default = '/opt/ml/processing/output')\n",
    "    parser.add_argument('--item', type = str, default = 'FCPOc3')\n",
    "    parser.add_argument('--target', type = str, default = 'y')\n",
    "    parser.add_argument('--metric', type = str, default = 'MAPE')    \n",
    "    parser.add_argument('--quality', type = str, default = 'fast_training')    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_tarfile(source_dir, output_filename=None):\n",
    "    ''' create a tarfile from a source directory'''\n",
    "    if output_filename == None:\n",
    "        output_filename = \"%s/tmptar.tar\" %(tempfile.mkdtemp())\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "    return output_filename \n",
    "\n",
    "def make_tarfile(source_dir, output_filename):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "    return os.path.join(source_dir, output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ############################################\n",
    "    ########## 필요 라이브러리 설치  ###########\n",
    "    ########################################### \n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'autogluon==0.6.1'])\n",
    "    from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "    \n",
    "    ######################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ######################################\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(\"### start training code\")    \n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    args = parse_args()\n",
    "        \n",
    "    logger.info(f\"args.train_dir: {args.train_dir}\")   \n",
    "    logger.info(f\"args.test_dir: {args.test_dir}\")   \n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")    \n",
    "    logger.info(f\"args.item: {args.item}\")   \n",
    "    logger.info(f\"args.target: {args.target}\")    \n",
    "    logger.info(f\"args.metric: {args.metric}\")   \n",
    "    logger.info(f\"args.quality: {args.quality}\")   \n",
    "    \n",
    "    train_dir = args.train_dir\n",
    "    test_dir = args.test_dir\n",
    "    output_dir = args.output_dir\n",
    "    prediction_dir = os.path.join(output_dir, 'prediction')\n",
    "    leaderboard_dir = os.path.join(output_dir, 'leaderboard')\n",
    "    model_dir = os.path.join(output_dir, 'model')\n",
    "    \n",
    "    for path in [prediction_dir, leaderboard_dir, model_dir]:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "    item = args.item\n",
    "    target = args.target\n",
    "    metric = args.metric\n",
    "    quality = args.quality\n",
    "    \n",
    "    trlist = sorted(os.listdir(train_dir))\n",
    "    telist = sorted(os.listdir(test_dir))\n",
    "    \n",
    "    logger.info(f\"the list of train data {trlist}\")\n",
    "    logger.info(f\"the list of train data {telist}\")\n",
    "    \n",
    "    for train_file, test_file in zip(trlist, telist):\n",
    "        logger.info(\"### Reading input data\")\n",
    "        logger.info(f\"### train data: {train_file}\")\n",
    "        logger.info(f\"### test data: {test_file}\")\n",
    "        \n",
    "        df_train = pd.read_csv(os.path.join(train_dir, train_file))\n",
    "        df_test = pd.read_csv(os.path.join(test_dir, test_file))      \n",
    "        # df_train = df_train[df_train['ric'] != 'MCCc3']\n",
    "        # df_test = df_test[df_test['ric'] != 'MCCc3']\n",
    "        \n",
    "        logger.info(\"### Convert TimeSeriesDataFrame\")\n",
    "        df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "        df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "\n",
    "        tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_train,\n",
    "            id_column=\"ric\",\n",
    "            timestamp_column=\"ds\",\n",
    "        )\n",
    "        tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_test,\n",
    "            id_column=\"ric\",\n",
    "            timestamp_column=\"ds\",\n",
    "        )\n",
    "\n",
    "        logger.info(\"### Show the range of date for training and test\")    \n",
    "        logger.info('Item:', item)\n",
    "        logger.info('Target:', target)   \n",
    "        logger.info('Train:',tdf_train.loc[item][target].index.min(),'~',tdf_train.loc[item][target].index.max())\n",
    "        logger.info('Test:',tdf_test.loc[item][target].index.min(),'~',tdf_test.loc[item][target].index.max())\n",
    "        logger.info('The number of test data:',len(tdf_test.loc[item][target]))\n",
    "\n",
    "        logger.info(\"### Training AutoGluon Model\")    \n",
    "        predictor = TimeSeriesPredictor(\n",
    "            path = model_dir,\n",
    "            target = target,\n",
    "            prediction_length = len(tdf_test.loc[item][target]),\n",
    "            eval_metric = metric,\n",
    "        )\n",
    "        predictor.fit(\n",
    "            train_data = tdf_train,\n",
    "            presets = quality\n",
    "        )\n",
    "        logger.info(\"the list of data in model_dir {}\".format(os.listdir(model_dir)))\n",
    "        tar_file_path = make_tarfile(model_dir, f'{model_dir}/model.tar.gz')\n",
    "        logger.info(\"Saving model to {}\".format(tar_file_path))\n",
    "\n",
    "        predictor_leaderboard = predictor.leaderboard(tdf_test, silent = True)\n",
    "        predictor_leaderboard = predictor_leaderboard.sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                  ascending = False)\n",
    "        predictor_leaderboard.to_csv(os.path.join(leaderboard_dir,\n",
    "                                                  f'leaderboard-{test_file}'),\n",
    "                                     index = False)\n",
    "        logger.info(f\"predictor_leaderboard sample: head(2) \\n {predictor_leaderboard.head(2)}\")\n",
    "        \n",
    "        top_model_name = predictor_leaderboard.loc[0, 'model']\n",
    "        # second_model_name = predictor_leaderboard.loc[1, 'model']\n",
    "        \n",
    "        prediction_ag_model_01 = predictor.predict(data = tdf_train,\n",
    "                                                   model = top_model_name)\n",
    "#         prediction_ag_model_02 = predictor.predict(data = tdf_train,\n",
    "#                                                    model = second_model_name)\n",
    "        pred_result_01 = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model_01.loc['FCPOc3'],\n",
    "                                  left_index = True, right_index = True, how = 'left')\n",
    "        pred_result_01.to_csv(os.path.join(prediction_dir,\n",
    "                                           f'pred-{top_model_name}-{test_file}'))   \n",
    "        # pred_result_02 = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model_02.loc['FCPOc3'],\n",
    "        #                           left_index = True, right_index = True, how = 'left')\n",
    "        # pred_result_02.to_csv(os.path.join(prediction_dir,\n",
    "        #                                    f'pred-{second_model_name}-{test_file}'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b928-35ef-4fb7-95b0-fdb7fd00c481",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-3. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eea0bef0-b16a-4b98-8300-d43b99f08419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.2/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.2/model_validation.py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import argparse\n",
    "import json\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import joblib\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import logging.handlers\n",
    "import calendar\n",
    "import tarfile\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"dev/ForecastPalmOilPrice\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "def check_performance_threshold(iput_df : pd.DataFrame,\n",
    "                                identifier: str,\n",
    "                                threshold : float = -100):\n",
    "    tmp = {}\n",
    "    satisfied_df = iput_df[iput_df['score_val'] > threshold]\n",
    "    if len(satisfied_df) > 0:\n",
    "        tmp['identifier'] = identifier\n",
    "        tmp['model'] = list(satisfied_df['model'])\n",
    "        tmp['performance'] = list(satisfied_df['score_val'])\n",
    "    return tmp\n",
    "\n",
    "def get_model_performance_report(data):\n",
    "    result = defaultdict(list)\n",
    "    models_ext = [row[\"model\"] for row in data if row]\n",
    "    models = [item for sublist in models_ext for item in sublist]\n",
    "    performance_ext = [row[\"performance\"] for row in data if row]\n",
    "    performance = [item for sublist in performance_ext for item in sublist]\n",
    "    \n",
    "    count_models = Counter(models)\n",
    "    \n",
    "    for keys, values in zip(models, performance):\n",
    "        result[keys].append(values)\n",
    "\n",
    "    for key, values in result.items():\n",
    "        result[key] = []\n",
    "        result[key].append(count_models[key])\n",
    "        result[key].append(sum(values) / len(values))\n",
    "        result[key].append(np.std(values))\n",
    "    \n",
    "    # 정렬 1순위 : 비즈니스담당자의 Metric에 선정된 Count 높은 순, 2순위: 표준편차가 작은 순(그래서 -처리해줌)\n",
    "    result = sorted(result.items(), key=lambda k_v: (k_v[1][0], -k_v[1][2]), reverse=True) \n",
    "    return result\n",
    "\n",
    "def register_model_in_aws_registry(model_zip_path: str,\n",
    "                                   model_package_group_name: str,\n",
    "                                   model_description: str,\n",
    "                                   model_tags: dict,############################# parameter 추가할것: golden train path와 test path \n",
    "                                   model_status: str,\n",
    "                                   sm_client) -> str:\n",
    "    create_model_package_input_dict = {\n",
    "        \"ModelPackageGroupName\": model_package_group_name,\n",
    "        \"ModelPackageDescription\": model_description, # ex AutoGluon - WeightedEnsemble\n",
    "        \"CustomerMetadataProperties\": model_tags,\n",
    "        \"ModelApprovalStatus\": model_status,\n",
    "        \"InferenceSpecification\": {\n",
    "            \"Containers\": [\n",
    "                {\n",
    "                    \"Image\": '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/autogluon-inference:0.4-cpu-py38',\n",
    "                    \"ModelDataUrl\": model_zip_path\n",
    "                }\n",
    "            ],\n",
    "            \"SupportedContentTypes\": [\"text/csv\"],\n",
    "            \"SupportedResponseMIMETypes\": [\"text/csv\"],\n",
    "        }\n",
    "    }\n",
    "    create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "    model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "    return model_package_arn\n",
    "\n",
    "\n",
    "def register_manifest(source_path,\n",
    "                      target_path,\n",
    "                      s3_client,\n",
    "                      BUCKET_NAME_USECASE):\n",
    "    template_json = {\"fileLocations\": [{\"URIPrefixes\": []}],\n",
    "                     \"globalUploadSettings\": {\n",
    "                         \"format\": \"CSV\",\n",
    "                         \"delimiter\": \",\"\n",
    "                     }}\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    response_iterator = paginator.paginate(Bucket = BUCKET_NAME_USECASE,\n",
    "                                           Prefix = source_path.split(BUCKET_NAME_USECASE+'/')[1]\n",
    "                                          )\n",
    "    for page in response_iterator:\n",
    "        for content in page['Contents']:\n",
    "            template_json['fileLocations'][0]['URIPrefixes'].append(f's3://{BUCKET_NAME_USECASE}/'+content['Key'])\n",
    "    with open(f'./manifest_testing.manifest', 'w') as f:\n",
    "        json.dump(template_json, f, indent=2)\n",
    "\n",
    "    res = s3_client.upload_file('./manifest_testing.manifest',\n",
    "                                BUCKET_NAME_USECASE,\n",
    "                                f\"{target_path.split(BUCKET_NAME_USECASE+'/')[1]}/visual_validation.manifest\")\n",
    "    return f\"{target_path.split(BUCKET_NAME_USECASE+'/')[1]}/visual_validation.manifest\"\n",
    "    \n",
    "def refresh_of_spice_datasets(user_account_id,\n",
    "                              qs_data_name,\n",
    "                              manifest_file_path,\n",
    "                              BUCKET_NAME_USECASE,\n",
    "                              qs_client):\n",
    "    \n",
    "    ds_list = qs_client.list_data_sources(AwsAccountId='108594546720')\n",
    "    datasource_ids = [summary[\"DataSourceId\"] for summary in ds_list[\"DataSources\"] if qs_data_name in summary[\"Name\"]]    \n",
    "    for datasource_id in datasource_ids:\n",
    "        response = qs_client.update_data_source(\n",
    "            AwsAccountId=user_account_id,\n",
    "            DataSourceId=datasource_id,\n",
    "            Name=qs_data_name,\n",
    "            DataSourceParameters={\n",
    "                'S3Parameters': {\n",
    "                    'ManifestFileLocation': {\n",
    "                        'Bucket': BUCKET_NAME_USECASE,\n",
    "                        'Key':  manifest_file_path\n",
    "                    },\n",
    "                },\n",
    "            })\n",
    "        logger.info(f\"datasource_id:{datasource_id} 의 manifest를 업데이트: {response}\")\n",
    "    \n",
    "    res = qs_client.list_data_sets(AwsAccountId = user_account_id)\n",
    "    datasets_ids = [summary[\"DataSetId\"] for summary in res[\"DataSetSummaries\"] if qs_data_name in summary[\"Name\"]]\n",
    "    ingestion_ids = []\n",
    "\n",
    "    for dataset_id in datasets_ids:\n",
    "        try:\n",
    "            ingestion_id = str(calendar.timegm(time.gmtime()))\n",
    "            qs_client.create_ingestion(DataSetId = dataset_id,\n",
    "                                       IngestionId = ingestion_id,\n",
    "                                       AwsAccountId = user_account_id)\n",
    "            ingestion_ids.append(ingestion_id)\n",
    "        except Exception as e:\n",
    "            logger.info(e)\n",
    "            pass\n",
    "    for ingestion_id, dataset_id in zip(ingestion_ids, datasets_ids):\n",
    "        while True:\n",
    "            response = qs_client.describe_ingestion(DataSetId = dataset_id,\n",
    "                                                    IngestionId = ingestion_id,\n",
    "                                                    AwsAccountId = user_account_id)\n",
    "            if response['Ingestion']['IngestionStatus'] in ('INITIALIZED', 'QUEUED', 'RUNNING'):\n",
    "                time.sleep(5)     #change sleep time according to your dataset size\n",
    "            elif response['Ingestion']['IngestionStatus'] == 'COMPLETED':\n",
    "                print(\"refresh completed. RowsIngested {0}, RowsDropped {1}, IngestionTimeInSeconds {2}, IngestionSizeInBytes {3}\".format(\n",
    "                    response['Ingestion']['RowInfo']['RowsIngested'],\n",
    "                    response['Ingestion']['RowInfo']['RowsDropped'],\n",
    "                    response['Ingestion']['IngestionTimeInSeconds'],\n",
    "                    response['Ingestion']['IngestionSizeInBytes']))\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"refresh failed for {0}! - status {1}\".format(dataset_id,\n",
    "                                                                          response['Ingestion']['IngestionStatus']))\n",
    "                break\n",
    "    return response\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--leaderboard_path', type=str, default=\"/opt/ml/processing/input/leaderboard\")   \n",
    "    parser.add_argument('--algorithm_name', type=str)\n",
    "    parser.add_argument('--model_base_path', type=str)\n",
    "    parser.add_argument('--manifest_base_path', type=str)\n",
    "    parser.add_argument('--prediction_base_path', type=str)\n",
    "    parser.add_argument('--threshold', type=str, default=\"-100\")   \n",
    "    parser.add_argument('--model_package_group_name', type=str, default = BUCKET_NAME_USECASE)  \n",
    "    parser.add_argument('--qs_data_name', type=str, default = 'model_result')    \n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['AWS_ACCESS_KEY_ID']\n",
    "    ACCESS_SECRET_KEY = keychain['AWS_ACCESS_SECRET_KEY']\n",
    "    BUCKET_NAME_USECASE = keychain['PROJECT_BUCKET_NAME']\n",
    "    DATALAKE_BUCKET_NAME = keychain['DATALAKE_BUCKET_NAME']\n",
    "    S3_PATH_REUTER = keychain['S3_PATH_REUTER']\n",
    "    S3_PATH_WWO = keychain['S3_PATH_WWO']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_FORECAST = keychain['S3_PATH_PREDICTION']\n",
    "    \n",
    "    boto3_session = boto3.Session(aws_access_key_id = ACCESS_KEY_ID,\n",
    "                                  aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                                  region_name = 'ap-northeast-2')\n",
    "    \n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3_session.client('sagemaker')\n",
    "    qs_client = boto3_session.client('quicksight')\n",
    "\n",
    "    sts_client = boto3_session.client(\"sts\")\n",
    "    user_account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "    ######################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ######################################\n",
    "    args = parse_args()\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(\"### start training code\")    \n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    logger.info(f\"args.algorithm_name: {args.algorithm_name}\")    \n",
    "    logger.info(f\"args.leaderboard_path: {args.leaderboard_path}\")    \n",
    "    logger.info(f\"args.model_base_path: {args.model_base_path}\")\n",
    "    logger.info(f\"args.manifest_base_path: {args.manifest_base_path}\")\n",
    "    logger.info(f\"args.prediction_base_path: {args.prediction_base_path}\")\n",
    "    logger.info(f\"args.threshold: {args.threshold}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "    logger.info(f\"args.qs_data_name: {args.qs_data_name}\")\n",
    "  \n",
    "    algorithm_name = args.algorithm_name\n",
    "    leaderboard_path = args.leaderboard_path\n",
    "    model_base_path = args.model_base_path\n",
    "    manifest_base_path = args.manifest_base_path\n",
    "    prediction_base_path = args.prediction_base_path\n",
    "    threshold = float(args.threshold)\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    qs_data_name = args.qs_data_name\n",
    "    \n",
    "    lb_list = sorted(os.listdir(leaderboard_path))\n",
    "    logger.info(f\"leaderboard file list in {leaderboard_path}: {lb_list}\")\n",
    "    satisfied_info = []\n",
    "    train_data_base_path = manifest_base_path\n",
    "    test_data_base_path = manifest_base_path\n",
    "    train_replace_dict = {'trained-model' : S3_PATH_GOLDEN,\n",
    "                          'manifest' : 'train'}\n",
    "    for key in train_replace_dict.keys():\n",
    "        train_data_base_path = train_data_base_path.replace(key, train_replace_dict[key])\n",
    "    test_replace_dict = {'trained-model' : S3_PATH_GOLDEN,\n",
    "                          'manifest' : 'test'}\n",
    "    for key in test_replace_dict.keys():\n",
    "        test_data_base_path = test_data_base_path.replace(key, test_replace_dict[key])\n",
    "        \n",
    "    for idx, f_path in enumerate(lb_list):\n",
    "        leaderboard = pd.read_csv(f'{leaderboard_path}/{f_path}').sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                              ascending = False)\n",
    "        satisfied_info.append(check_performance_threshold(iput_df = leaderboard,\n",
    "                                                          identifier = f'fold{idx}',\n",
    "                                                          threshold = threshold))\n",
    "    model_report = get_model_performance_report(satisfied_info)\n",
    "    logger.info(f\"\\n####model_report: {model_report}\")\n",
    "    if model_report[0][1][0] == len(lb_list): # Fold 내 모든 성능이 비즈니스 담당자가 설정한 값을 만족한다면\n",
    "        logger.info(f\"\\n#### Pass the 1st minimum performance valiation\")\n",
    "        manifest_file_path = register_manifest(prediction_base_path, \n",
    "                                               manifest_base_path,\n",
    "                                               s3_client,\n",
    "                                               BUCKET_NAME_USECASE)\n",
    "\n",
    "        model_package_arn = register_model_in_aws_registry(model_zip_path = f\"{model_base_path}/model.tar.gz\",\n",
    "                                                           model_package_group_name = model_package_group_name,\n",
    "                                                           model_description = algorithm_name,\n",
    "                                                           model_tags = {'champion_model' : str(model_report[0][0]),\n",
    "                                                                         'passed_the_number_of_folds' : str(model_report[0][1][0]),\n",
    "                                                                         'average_metric' : str(model_report[0][1][1]),\n",
    "                                                                         'std_metric' : str(model_report[0][1][2]),\n",
    "                                                                         'train_data' : str(train_data_base_path),\n",
    "                                                                         'test_data' : str(test_data_base_path),\n",
    "                                                                        },\n",
    "                                                           model_status = 'PendingManualApproval',\n",
    "                                                           sm_client = sm_client)\n",
    "        logger.info('### Passed ModelPackage Version ARN : {}'.format(model_package_arn))\n",
    "        logger.info('### manifest_file_path : {}'.format(manifest_file_path))\n",
    "        res = refresh_of_spice_datasets(user_account_id,\n",
    "                                        qs_data_name,\n",
    "                                        manifest_file_path,\n",
    "                                        BUCKET_NAME_USECASE,\n",
    "                                        qs_client)\n",
    "        logger.info('### refresh_of_spice_datasets : {}'.format(res))\n",
    "    else:\n",
    "        logger.info(f\"\\n#### Filtered at 1st valiation\")\n",
    "        model_package_arn = register_model_in_aws_registry(model_zip_path = f\"{model_base_path}/model.tar.gz\",\n",
    "                                                           model_package_group_name = model_package_group_name,\n",
    "                                                           model_description = algorithm_name,\n",
    "                                                           model_tags = {'champion_model' : str(model_report[0][0]),\n",
    "                                                                         'passed_the_number_of_folds' : str(model_report[0][1][0]),\n",
    "                                                                         'average_metric' : str(model_report[0][1][1]),\n",
    "                                                                         'std_metric' : str(model_report[0][1][2]),\n",
    "                                                                         'train_data' : str(train_data_base_path),\n",
    "                                                                         'test_data' : str(test_data_base_path),\n",
    "                                                                        },\n",
    "                                                           model_status = 'Rejected',\n",
    "                                                           sm_client = sm_client)\n",
    "        logger.info('### Rejected ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc0e0a39-3486-45f3-a428-15c90d6f93c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"champion_model\": \"WeightedEnsemble\", \"passed_the_number_of_folds\": 3, \"average_metric\": -0.07664768989270467, \"std_metric\": 0.010357877348928431}'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# json.dumps({'champion_model' : model_report[0][0],\n",
    "#             'passed_the_number_of_folds' : model_report[0][0][0],\n",
    "#             'average_metric' : model_report[0][0][1],\n",
    "#             'std_metric' : model_report[0][0][2],})       \n",
    "json.dumps({'champion_model' : 'WeightedEnsemble',\n",
    "            'passed_the_number_of_folds' : 3,\n",
    "            'average_metric' : -0.07664768989270467,\n",
    "            'std_metric' : 0.010357877348928431})       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43ea47-dcc7-4973-8303-956c65f5f7c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce81ec-fdd6-41b0-8a82-2465c664cc31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 라이브러리 및 변수 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d65ec2-afc2-4954-ad35-7f59fd097b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import sagemaker\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6be1cb17-7559-4e20-b593-7f850b3daca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2023-03-16 20:43:49.766999\n"
     ]
    }
   ],
   "source": [
    "# 한국 시간\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "print(f\"Start job time: {KST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "017b0400-5458-4c22-86b2-dd494f5a215c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code version: 1.2\n"
     ]
    }
   ],
   "source": [
    "# 코드 버전\n",
    "code_version = '1.2'\n",
    "print(f\"Code version: {code_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c32c0e4-17bb-44f1-944a-c78ade63795b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    secret_name = \"dev/ForecastPalmOilPrice\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name,\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "keychain = json.loads(get_secret())\n",
    "ACCESS_KEY_ID = keychain['AWS_ACCESS_KEY_ID']\n",
    "ACCESS_SECRET_KEY = keychain['AWS_ACCESS_SECRET_KEY']\n",
    "BUCKET_NAME_USECASE = keychain['PROJECT_BUCKET_NAME']\n",
    "DATALAKE_BUCKET_NAME = keychain['DATALAKE_BUCKET_NAME']\n",
    "S3_PATH_REUTER = keychain['S3_PATH_REUTER']\n",
    "S3_PATH_WWO = keychain['S3_PATH_WWO']\n",
    "S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "S3_PATH_FORECAST = keychain['S3_PATH_PREDICTION']\n",
    "\n",
    "boto3_session = boto3.Session(aws_access_key_id = ACCESS_KEY_ID,\n",
    "                              aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                              region_name = 'ap-northeast-2')\n",
    "\n",
    "s3_client = boto3_session.client('s3')\n",
    "sm_client = boto3_session.client('sagemaker')\n",
    "qs_client = boto3_session.client('quicksight')\n",
    "\n",
    "sts_client = boto3_session.client(\"sts\")\n",
    "user_account_id = sts_client.get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231675a-a0e4-49ad-bb2c-3bf40ccff01d",
   "metadata": {},
   "source": [
    "노트북에 저장된 변수를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba1eda-81d5-4df9-9e5c-b6682982b8fa",
   "metadata": {},
   "source": [
    "# 2. 모델 빌딩 파이프라인 의 스텝(Step) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9d664-d5a1-44a1-a249-cbd1aa294a7e",
   "metadata": {},
   "source": [
    "## 2.1 모델 빌딩 파이프라인 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eed86952-8498-4d96-b359-3d23461042b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: src/v1.2/model_validation.py to s3://crude-palm-oil-prices-forecast/src/model_validation.py\n",
      "upload: src/v1.2/train.py to s3://crude-palm-oil-prices-forecast/src/train.py\n",
      "upload: src/v1.2/visualization.py to s3://crude-palm-oil-prices-forecast/src/visualization.py\n",
      "upload: src/v1.2/preprocessing.py to s3://crude-palm-oil-prices-forecast/src/preprocessing.py\n",
      "upload: src/v1.2/prediction.py to s3://crude-palm-oil-prices-forecast/src/prediction.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp 'src/v1.2' 's3://crude-palm-oil-prices-forecast/src' --recursive --exclude \".ipynb_checkpoints*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7d15ee3-98aa-4c58-819b-5ea7479715aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_version: 1.2\n",
      "preprocessing_code: s3://crude-palm-oil-prices-forecast/src/preprocessing.py\n",
      "training_code: s3://crude-palm-oil-prices-forecast/src/train.py\n",
      "model_validation_code: s3://crude-palm-oil-prices-forecast/src/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "preprocessing_code = 's3://crude-palm-oil-prices-forecast/src/preprocessing.py'\n",
    "training_code = 's3://crude-palm-oil-prices-forecast/src/train.py'\n",
    "model_validation_code = 's3://crude-palm-oil-prices-forecast/src/model_validation.py'\n",
    "print('code_version:',code_version)\n",
    "print('preprocessing_code:',preprocessing_code)\n",
    "print('training_code:',training_code)\n",
    "print('model_validation_code:',model_validation_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3eaa7cf-ebeb-4438-9a18-d5fe1bf12971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2023-03-17 17:43:09.097379\n",
      "code verison: 1.2\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "###################\n",
    "# 0) 변수 선언 ###\n",
    "##################\n",
    "# 한국 시간\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "KST_aday_before = KST - relativedelta(days=1) \n",
    "yyyy, mm, dd = str(KST_aday_before.year), str(KST_aday_before.month).zfill(2), str(KST_aday_before.day).zfill(2)\n",
    "\n",
    "split_start = '2018-01-01'\n",
    "split_end = KST.strftime('%Y-%m-%d')\n",
    "timestamp = time.mktime(KST.timetuple())\n",
    "\n",
    "# 프로젝트 변수\n",
    "project_prefix = BUCKET_NAME_USECASE\n",
    "usecase_base_path = os.path.join('s3://', BUCKET_NAME_USECASE)\n",
    "datalake_base_path = os.path.join('s3://', DATALAKE_BUCKET_NAME, BUCKET_NAME_USECASE)\n",
    "raw_data_path = os.path.join(datalake_base_path, 'EikonDataAPI')\n",
    "\n",
    "# S3 디렉토리 위치(data path)\n",
    "staged_data_dir = os.path.join(usecase_base_path, keychain['S3_PATH_STAGE'], KST_aday_before.strftime('%Y/%m/%d'),str(timestamp))\n",
    "golden_data_dir = os.path.join(usecase_base_path, keychain['S3_PATH_GOLDEN'], KST_aday_before.strftime('%Y/%m/%d'),str(timestamp))\n",
    "trained_model_dir = os.path.join(usecase_base_path, keychain['S3_PATH_TRAIN'], KST_aday_before.strftime('%Y/%m/%d'),str(timestamp))\n",
    "\n",
    "# 전처리 결과 데이터 위치(Golden data path)\n",
    "train_data_dir = os.path.join(golden_data_dir,'train')\n",
    "test_data_dir = os.path.join(golden_data_dir,'test')\n",
    "scaler_data_dir = os.path.join(golden_data_dir,'scaler-files')\n",
    "\n",
    "num_fold = '3'\n",
    "scaler_switch = '1' # '0': scaling, '1': scaling\n",
    "print(f\"Start job time: {KST}\")\n",
    "print(f\"code verison: {code_version}\")\n",
    "\n",
    "###################\n",
    "## 1) 데이터 전처리를 위한 파이프라인 변수  ####################################\n",
    "###################\n",
    "processing_instance_type = ParameterString(\n",
    "    name = \"ProcessingInstanceType\",\n",
    "    default_value = \"ml.c5.xlarge\" # cpu 성능이 더 중요하기 때문에 m5보다 비교적 가격이 저렴한 c5.xlarge를 선택하였다.\n",
    ")\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name = \"ProcessingInstanceCount\",\n",
    "    default_value = 1\n",
    ")\n",
    "###################\n",
    "## 2) 데이터 학습을 위한 파이프라인 변수  ####################################\n",
    "###################\n",
    "train_instance_type = ParameterString(\n",
    "    name = \"TrainingInstanceType\",\n",
    "    default_value = \"ml.m5.xlarge\"\n",
    ")\n",
    "train_instance_count = ParameterInteger(\n",
    "    name = \"TrainInstanceCount\",\n",
    "    default_value = 1\n",
    ")\n",
    "###################\n",
    "## 3) 모델 검증을 위한 파이프라인 변수  ####################################\n",
    "###################\n",
    "model_validation_instance_count = ParameterInteger(\n",
    "    name=\"ModelValidationInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "model_validation_instance_type = ParameterString(\n",
    "    name=\"ModelValidationInstanceType\",\n",
    "    default_value='ml.c5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9497ace7-b074-449f-b652-2ec25023acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "# cache_config = CacheConfig(enable_caching=True, \n",
    "#                            expire_after=\"7d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa50b16-6c49-4b1f-b0d4-b2ebfb36683c",
   "metadata": {},
   "source": [
    "## 2.3 프로세서 단계 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "038bde6e-fff9-4e3b-8d90-e3ca5115e57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = list({'FCPOc3':'1',\n",
    "          'b': 2,})\n",
    "b = [{'FCPOc3':'1',\n",
    "          'b': 2,}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d9168e0-ecf7-4176-b32e-97c8fa092f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FCPOc3', 'b']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34b5f025-0c19-40dd-a934-f61f76a47a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'FCPOc3': '1', 'b': 2}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6177b3b-24cb-4690-81e2-1aa8ee5747a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "skframework_version = \"1.0-1\"#\"0.23-1\"\n",
    "item = 'FCPOc3'\n",
    "target = 'y'\n",
    "metric = 'MAPE'\n",
    "quality = 'medium_quality'#'medium_quality'#'fast_training'\n",
    "###################\n",
    "# 1) 데이터 전처리 ###\n",
    "##################\n",
    "skprocessor_preprocessing = SKLearnProcessor(\n",
    "    framework_version = skframework_version,\n",
    "    instance_type = \"ml.c5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    base_job_name = f\"{BUCKET_NAME_USECASE}(Preprocessing)\",\n",
    "    role = role,\n",
    ")\n",
    "\n",
    "################\n",
    "# 2) 모델 학습 ###\n",
    "###############\n",
    "image_uri = retrieve(framework='mxnet',\n",
    "                     region='ap-northeast-2',\n",
    "                     version='1.9.0',\n",
    "                     py_version='py38',\n",
    "                     image_scope='training',\n",
    "                     instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "script_processor_training = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=image_uri,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    base_job_name = f\"{BUCKET_NAME_USECASE}(Train Model)\",\n",
    "    role = role,\n",
    ")\n",
    "################\n",
    "# 3) 모델 검증 ###\n",
    "###############\n",
    "skprocessor_model_validation = SKLearnProcessor(\n",
    "    framework_version = skframework_version,\n",
    "    instance_type = \"ml.c5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    base_job_name = f\"{BUCKET_NAME_USECASE}(Model Validation)\",\n",
    "    role = role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557e393-90b6-49a5-9a1e-89218ee04fbf",
   "metadata": {},
   "source": [
    "## 2.4 파이프라인 스텝 단계 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5f3bb63-c9f0-4fe1-b676-34ed600d5414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# 1) 데이터 전처리 ###\n",
    "##################\n",
    "step_preprocessing = ProcessingStep(\n",
    "    name = f\"{BUCKET_NAME_USECASE}-Preprocessing\",\n",
    "    processor = skprocessor_preprocessing,\n",
    "    inputs = [\n",
    "        ProcessingInput(input_name = 'input_stage_data_path',\n",
    "                        source = raw_data_path,\n",
    "                        destination = '/opt/ml/processing/input'),\n",
    "    ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name = \"stage\",\n",
    "                         source = '/opt/ml/processing/output/stage',\n",
    "                         destination = staged_data_dir),\n",
    "        ProcessingOutput(output_name = \"scaler\",\n",
    "                         source = '/opt/ml/processing/output/scaler',\n",
    "                         destination = scaler_data_dir),        \n",
    "        ProcessingOutput(output_name = \"train\",\n",
    "                         source = '/opt/ml/processing/output/train',\n",
    "                         destination = train_data_dir),\n",
    "        ProcessingOutput(output_name = \"test\",\n",
    "                         source = '/opt/ml/processing/output/test',\n",
    "                         destination = test_data_dir),\n",
    "    ],\n",
    "    job_arguments = [\"--split_start\", split_start,\n",
    "                     \"--split_end\", split_end,\n",
    "                     \"--num_fold\", num_fold,\n",
    "                     \"--scaler_switch\", scaler_switch], \n",
    "    code = preprocessing_code,\n",
    ")\n",
    "################\n",
    "# 2) 모델 학습 ###\n",
    "###############\n",
    "\n",
    "step_train = ProcessingStep(\n",
    "    name = f\"{BUCKET_NAME_USECASE}-Training\",\n",
    "    processor = script_processor_training,\n",
    "    inputs=[\n",
    "        ProcessingInput(source = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                        destination = \"/opt/ml/processing/input/train\"),\n",
    "        ProcessingInput(source = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                        destination = \"/opt/ml/processing/input/test\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name = \"prediction_data\",\n",
    "                         source = \"/opt/ml/processing/output/prediction\",\n",
    "                         destination = f'{trained_model_dir}/prediction'),\n",
    "        ProcessingOutput(output_name = \"leaderboard_data\",\n",
    "                         source = \"/opt/ml/processing/output/leaderboard\",\n",
    "                         destination = f'{trained_model_dir}/leaderboard'),        \n",
    "        ProcessingOutput(output_name = \"model_data\",\n",
    "                         source = \"/opt/ml/processing/output/model\",\n",
    "                         destination = f'{trained_model_dir}/model'),        \n",
    "        ProcessingOutput(output_name = \"manifest_data\",\n",
    "                         source = \"/opt/ml/processing/output/manifest\",\n",
    "                         destination = f'{trained_model_dir}/manifest')\n",
    "        ],\n",
    "    job_arguments = [\"--item\", item,\n",
    "                     \"--target\", target,\n",
    "                     \"--metric\", metric,\n",
    "                     \"--quality\", quality],\n",
    "    code = training_code\n",
    ")\n",
    "\n",
    "\n",
    "################\n",
    "# 3) 모델 검증 ###\n",
    "###############\n",
    "# 참조:https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html#API_DescribeTrainingJob_ResponseSyntax\n",
    "step_model_validaion = ProcessingStep(\n",
    "    name = f\"{project_prefix}-Model_validation\",\n",
    "    processor = skprocessor_model_validation,\n",
    "    inputs=[\n",
    "        ProcessingInput(source = step_train.properties.ProcessingOutputConfig.Outputs[\"leaderboard_data\"].S3Output.S3Uri,\n",
    "                        destination = \"/opt/ml/processing/input/leaderboard\")\n",
    "    ],\n",
    "    job_arguments = [\"--algorithm_name\", 'Autogluon',\n",
    "                     \"--model_base_path\", step_train.properties.ProcessingOutputConfig.Outputs[\"model_data\"].S3Output.S3Uri,\n",
    "                     \"--manifest_base_path\", step_train.properties.ProcessingOutputConfig.Outputs[\"manifest_data\"].S3Output.S3Uri,\n",
    "                     \"--prediction_base_path\", step_train.properties.ProcessingOutputConfig.Outputs[\"prediction_data\"].S3Output.S3Uri,\n",
    "                     \"--threshold\", \"-100\",\n",
    "                     \"--model_package_group_name\", BUCKET_NAME_USECASE,\n",
    "                     \"--qs_data_name\", \"model_result\",\n",
    "              ],\n",
    "    code = model_validation_code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b079639-15f9-4a02-93eb-da899709bf5a",
   "metadata": {},
   "source": [
    "## 2.5 최종 파이프라인 정의 및 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89624b5-bc17-4677-b1e6-660fd328c49e",
   "metadata": {},
   "source": [
    "### 최종 파이프라인 정의\n",
    "1. Processing: staged data내에서 데이터를 추출하여 데이터 통합 그리고 데이터 전처리 진행\n",
    "2. Training: \n",
    "3. Model Validation:\n",
    "4. Model Prediction(Infernece):\n",
    "     - 굳이 Endpoint를 생성할 필요가 없다.\n",
    "     \n",
    "참조하자: [MLOps Pipeline](https://aws.amazon.com/ko/blogs/machine-learning/deploy-an-mlops-solution-that-hosts-your-model-endpoints-in-aws-lambda/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a1ce443-0600-42ad-b997-b3acf5bace2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = project_prefix\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        # 1) preprocessing's parameters \n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        # 2) training's parameters        \n",
    "        train_instance_type,        \n",
    "        train_instance_count,   \n",
    "        # 3) model validating's parameters\n",
    "        model_validation_instance_type,\n",
    "        model_validation_instance_count,\n",
    "    ],\n",
    "   steps=[step_preprocessing,\n",
    "          step_train,\n",
    "          step_model_validaion]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221059ed-17dc-475e-a866-1f4f5ff627f9",
   "metadata": {},
   "source": [
    "### 파이프라인 정의 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5f889ea-c4ba-4984-b2e6-8ed8139a1c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'TrainInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'ModelValidationInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'ModelValidationInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'crude-palm-oil-prices-forecast-Preprocessing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "     'ContainerArguments': ['--split_start',\n",
       "      '2018-01-01',\n",
       "      '--split_end',\n",
       "      '2023-03-16',\n",
       "      '--num_fold',\n",
       "      '3',\n",
       "      '--scaler_switch',\n",
       "      '1'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "    'ProcessingInputs': [{'InputName': 'input_stage_data_path',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://ai-data-lake/crude-palm-oil-prices-forecast/EikonDataAPI',\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'stage',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/staged-data/2023/03/15/1678999655.0',\n",
       "        'LocalPath': '/opt/ml/processing/output/stage',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'scaler',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/15/1678999655.0/scaler-files',\n",
       "        'LocalPath': '/opt/ml/processing/output/scaler',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/15/1678999655.0/train',\n",
       "        'LocalPath': '/opt/ml/processing/output/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/15/1678999655.0/test',\n",
       "        'LocalPath': '/opt/ml/processing/output/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'crude-palm-oil-prices-forecast-Training',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/mxnet-training:1.9.0-cpu-py38',\n",
       "     'ContainerArguments': ['--item',\n",
       "      'FCPOc3',\n",
       "      '--target',\n",
       "      'y',\n",
       "      '--metric',\n",
       "      'MAPE',\n",
       "      '--quality',\n",
       "      'medium_quality'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/train.py']},\n",
       "    'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.crude-palm-oil-prices-forecast-Preprocessing.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/train',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.crude-palm-oil-prices-forecast-Preprocessing.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/train.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'prediction_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/15/1678999655.0/prediction',\n",
       "        'LocalPath': '/opt/ml/processing/output/prediction',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'leaderboard_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/15/1678999655.0/leaderboard',\n",
       "        'LocalPath': '/opt/ml/processing/output/leaderboard',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'model_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/15/1678999655.0/model',\n",
       "        'LocalPath': '/opt/ml/processing/output/model',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'manifest_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/15/1678999655.0/manifest',\n",
       "        'LocalPath': '/opt/ml/processing/output/manifest',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'crude-palm-oil-prices-forecast-Model_validation',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "     'ContainerArguments': ['--algorithm_name',\n",
       "      'Autogluon',\n",
       "      '--model_base_path',\n",
       "      {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['model_data'].S3Output.S3Uri\"},\n",
       "      '--manifest_base_path',\n",
       "      {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['manifest_data'].S3Output.S3Uri\"},\n",
       "      '--prediction_base_path',\n",
       "      {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['prediction_data'].S3Output.S3Uri\"},\n",
       "      '--threshold',\n",
       "      '-100',\n",
       "      '--model_package_group_name',\n",
       "      'crude-palm-oil-prices-forecast',\n",
       "      '--qs_data_name',\n",
       "      'model_result'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/model_validation.py']},\n",
       "    'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['leaderboard_data'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/leaderboard',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/model_validation.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}]}}]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f77f33-f76e-442d-aaef-88f5e3956b9e",
   "metadata": {},
   "source": [
    "### 파이프라인 정의를 제출하고 실행하기\n",
    "- 요청만 하고 기다리진 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bcdb8632-f94b-4b2b-a353-5bc0f9c50006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'golden-data'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_PATH_GOLDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24d24151-cf61-4b65-ad29-4d4c3cd52479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://crude-palm-oil-prices-forecast/golden-data/2023/03/15/1678999655.0/train'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/15/1678999655.0/leaderboard'\n",
    "replace_dict = {'trained-model' : S3_PATH_GOLDEN,\n",
    "             'leaderboard' : 'train'}\n",
    "\n",
    "for key in replace_dict.keys():\n",
    "    txt = txt.replace(key, replace_dict[key])\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d22ff4-d0cb-4178-8d59-7b0e6838797d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "pipeline.upsert(role_arn = sagemaker.get_execution_role())\n",
    "execution = pipeline.start()\n",
    "#실행이 완료될 때까지 기다린다.\n",
    "execution.wait() \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ead1a434-b86c-4397-904a-2b3b37cb9ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터 전처리~모델 검증시간 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mend\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터 전처리~모델 검증시간 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((end\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'end' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"데이터 전처리~모델 검증시간 : {end - start:.1f} sec\")\n",
    "print(f\"데이터 전처리~모델 검증시간 : {((end - start)/60):.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9753aed-e16b-44c0-9178-e17467b5e710",
   "metadata": {},
   "source": [
    "[22년 11월 30일 1차 테스트]    \n",
    "- 데이터 전처리~모델 검증 시간(초) : 1688.9 sec\n",
    "- 데이터 전처리~모델 검증 시간(분) : 28.1 min   \n",
    "\n",
    "[22년 11월 30일 2차 테스트]    \n",
    "- 데이터 전처리~모델 검증 시간(초) : 1719.9 sec\n",
    "- 데이터 전처리~모델 검증 시간(분) : 28.7 min\n",
    "\n",
    "[22년 12월 20일 1차 테스트]    \n",
    "- 데이터 전처리~모델 검증 시간(초) : 1749.8 sec\n",
    "- 데이터 전처리~모델 검증 시간(분) : 29.2 min\n",
    "---\n",
    "New! [23년 3월 1일 테스트]    \n",
    "- 데이터 전처리 시간(초) = 20m22s\n",
    "- 모델링 시간(초) = 12m54s\n",
    "- 모델 검증 시간(초) = 4m22s\n",
    "- 총계: 37m42s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cb8c9-62f1-41b1-909c-832285bdcb6c",
   "metadata": {},
   "source": [
    "### Debug Model Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5f1efc9-3f08-4d28-8d8c-9b8e0faf8581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:pipeline/crude-palm-oil-prices-forecast',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:pipeline/crude-palm-oil-prices-forecast/execution/7dnj1nllyv4i',\n",
       " 'PipelineExecutionDisplayName': 'execution-1677798340834',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'crude-palm-oil-prices-forecast',\n",
       "  'TrialName': '7dnj1nllyv4i'},\n",
       " 'CreationTime': datetime.datetime(2023, 3, 2, 23, 5, 40, 736000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 3, 2, 23, 41, 35, 797000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '3a8c2ae5-ba0c-4139-bbb0-7e74d4d9be74',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3a8c2ae5-ba0c-4139-bbb0-7e74d4d9be74',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '541',\n",
       "   'date': 'Fri, 03 Mar 2023 04:40:46 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09f174e9-f39a-4973-af7f-dbc398840e11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineSummaries': [{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:pipeline/crude-palm-oil-prices-forecast',\n",
       "   'PipelineName': 'crude-palm-oil-prices-forecast',\n",
       "   'PipelineDisplayName': 'crude-palm-oil-prices-forecast',\n",
       "   'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "   'CreationTime': datetime.datetime(2023, 1, 5, 23, 2, 43, 874000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2023, 3, 2, 23, 41, 35, 801000, tzinfo=tzlocal())}],\n",
       " 'ResponseMetadata': {'RequestId': '4a36f29d-0570-4ee9-b37a-8defeb2b1e91',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4a36f29d-0570-4ee9-b37a-8defeb2b1e91',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '400',\n",
       "   'date': 'Fri, 03 Mar 2023 06:41:23 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.list_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36c4b365-3adc-4c86-8146-64ca2d4ea645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'crude-palm-oil-prices-forecast-Model_validation',\n",
       "  'StartTime': datetime.datetime(2023, 3, 2, 23, 37, 9, 131000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2023, 3, 2, 23, 41, 35, 522000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-7dnj1nllyv4i-crude-palm-oil-price-emdi4envfd'}}},\n",
       " {'StepName': 'crude-palm-oil-prices-forecast-Training',\n",
       "  'StartTime': datetime.datetime(2023, 3, 2, 23, 24, 38, 948000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2023, 3, 2, 23, 37, 8, 548000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-7dnj1nllyv4i-crude-palm-oil-price-dl2emknrlo'}}},\n",
       " {'StepName': 'crude-palm-oil-prices-forecast-Preprocessing',\n",
       "  'StartTime': datetime.datetime(2023, 3, 2, 23, 5, 41, 693000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2023, 3, 2, 23, 24, 38, 307000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-7dnj1nllyv4i-crude-palm-oil-price-h1zgdgd5w6'}}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c52e7329-d8c9-4c85-a2ab-3d584299c3af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TrainingJob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_response \u001b[38;5;241m=\u001b[39m execution\u001b[38;5;241m.\u001b[39mlist_steps()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m train_arn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMetadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrainingJob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# index -1은 가장 처음 실행 step\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train_job_name \u001b[38;5;241m=\u001b[39m train_arn\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Processing job name만 추출\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_response \u001b[38;5;241m=\u001b[39m sm_client\u001b[38;5;241m.\u001b[39mdescribe_training_job(TrainingJobName \u001b[38;5;241m=\u001b[39m train_job_name)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TrainingJob'"
     ]
    }
   ],
   "source": [
    "train_response = execution.list_steps()[2]\n",
    "train_arn = train_response['Metadata']['TrainingJob']['Arn'] # index -1은 가장 처음 실행 step\n",
    "train_job_name = train_arn.split('/')[-1] # Processing job name만 추출\n",
    "train_response = sm_client.describe_training_job(TrainingJobName = train_job_name)\n",
    "train_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6ec5e-1baf-4c1e-b058-07408327f6a8",
   "metadata": {},
   "source": [
    "# 3. Future Works\n",
    "참고사항: 앞으로 얼마나 예측을 할것인지에 대해서 split_date를 설정하여 앞으로 몇일을 예측할 수 있다.(현재 분기점은 '2022-10-31' 기준으로 되어있다.)\n",
    "\n",
    "- 1 iter때 Prediction까지 진행을 하고 Model Registry 내 PendingManualApproval 모델이 저장이되면, 예측값과 실제값을 Quicksight 내 보여주고 Approve시 PendingManualApproval -> Approve 상태 전환\n",
    "    => 사내에서는 보안 이슈로 인해서 data 업로드가 어려움\n",
    "- Model Registry 내 model 상태가 업데이트 되면, Deployment 파이프라인 수행\n",
    "- CodeCommit을 통한 소스코드 관리\n",
    "    => 이거 첫번째로 찍먹해보자\n",
    "\n",
    "- Multi-model을 적용하는 파이프라인\n",
    "    => "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfed34-8a55-4637-b5f9-18cf9af8eaef",
   "metadata": {},
   "source": [
    "# 4. Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e9ed4e8-6c88-4495-9afb-62fc3dd1d093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('pipeline_definition.json') as f:\n",
    "    pipeline_definition = json.loads(f.read())\n",
    "# pipeline_definition = pipeline_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9a0fa4c-b944-43c3-8514-3e99b1cc41bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "   'InstanceCount': 1,\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "  'ContainerArguments': ['--split_start',\n",
       "   '2018-01-01',\n",
       "   '--split_end',\n",
       "   '2023-03-14',\n",
       "   '--num_fold',\n",
       "   '3',\n",
       "   '--scaler_switch',\n",
       "   '1'],\n",
       "  'ContainerEntrypoint': ['python3',\n",
       "   '/opt/ml/processing/input/code/preprocessing.py']},\n",
       " 'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       " 'ProcessingInputs': [{'InputName': 'input_stage_data_path',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://ai-data-lake/crude-palm-oil-prices-forecast/EikonDataAPI',\n",
       "    'LocalPath': '/opt/ml/processing/input',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'code',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/preprocessing.py',\n",
       "    'LocalPath': '/opt/ml/processing/input/code',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'stage',\n",
       "    'AppManaged': False,\n",
       "    'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/staged-data/2023/03/02/1677830737.0',\n",
       "     'LocalPath': '/opt/ml/processing/output/stage',\n",
       "     'S3UploadMode': 'EndOfJob'}},\n",
       "   {'OutputName': 'scaler',\n",
       "    'AppManaged': False,\n",
       "    'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/02/1677830737.0/scaler-files',\n",
       "     'LocalPath': '/opt/ml/processing/output/scaler',\n",
       "     'S3UploadMode': 'EndOfJob'}},\n",
       "   {'OutputName': 'train',\n",
       "    'AppManaged': False,\n",
       "    'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/02/1677830737.0/train',\n",
       "     'LocalPath': '/opt/ml/processing/output/train',\n",
       "     'S3UploadMode': 'EndOfJob'}},\n",
       "   {'OutputName': 'test',\n",
       "    'AppManaged': False,\n",
       "    'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/02/1677830737.0/test',\n",
       "     'LocalPath': '/opt/ml/processing/output/test',\n",
       "     'S3UploadMode': 'EndOfJob'}}]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_definition['Steps'][0]['Arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdb0ce55-e5ea-41dc-a97e-2fbe2cc8bad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('pipeline_definition.json', 'r') as f:\n",
    "    pipeline_definition = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51434253-d53e-4312-a63b-f95f28e183f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def register_manifest(source_path,\n",
    "                      target_path,\n",
    "                      s3_client,\n",
    "                      BUCKET_NAME_USECASE):\n",
    "    template_json = {\"fileLocations\": [{\"URIPrefixes\": []}],\n",
    "                     \"globalUploadSettings\": {\n",
    "                         \"format\": \"CSV\",\n",
    "                         \"delimiter\": \",\"\n",
    "                     }}\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    response_iterator = paginator.paginate(Bucket = BUCKET_NAME_USECASE,\n",
    "                                           Prefix = source_path.split(BUCKET_NAME_USECASE+'/')[1]\n",
    "                                          )\n",
    "    for page in response_iterator:\n",
    "        for content in page['Contents']:\n",
    "            template_json['fileLocations'][0]['URIPrefixes'].append(f's3://{BUCKET_NAME_USECASE}/'+content['Key'])\n",
    "    with open(f'./manifest_testing.manifest', 'w') as f:\n",
    "        json.dump(template_json, f, indent=2)\n",
    "\n",
    "    res = s3_client.upload_file('./manifest_testing.manifest',\n",
    "                                BUCKET_NAME_USECASE,\n",
    "                                f\"{target_path.split(BUCKET_NAME_USECASE+'/')[1]}/visual_validation.manifest\")\n",
    "    return f\"{target_path.split(BUCKET_NAME_USECASE+'/')[1]}/visual_validation.manifest\"\n",
    "    \n",
    "\n",
    "def refresh_of_spice_datasets(user_account_id,\n",
    "                              qs_data_name,\n",
    "                              manifest_file_path,\n",
    "                              BUCKET_NAME_USECASE,\n",
    "                              qs_client):\n",
    "    \n",
    "    ds_list = qs_client.list_data_sources(AwsAccountId=user_account_id)\n",
    "    datasource_ids = [summary[\"DataSourceId\"] for summary in ds_list[\"DataSources\"] if qs_data_name in summary[\"Name\"]]    \n",
    "    for datasource_id in datasource_ids:\n",
    "        response = qs_client.update_data_source(\n",
    "            AwsAccountId=user_account_id,\n",
    "            DataSourceId=datasource_id,\n",
    "            Name=qs_data_name,\n",
    "            DataSourceParameters={\n",
    "                'S3Parameters': {\n",
    "                    'ManifestFileLocation': {\n",
    "                        'Bucket': BUCKET_NAME_USECASE,\n",
    "                        'Key':  manifest_file_path\n",
    "                    },\n",
    "                },\n",
    "            })\n",
    "        logger.info(f\"datasource_id:{datasource_id} 의 manifest를 업데이트: {response}\")\n",
    "    \n",
    "    res = qs_client.list_data_sets(AwsAccountId = user_account_id)\n",
    "    datasets_ids = [summary[\"DataSetId\"] for summary in res[\"DataSetSummaries\"] if qs_data_name in summary[\"Name\"]]\n",
    "    ingestion_ids = []\n",
    "\n",
    "    for dataset_id in datasets_ids:\n",
    "        try:\n",
    "            ingestion_id = str(calendar.timegm(time.gmtime()))\n",
    "            qs_client.create_ingestion(DataSetId = dataset_id,\n",
    "                                       IngestionId = ingestion_id,\n",
    "                                       AwsAccountId = user_account_id)\n",
    "            ingestion_ids.append(ingestion_id)\n",
    "        except Exception as e:\n",
    "            logger.info(e)\n",
    "            pass\n",
    "    for ingestion_id, dataset_id in zip(ingestion_ids, datasets_ids):\n",
    "        while True:\n",
    "            response = qs_client.describe_ingestion(DataSetId = dataset_id,\n",
    "                                                    IngestionId = ingestion_id,\n",
    "                                                    AwsAccountId = user_account_id)\n",
    "            if response['Ingestion']['IngestionStatus'] in ('INITIALIZED', 'QUEUED', 'RUNNING'):\n",
    "                time.sleep(5)     #change sleep time according to your dataset size\n",
    "            elif response['Ingestion']['IngestionStatus'] == 'COMPLETED':\n",
    "                print(\"refresh completed. RowsIngested {0}, RowsDropped {1}, IngestionTimeInSeconds {2}, IngestionSizeInBytes {3}\".format(\n",
    "                    response['Ingestion']['RowInfo']['RowsIngested'],\n",
    "                    response['Ingestion']['RowInfo']['RowsDropped'],\n",
    "                    response['Ingestion']['IngestionTimeInSeconds'],\n",
    "                    response['Ingestion']['IngestionSizeInBytes']))\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"refresh failed for {0}! - status {1}\".format(dataset_id,\n",
    "                                                                          response['Ingestion']['IngestionStatus']))\n",
    "                break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5769f7d3-1e7d-4639-8d39-4f5ca14dd07f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_list = qs_client.list_data_sources(AwsAccountId=user_account_id)\n",
    "datasource_ids =[summary[\"DataSourceId\"] for summary in ds_list[\"DataSources\"] if 'forecast result' in summary[\"Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f3153d-fbdd-411a-864e-a41b42018c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qs_data_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m datasource_id \u001b[38;5;129;01min\u001b[39;00m datasource_ids:\n\u001b[1;32m      2\u001b[0m     response \u001b[38;5;241m=\u001b[39m qs_client\u001b[38;5;241m.\u001b[39mupdate_data_source(\n\u001b[1;32m      3\u001b[0m         AwsAccountId\u001b[38;5;241m=\u001b[39muser_account_id,\n\u001b[1;32m      4\u001b[0m         DataSourceId\u001b[38;5;241m=\u001b[39mdatasource_id,\n\u001b[0;32m----> 5\u001b[0m         Name\u001b[38;5;241m=\u001b[39m\u001b[43mqs_data_name\u001b[49m,\n\u001b[1;32m      6\u001b[0m         DataSourceParameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS3Parameters\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mManifestFileLocation\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      9\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBucket\u001b[39m\u001b[38;5;124m'\u001b[39m: BUCKET_NAME_USECASE,\n\u001b[1;32m     10\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted-data/2023/03/20/1679388443.0/manifest/visual_validation.manifest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m                 },\n\u001b[1;32m     12\u001b[0m             },\n\u001b[1;32m     13\u001b[0m         })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qs_data_name' is not defined"
     ]
    }
   ],
   "source": [
    "for datasource_id in datasource_ids:\n",
    "    response = qs_client.update_data_source(\n",
    "        AwsAccountId=user_account_id,\n",
    "        DataSourceId=datasource_id,\n",
    "        Name=qs_data_name,\n",
    "        DataSourceParameters={\n",
    "            'S3Parameters': {\n",
    "                'ManifestFileLocation': {\n",
    "                    'Bucket': BUCKET_NAME_USECASE,\n",
    "                    'Key':  'predicted-data/2023/03/20/1679388443.0/manifest/visual_validation.manifest'\n",
    "                },\n",
    "            },\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a4eaaa-148f-4b12-9909-57ccb40d6ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c185c3d8-f575-4cac-b9cc-39bcc9b6255b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 21 Mar 2023 00:10:42 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '8814',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'c185c3d8-f575-4cac-b9cc-39bcc9b6255b'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Status': 200,\n",
       " 'DataSources': [{'Arn': 'arn:aws:quicksight:ap-northeast-2:108594546720:datasource/8c2f563f-9a63-4faf-80d2-c1fc61a9a8cd',\n",
       "   'DataSourceId': '8c2f563f-9a63-4faf-80d2-c1fc61a9a8cd',\n",
       "   'Name': 'Sales Pipeline',\n",
       "   'Type': 'S3',\n",
       "   'Status': 'UPDATE_SUCCESSFUL',\n",
       "   'CreatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 776000, tzinfo=tzlocal()),\n",
       "   'LastUpdatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 886000, tzinfo=tzlocal()),\n",
       "   'DataSourceParameters': {'S3Parameters': {'ManifestFileLocation': {'Bucket': 'spaceneedle-samplefiles.prod.ap-northeast-2',\n",
       "      'Key': 'sales/manifest.json'}}}},\n",
       "  {'Arn': 'arn:aws:quicksight:ap-northeast-2:108594546720:datasource/5308e376-ab57-440f-b38d-acf9eb635993',\n",
       "   'DataSourceId': '5308e376-ab57-440f-b38d-acf9eb635993',\n",
       "   'Name': 'model_result',\n",
       "   'Type': 'S3',\n",
       "   'Status': 'UPDATE_SUCCESSFUL',\n",
       "   'CreatedTime': datetime.datetime(2023, 1, 31, 23, 1, 17, 949000, tzinfo=tzlocal()),\n",
       "   'LastUpdatedTime': datetime.datetime(2023, 3, 20, 23, 58, 15, 277000, tzinfo=tzlocal()),\n",
       "   'DataSourceParameters': {'S3Parameters': {'ManifestFileLocation': {'Bucket': 'crude-palm-oil-prices-forecast',\n",
       "      'Key': 'predicted-data/2023/03/20/1679388443.0/manifest/visual_validation.manifest'}}}},\n",
       "  {'Arn': 'arn:aws:quicksight:ap-northeast-2:108594546720:datasource/e79c5579-2910-4a20-8a94-5b3857c53023',\n",
       "   'DataSourceId': 'e79c5579-2910-4a20-8a94-5b3857c53023',\n",
       "   'Name': 'forecast result',\n",
       "   'Type': 'S3',\n",
       "   'Status': 'UPDATE_SUCCESSFUL',\n",
       "   'CreatedTime': datetime.datetime(2023, 2, 6, 6, 40, 2, 384000, tzinfo=tzlocal()),\n",
       "   'LastUpdatedTime': datetime.datetime(2023, 3, 20, 9, 7, 32, 634000, tzinfo=tzlocal()),\n",
       "   'DataSourceParameters': {'S3Parameters': {'ManifestFileLocation': {'Bucket': 'crude-palm-oil-prices-forecast',\n",
       "      'Key': 'predicted-data/2023/03/19/1679292475.0/manifest/visual_validation.manifest'}}}},\n",
       "  {'Arn': 'arn:aws:quicksight:ap-northeast-2:108594546720:datasource/b87f301d-faa0-4e51-b260-832073a1564d',\n",
       "   'DataSourceId': 'b87f301d-faa0-4e51-b260-832073a1564d',\n",
       "   'Name': 'Web and Social Media Analytics',\n",
       "   'Type': 'S3',\n",
       "   'Status': 'UPDATE_SUCCESSFUL',\n",
       "   'CreatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 777000, tzinfo=tzlocal()),\n",
       "   'LastUpdatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 887000, tzinfo=tzlocal()),\n",
       "   'DataSourceParameters': {'S3Parameters': {'ManifestFileLocation': {'Bucket': 'spaceneedle-samplefiles.prod.ap-northeast-2',\n",
       "      'Key': 'marketing/manifest.json'}}}},\n",
       "  {'Arn': 'arn:aws:quicksight:ap-northeast-2:108594546720:datasource/e0fdfcd5-aebf-456c-bc2f-3a505ff3938f',\n",
       "   'DataSourceId': 'e0fdfcd5-aebf-456c-bc2f-3a505ff3938f',\n",
       "   'Name': 'Business Review',\n",
       "   'Type': 'S3',\n",
       "   'Status': 'UPDATE_SUCCESSFUL',\n",
       "   'CreatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 777000, tzinfo=tzlocal()),\n",
       "   'LastUpdatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 866000, tzinfo=tzlocal()),\n",
       "   'DataSourceParameters': {'S3Parameters': {'ManifestFileLocation': {'Bucket': 'spaceneedle-samplefiles.prod.ap-northeast-2',\n",
       "      'Key': 'revenue/manifest.json'}}}},\n",
       "  {'Arn': 'arn:aws:quicksight:ap-northeast-2:108594546720:datasource/ed97e6ba-f183-4d25-83f3-3cc4bff71258',\n",
       "   'DataSourceId': 'ed97e6ba-f183-4d25-83f3-3cc4bff71258',\n",
       "   'Name': 'People Overview',\n",
       "   'Type': 'S3',\n",
       "   'Status': 'UPDATE_SUCCESSFUL',\n",
       "   'CreatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 773000, tzinfo=tzlocal()),\n",
       "   'LastUpdatedTime': datetime.datetime(2023, 2, 1, 1, 30, 8, 895000, tzinfo=tzlocal()),\n",
       "   'DataSourceParameters': {'S3Parameters': {'ManifestFileLocation': {'Bucket': 'spaceneedle-samplefiles.prod.ap-northeast-2',\n",
       "      'Key': 'hr/manifest.json'}}}},\n",
       "  {'Arn': 'arn:aws:quicksight:ap-northeast-2:108594546720:datasource/79c28a95-6ef6-4b69-a80a-2fb85181eec0',\n",
       "   'DataSourceId': '79c28a95-6ef6-4b69-a80a-2fb85181eec0',\n",
       "   'Name': 'prediction_result.csv',\n",
       "   'Type': 'FILE',\n",
       "   'CreatedTime': datetime.datetime(2023, 1, 15, 23, 19, 0, 559000, tzinfo=tzlocal()),\n",
       "   'LastUpdatedTime': datetime.datetime(2023, 1, 15, 23, 19, 0, 797000, tzinfo=tzlocal())}],\n",
       " 'RequestId': 'c185c3d8-f575-4cac-b9cc-39bcc9b6255b'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_client.list_data_sources(AwsAccountId=user_account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a4674-520e-4f31-a714-0dc55790a285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
