{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58d54f1-ccf2-445f-96f0-256d76fbd162",
   "metadata": {},
   "source": [
    "# SageMaker 모델 빌드 파이프라인을 이용한 모델 빌드 오케스트레이션\n",
    "Amazon SageMaker Model building pipeline은 머신러닝 워크플로우를 개발하는 데이터 과학자, 엔지니어들에게 SageMaker작업과 재생산가능한 머신러닝 파이프라인을 오케스트레이션하는 기능을 제공합니다. 또한 커스텀빌드된 모델을 실시간 추론환경이나 배치변환을 통한 추론 실행환경으로 배포하거나, 생성된 아티팩트의 계보(lineage)를 추적하는 기능을 제공합니다. 이 기능들을 통해 모델 아티팩트를 배포하고, 업무환경에서의 워크플로우를 배포/모니터링하고, 간단한 인터페이스를 통해 아티팩트의 계보 추적하고, 머신러닝 애플리케이션 개발의 베스트 프렉티스를 도입하여, 보다 안정적인 머신러닝 애플리케이션 운영환경을 구현할 수 있습니다.\n",
    "\n",
    "SageMaker pipeline 서비스는 JSON 선언으로 구현된 SageMaker Pipeline DSL(Domain Specific Language, 도메인종속언어)를 지원합니다. 이 DSL은 파이프라인 파라마터와 SageMaker 작업단계의 DAG(Directed Acyclic Graph)를 정의합니다. SageMaker Python SDK를 이용하면 이 파이프라인 DSL의 생성을 보다 간편하게 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03365354-35af-4bdb-9abc-c8c3a3c5da5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. 사용 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad58b1-5eb6-4f31-b2b4-62f96c960ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d8dde1-3036-44bb-953f-c4b8fa25c82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v2.0/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v2.0/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "from logging.config import dictConfig\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "KST_aday_before = KST - relativedelta(days=1) \n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD', 'EUR','JPY', 'KRW', 'MYR', 'GBP', 'INR','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'GCc1', 'GCc2', 'GCc3','GENMKL','HSI','IOIBKL', 'KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        l.addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "\n",
    "    secret_name = \"dev/ForecastPalmOilPrice\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    #2023-01-07: Interpolate 했음에도 불구하고, null값인경우 close 값으로 채움\n",
    "    df[col] = df[col].fillna(df['y'])\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  output_dir : str,\n",
    "                  ric,) -> tuple:\n",
    "                  # s3_resource,\n",
    "                  # BUCKET_NAME_USECASE,\n",
    "                  # S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    joblib.dump(scaler, os.path.join(output_dir, f'{ric}_{col_name}_scaler.pkl'))    \n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\") \n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--split_start', type=str, default='2014-07-02')    \n",
    "    parser.add_argument('--split_end', type=str, default=KST.strftime('%Y-%m-%d'))\n",
    "    parser.add_argument('--num_fold', type=str, default='5')\n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = '1', help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "     \n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    \n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "\n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    args = parse_args()\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_start: {args.split_start}\")   \n",
    "    logger.info(f\"args.split_end: {args.split_end}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")\n",
    "    logger.info(f\"args.num_fold: {args.num_fold}\")\n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_start = args.split_start\n",
    "    split_end = args.split_end\n",
    "    num_fold = int(args.num_fold)\n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['AWS_ACCESS_KEY_ID']\n",
    "    ACCESS_SECRET_KEY = keychain['AWS_ACCESS_SECRET_KEY']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['PROJECT_BUCKET_NAME']\n",
    "    DATALAKE_BUCKET_NAME = keychain['DATALAKE_BUCKET_NAME']\n",
    "\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    logger.info(f\"### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    equalSignList = ['CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=', 'CNY=']\n",
    "    for file in path_list:\n",
    "        try:\n",
    "            df_tmp = pd.read_csv(file, encoding='utf-8') \n",
    "            df_tmp['RIC'] = df_tmp['RIC'].apply(lambda x: x[:-1] if x in equalSignList else x)\n",
    "            df_sum = pd.concat([df_sum, df_tmp])\n",
    "        except:\n",
    "            logger.info(f\"{file} is empty \")\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{base_output_dir}/stage/stage_integrated.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    logger.info(f\"Integrated data.info \\n {df_sum.info()}\")\n",
    "\n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    \n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= split_start].reset_index(drop = True)\n",
    "        \n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if scaler_switch == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value,\n",
    "                                                  col,\n",
    "                                                  f'{base_output_dir}/scaler',\n",
    "                                                  name,) #s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Transform data info \\n {df_sum.head(2)}\")\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "        \n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    df_train_fold0 = df_golden[df_golden['ds'] < split_end]\n",
    "\n",
    "    # train 데이터 나누기\n",
    "    for cnt in range(num_fold):\n",
    "        split_end = (dt.strptime(split_end, '%Y-%m-%d') - relativedelta(days=30)).strftime('%Y-%m-%d')\n",
    "    \n",
    "        logger.info(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < {split_end}]\")\n",
    "        exec(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < split_end]\")\n",
    "        exec(f\"df_train_fold{cnt+1}.to_csv('{base_output_dir}/train/train_fold{cnt+1}.csv', index = False)\")\n",
    "\n",
    "        logger.info(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= {split_end}]\")\n",
    "        exec(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= split_end]\")\n",
    "        exec(f\"df_test_fold{cnt+1}.to_csv('{base_output_dir}/test/test_fold{cnt+1}.csv', index = False)\")\n",
    "    \n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    \n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299d9b7-41e1-4f7c-9550-0437e94fa2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-2. train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b96b625-2eee-4960-9046-bea2656bb433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v2.0/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v2.0/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import joblib # from sklearn.externals import joblib\n",
    "import pickle\n",
    "import tarfile # model registry에는 uri만 등록된다.\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "from logging.config import dictConfig\n",
    "\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_dir\", type=str, default='/opt/ml/processing/input/train')\n",
    "    parser.add_argument(\"--test_dir\", type=str, default='/opt/ml/processing/input/test')\n",
    "    parser.add_argument('--output_dir', type = str, default = '/opt/ml/processing/output')\n",
    "    parser.add_argument('--item', type = str, default = 'FCPOc3')\n",
    "    parser.add_argument('--target', type = str, default = 'y')\n",
    "    parser.add_argument('--metric', type = str, default = 'MAPE')    \n",
    "    parser.add_argument('--quality', type = str, default = 'fast_training')    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def create_tarfile(source_dir, output_filename=None):\n",
    "    ''' create a tarfile from a source directory'''\n",
    "    if output_filename == None:\n",
    "        output_filename = \"%s/tmptar.tar\" %(tempfile.mkdtemp())\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "    return output_filename \n",
    "\n",
    "def make_tarfile(source_dir, output_filename):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "    return os.path.join(source_dir, output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ############################################\n",
    "    ########## 필요 라이브러리 설치  ###########\n",
    "    ########################################### \n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'autogluon==0.6.1'])\n",
    "    from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "    \n",
    "    ######################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ######################################\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(\"### start training code\")    \n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    args = parse_args()\n",
    "        \n",
    "    logger.info(f\"args.train_dir: {args.train_dir}\")   \n",
    "    logger.info(f\"args.test_dir: {args.test_dir}\")   \n",
    "    logger.info(f\"args.output_dir: {args.output_dir}\")    \n",
    "    logger.info(f\"args.item: {args.item}\")   \n",
    "    logger.info(f\"args.target: {args.target}\")    \n",
    "    logger.info(f\"args.metric: {args.metric}\")   \n",
    "    logger.info(f\"args.quality: {args.quality}\")   \n",
    "    \n",
    "    train_dir = args.train_dir\n",
    "    test_dir = args.test_dir\n",
    "    output_dir = args.output_dir\n",
    "    prediction_dir = os.path.join(output_dir, 'prediction')\n",
    "    leaderboard_dir = os.path.join(output_dir, 'leaderboard')\n",
    "    model_dir = os.path.join(output_dir, 'model')\n",
    "    \n",
    "    for path in [prediction_dir, leaderboard_dir, model_dir]:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "    item = args.item\n",
    "    target = args.target\n",
    "    metric = args.metric\n",
    "    quality = args.quality\n",
    "    \n",
    "    trlist = sorted(os.listdir(train_dir))\n",
    "    telist = sorted(os.listdir(test_dir))\n",
    "    \n",
    "    logger.info(f\"the list of train data {trlist}\")\n",
    "    logger.info(f\"the list of train data {telist}\")\n",
    "    \n",
    "    for train_file, test_file in zip(trlist, telist):\n",
    "        logger.info(\"### Reading input data\")\n",
    "        logger.info(f\"### train data: {train_file}\")\n",
    "        logger.info(f\"### test data: {test_file}\")\n",
    "        \n",
    "        df_train = pd.read_csv(os.path.join(train_dir, train_file))\n",
    "        df_test = pd.read_csv(os.path.join(test_dir, test_file))      \n",
    "        # df_train = df_train[df_train['ric'] != 'MCCc3']\n",
    "        # df_test = df_test[df_test['ric'] != 'MCCc3']\n",
    "        \n",
    "        logger.info(\"### Convert TimeSeriesDataFrame\")\n",
    "        df_train.loc[:, \"ds\"] = pd.to_datetime(df_train.loc[:, \"ds\"])\n",
    "        df_test.loc[:, \"ds\"] = pd.to_datetime(df_test.loc[:, \"ds\"])\n",
    "\n",
    "        tdf_train = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_train,\n",
    "            id_column=\"ric\",\n",
    "            timestamp_column=\"ds\",\n",
    "        )\n",
    "        tdf_test = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_test,\n",
    "            id_column=\"ric\",\n",
    "            timestamp_column=\"ds\",\n",
    "        )\n",
    "\n",
    "        logger.info(\"### Show the range of date for training and test\")    \n",
    "        logger.info('Item:', item)\n",
    "        logger.info('Target:', target)   \n",
    "        logger.info('Train:',tdf_train.loc[item][target].index.min(),'~',tdf_train.loc[item][target].index.max())\n",
    "        logger.info('Test:',tdf_test.loc[item][target].index.min(),'~',tdf_test.loc[item][target].index.max())\n",
    "        logger.info('The number of test data:',len(tdf_test.loc[item][target]))\n",
    "\n",
    "        logger.info(\"### Training AutoGluon Model\")    \n",
    "        predictor = TimeSeriesPredictor(\n",
    "            path = model_dir,\n",
    "            target = target,\n",
    "            prediction_length = len(tdf_test.loc[item][target]),\n",
    "            eval_metric = metric,\n",
    "        )\n",
    "        predictor.fit(\n",
    "            train_data = tdf_train,\n",
    "            presets = quality\n",
    "        )\n",
    "        logger.info(\"the list of data in model_dir {}\".format(os.listdir(model_dir)))\n",
    "        tar_file_path = make_tarfile(model_dir, f'{model_dir}/model.tar.gz')\n",
    "        logger.info(\"Saving model to {}\".format(tar_file_path))\n",
    "\n",
    "        predictor_leaderboard = predictor.leaderboard(tdf_test, silent = True)\n",
    "        predictor_leaderboard = predictor_leaderboard.sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                  ascending = False)\n",
    "        predictor_leaderboard.to_csv(os.path.join(leaderboard_dir,\n",
    "                                                  f'leaderboard-{test_file}'),\n",
    "                                     index = False)\n",
    "        logger.info(f\"predictor_leaderboard sample: head(2) \\n {predictor_leaderboard.head(2)}\")\n",
    "        \n",
    "        top_model_name = predictor_leaderboard.loc[0, 'model']\n",
    "        # second_model_name = predictor_leaderboard.loc[1, 'model']\n",
    "        \n",
    "        prediction_ag_model_01 = predictor.predict(data = tdf_train,\n",
    "                                                   model = top_model_name)\n",
    "#         prediction_ag_model_02 = predictor.predict(data = tdf_train,\n",
    "#                                                    model = second_model_name)\n",
    "        pred_result_01 = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model_01.loc['FCPOc3'],\n",
    "                                  left_index = True, right_index = True, how = 'left')\n",
    "        pred_result_01.to_csv(os.path.join(prediction_dir,\n",
    "                                           f'pred-{top_model_name}-{test_file}'))   \n",
    "        # pred_result_02 = pd.merge(tdf_test.loc['FCPOc3']['y'], prediction_ag_model_02.loc['FCPOc3'],\n",
    "        #                           left_index = True, right_index = True, how = 'left')\n",
    "        # pred_result_02.to_csv(os.path.join(prediction_dir,\n",
    "        #                                    f'pred-{second_model_name}-{test_file}'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b928-35ef-4fb7-95b0-fdb7fd00c481",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0-3. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea0bef0-b16a-4b98-8300-d43b99f08419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v2.0/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v2.0/model_validation.py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import argparse\n",
    "import json\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "import joblib\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "import logging.handlers\n",
    "import calendar\n",
    "import tarfile\n",
    "\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"dev/ForecastPalmOilPrice\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "def check_performance_threshold(iput_df : pd.DataFrame,\n",
    "                                identifier: str,\n",
    "                                threshold : float = -100):\n",
    "    tmp = {}\n",
    "    satisfied_df = iput_df[iput_df['score_val'] > threshold]\n",
    "    if len(satisfied_df) > 0:\n",
    "        tmp['identifier'] = identifier\n",
    "        tmp['model'] = list(satisfied_df['model'])\n",
    "        tmp['performance'] = list(satisfied_df['score_val'])\n",
    "    return tmp\n",
    "\n",
    "def get_model_performance_report(data):\n",
    "    result = defaultdict(list)\n",
    "    models_ext = [row[\"model\"] for row in data if row]\n",
    "    models = [item for sublist in models_ext for item in sublist]\n",
    "    performance_ext = [row[\"performance\"] for row in data if row]\n",
    "    performance = [item for sublist in performance_ext for item in sublist]\n",
    "    \n",
    "    count_models = Counter(models)\n",
    "    \n",
    "    for keys, values in zip(models, performance):\n",
    "        result[keys].append(values)\n",
    "\n",
    "    for key, values in result.items():\n",
    "        result[key] = []\n",
    "        result[key].append(count_models[key])\n",
    "        result[key].append(sum(values) / len(values))\n",
    "        result[key].append(np.std(values))\n",
    "    \n",
    "    # 정렬 1순위 : 비즈니스담당자의 Metric에 선정된 Count 높은 순, 2순위: 표준편차가 작은 순(음수처리)\n",
    "    result = sorted(result.items(), key=lambda k_v: (k_v[1][0], -k_v[1][2]), reverse=True) \n",
    "    return result\n",
    "\n",
    "def register_model_in_aws_registry(model_zip_path: str,\n",
    "                                   model_package_group_name: str,\n",
    "                                   model_description: str,\n",
    "                                   model_tags: dict,############################# parameter 추가할것: golden train path와 test path \n",
    "                                   model_status: str,\n",
    "                                   sm_client) -> str:\n",
    "    create_model_package_input_dict = {\n",
    "        \"ModelPackageGroupName\": model_package_group_name,\n",
    "        \"ModelPackageDescription\": model_description, # ex AutoGluon - WeightedEnsemble\n",
    "        \"CustomerMetadataProperties\": model_tags,\n",
    "        \"ModelApprovalStatus\": model_status,\n",
    "        \"InferenceSpecification\": {\n",
    "            \"Containers\": [\n",
    "                {\n",
    "                    \"Image\": '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/autogluon-inference:0.4-cpu-py38',\n",
    "                    \"ModelDataUrl\": model_zip_path\n",
    "                }\n",
    "            ],\n",
    "            \"SupportedContentTypes\": [\"text/csv\"],\n",
    "            \"SupportedResponseMIMETypes\": [\"text/csv\"],\n",
    "        }\n",
    "    }\n",
    "    create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "    model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "    return model_package_arn\n",
    "\n",
    "\n",
    "def register_manifest(source_path,\n",
    "                      target_path,\n",
    "                      s3_client,\n",
    "                      BUCKET_NAME_USECASE):\n",
    "    template_json = {\"fileLocations\": [{\"URIPrefixes\": []}],\n",
    "                     \"globalUploadSettings\": {\n",
    "                         \"format\": \"CSV\",\n",
    "                         \"delimiter\": \",\"\n",
    "                     }}\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    response_iterator = paginator.paginate(Bucket = BUCKET_NAME_USECASE,\n",
    "                                           Prefix = source_path.split(BUCKET_NAME_USECASE+'/')[1]\n",
    "                                          )\n",
    "    for page in response_iterator:\n",
    "        for content in page['Contents']:\n",
    "            template_json['fileLocations'][0]['URIPrefixes'].append(f's3://{BUCKET_NAME_USECASE}/'+content['Key'])\n",
    "    with open(f'./manifest_testing.manifest', 'w') as f:\n",
    "        json.dump(template_json, f, indent=2)\n",
    "\n",
    "    res = s3_client.upload_file('./manifest_testing.manifest',\n",
    "                                BUCKET_NAME_USECASE,\n",
    "                                f\"{target_path.split(BUCKET_NAME_USECASE+'/')[1]}/visual_validation.manifest\")\n",
    "    return f\"{target_path.split(BUCKET_NAME_USECASE+'/')[1]}/visual_validation.manifest\"\n",
    "    \n",
    "def refresh_of_spice_datasets(user_account_id,\n",
    "                              qs_data_name,\n",
    "                              manifest_file_path,\n",
    "                              BUCKET_NAME_USECASE,\n",
    "                              qs_client):\n",
    "    \n",
    "    ds_list = qs_client.list_data_sources(AwsAccountId='108594546720')\n",
    "    datasource_ids = [summary[\"DataSourceId\"] for summary in ds_list[\"DataSources\"] if qs_data_name in summary[\"Name\"]]    \n",
    "    for datasource_id in datasource_ids:\n",
    "        response = qs_client.update_data_source(\n",
    "            AwsAccountId=user_account_id,\n",
    "            DataSourceId=datasource_id,\n",
    "            Name=qs_data_name,\n",
    "            DataSourceParameters={\n",
    "                'S3Parameters': {\n",
    "                    'ManifestFileLocation': {\n",
    "                        'Bucket': BUCKET_NAME_USECASE,\n",
    "                        'Key':  manifest_file_path\n",
    "                    },\n",
    "                },\n",
    "            })\n",
    "        logger.info(f\"datasource_id:{datasource_id} 의 manifest를 업데이트: {response}\")\n",
    "    \n",
    "    res = qs_client.list_data_sets(AwsAccountId = user_account_id)\n",
    "    datasets_ids = [summary[\"DataSetId\"] for summary in res[\"DataSetSummaries\"] if qs_data_name in summary[\"Name\"]]\n",
    "    ingestion_ids = []\n",
    "\n",
    "    for dataset_id in datasets_ids:\n",
    "        try:\n",
    "            ingestion_id = str(calendar.timegm(time.gmtime()))\n",
    "            qs_client.create_ingestion(DataSetId = dataset_id,\n",
    "                                       IngestionId = ingestion_id,\n",
    "                                       AwsAccountId = user_account_id)\n",
    "            ingestion_ids.append(ingestion_id)\n",
    "        except Exception as e:\n",
    "            logger.info(e)\n",
    "            pass\n",
    "    for ingestion_id, dataset_id in zip(ingestion_ids, datasets_ids):\n",
    "        while True:\n",
    "            response = qs_client.describe_ingestion(DataSetId = dataset_id,\n",
    "                                                    IngestionId = ingestion_id,\n",
    "                                                    AwsAccountId = user_account_id)\n",
    "            if response['Ingestion']['IngestionStatus'] in ('INITIALIZED', 'QUEUED', 'RUNNING'):\n",
    "                time.sleep(5)     #change sleep time according to your dataset size\n",
    "            elif response['Ingestion']['IngestionStatus'] == 'COMPLETED':\n",
    "                print(\"refresh completed. RowsIngested {0}, RowsDropped {1}, IngestionTimeInSeconds {2}, IngestionSizeInBytes {3}\".format(\n",
    "                    response['Ingestion']['RowInfo']['RowsIngested'],\n",
    "                    response['Ingestion']['RowInfo']['RowsDropped'],\n",
    "                    response['Ingestion']['IngestionTimeInSeconds'],\n",
    "                    response['Ingestion']['IngestionSizeInBytes']))\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"refresh failed for {0}! - status {1}\".format(dataset_id,\n",
    "                                                                          response['Ingestion']['IngestionStatus']))\n",
    "                break\n",
    "    return response\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--leaderboard_path', type=str, default=\"/opt/ml/processing/input/leaderboard\")   \n",
    "    parser.add_argument('--algorithm_name', type=str)\n",
    "    parser.add_argument('--model_base_path', type=str)\n",
    "    parser.add_argument('--manifest_base_path', type=str)\n",
    "    parser.add_argument('--prediction_base_path', type=str)\n",
    "    parser.add_argument('--threshold', type=str, default=\"-100\")   \n",
    "    parser.add_argument('--model_package_group_name', type=str, default = BUCKET_NAME_USECASE)  \n",
    "    parser.add_argument('--qs_data_name', type=str, default = 'model_result')    \n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    logger.info(f\"\\n### Loading Key value from Secret Manager\")\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['AWS_ACCESS_KEY_ID']\n",
    "    ACCESS_SECRET_KEY = keychain['AWS_ACCESS_SECRET_KEY']\n",
    "    BUCKET_NAME_USECASE = keychain['PROJECT_BUCKET_NAME']\n",
    "    DATALAKE_BUCKET_NAME = keychain['DATALAKE_BUCKET_NAME']\n",
    "    S3_PATH_REUTER = keychain['S3_PATH_REUTER']\n",
    "    S3_PATH_WWO = keychain['S3_PATH_WWO']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_FORECAST = keychain['S3_PATH_PREDICTION']\n",
    "    \n",
    "    boto3_session = boto3.Session(aws_access_key_id = ACCESS_KEY_ID,\n",
    "                                  aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                                  region_name = 'ap-northeast-2')\n",
    "    \n",
    "    s3_client = boto3_session.client('s3')\n",
    "    sm_client = boto3_session.client('sagemaker')\n",
    "    qs_client = boto3_session.client('quicksight')\n",
    "\n",
    "    sts_client = boto3_session.client(\"sts\")\n",
    "    user_account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "    ######################################\n",
    "    ## 커맨드 인자, Hyperparameters 처리 ##\n",
    "    ######################################\n",
    "    args = parse_args()\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(\"### start training code\")    \n",
    "    logger.info(\"### Argument Info ###\")\n",
    "    logger.info(f\"args.algorithm_name: {args.algorithm_name}\")    \n",
    "    logger.info(f\"args.leaderboard_path: {args.leaderboard_path}\")    \n",
    "    logger.info(f\"args.model_base_path: {args.model_base_path}\")\n",
    "    logger.info(f\"args.manifest_base_path: {args.manifest_base_path}\")\n",
    "    logger.info(f\"args.prediction_base_path: {args.prediction_base_path}\")\n",
    "    logger.info(f\"args.threshold: {args.threshold}\")\n",
    "    logger.info(f\"args.model_package_group_name: {args.model_package_group_name}\")\n",
    "    logger.info(f\"args.qs_data_name: {args.qs_data_name}\")\n",
    "  \n",
    "    algorithm_name = args.algorithm_name\n",
    "    leaderboard_path = args.leaderboard_path\n",
    "    model_base_path = args.model_base_path\n",
    "    manifest_base_path = args.manifest_base_path\n",
    "    prediction_base_path = args.prediction_base_path\n",
    "    threshold = float(args.threshold)\n",
    "    model_package_group_name = args.model_package_group_name\n",
    "    qs_data_name = args.qs_data_name\n",
    "    \n",
    "    lb_list = sorted(os.listdir(leaderboard_path))\n",
    "    logger.info(f\"leaderboard file list in {leaderboard_path}: {lb_list}\")\n",
    "    satisfied_info = []\n",
    "    train_data_base_path = manifest_base_path\n",
    "    test_data_base_path = manifest_base_path\n",
    "    train_replace_dict = {'trained-model' : S3_PATH_GOLDEN,\n",
    "                          'manifest' : 'train'}\n",
    "    for key in train_replace_dict.keys():\n",
    "        train_data_base_path = train_data_base_path.replace(key, train_replace_dict[key])\n",
    "    test_replace_dict = {'trained-model' : S3_PATH_GOLDEN,\n",
    "                          'manifest' : 'test'}\n",
    "    for key in test_replace_dict.keys():\n",
    "        test_data_base_path = test_data_base_path.replace(key, test_replace_dict[key])\n",
    "        \n",
    "    for idx, f_path in enumerate(lb_list):\n",
    "        leaderboard = pd.read_csv(f'{leaderboard_path}/{f_path}').sort_values(by = ['score_val', 'score_test'],\n",
    "                                                                              ascending = False)\n",
    "        satisfied_info.append(check_performance_threshold(iput_df = leaderboard,\n",
    "                                                          identifier = f'fold{idx}',\n",
    "                                                          threshold = threshold))\n",
    "    model_report = get_model_performance_report(satisfied_info)\n",
    "    logger.info(f\"\\n####model_report: {model_report}\")\n",
    "    if model_report[0][1][0] == len(lb_list): # Fold 내 모든 성능이 비즈니스 담당자가 설정한 값을 만족한다면\n",
    "        logger.info(f\"\\n#### Pass the 1st minimum performance valiation\")\n",
    "        manifest_file_path = register_manifest(prediction_base_path, \n",
    "                                               manifest_base_path,\n",
    "                                               s3_client,\n",
    "                                               BUCKET_NAME_USECASE)\n",
    "\n",
    "        model_package_arn = register_model_in_aws_registry(model_zip_path = f\"{model_base_path}/model.tar.gz\",\n",
    "                                                           model_package_group_name = model_package_group_name,\n",
    "                                                           model_description = algorithm_name,\n",
    "                                                           model_tags = {'champion_model' : str(model_report[0][0]),\n",
    "                                                                         'passed_the_number_of_folds' : str(model_report[0][1][0]),\n",
    "                                                                         'average_metric' : str(model_report[0][1][1]),\n",
    "                                                                         'std_metric' : str(model_report[0][1][2]),\n",
    "                                                                         'train_data' : str(train_data_base_path),\n",
    "                                                                         'test_data' : str(test_data_base_path),\n",
    "                                                                        },\n",
    "                                                           model_status = 'PendingManualApproval',\n",
    "                                                           sm_client = sm_client)\n",
    "        logger.info('### Passed ModelPackage Version ARN : {}'.format(model_package_arn))\n",
    "        logger.info('### manifest_file_path : {}'.format(manifest_file_path))\n",
    "        res = refresh_of_spice_datasets(user_account_id,\n",
    "                                        qs_data_name,\n",
    "                                        manifest_file_path,\n",
    "                                        BUCKET_NAME_USECASE,\n",
    "                                        qs_client)\n",
    "        logger.info('### refresh_of_spice_datasets : {}'.format(res))\n",
    "    else:\n",
    "        logger.info(f\"\\n#### Filtered at 1st valiation\")\n",
    "        model_package_arn = register_model_in_aws_registry(model_zip_path = f\"{model_base_path}/model.tar.gz\",\n",
    "                                                           model_package_group_name = model_package_group_name,\n",
    "                                                           model_description = algorithm_name,\n",
    "                                                           model_tags = {'champion_model' : str(model_report[0][0]),\n",
    "                                                                         'passed_the_number_of_folds' : str(model_report[0][1][0]),\n",
    "                                                                         'average_metric' : str(model_report[0][1][1]),\n",
    "                                                                         'std_metric' : str(model_report[0][1][2]),\n",
    "                                                                         'train_data' : str(train_data_base_path),\n",
    "                                                                         'test_data' : str(test_data_base_path),\n",
    "                                                                        },\n",
    "                                                           model_status = 'Rejected',\n",
    "                                                           sm_client = sm_client)\n",
    "        logger.info('### Rejected ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43ea47-dcc7-4973-8303-956c65f5f7c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce81ec-fdd6-41b0-8a82-2465c664cc31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 라이브러리 및 변수 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d65ec2-afc2-4954-ad35-7f59fd097b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import sagemaker\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import calendar\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be1cb17-7559-4e20-b593-7f850b3daca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2023-03-24 16:37:01.391408\n"
     ]
    }
   ],
   "source": [
    "# 한국 시간\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "KST_aday_before = KST - relativedelta(days=1) \n",
    "yyyy, mm, dd = str(KST_aday_before.year), str(KST_aday_before.month).zfill(2), str(KST_aday_before.day).zfill(2)\n",
    "print(f\"Start job time: {KST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017b0400-5458-4c22-86b2-dd494f5a215c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code version: 2.0\n"
     ]
    }
   ],
   "source": [
    "# 코드 버전\n",
    "code_version = '2.0'\n",
    "print(f\"Code version: {code_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c32c0e4-17bb-44f1-944a-c78ade63795b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    secret_name = \"dev/ForecastPalmOilPrice\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name,\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "keychain = json.loads(get_secret())\n",
    "ACCESS_KEY_ID = keychain['AWS_ACCESS_KEY_ID']\n",
    "ACCESS_SECRET_KEY = keychain['AWS_ACCESS_SECRET_KEY']\n",
    "BUCKET_NAME_USECASE = keychain['PROJECT_BUCKET_NAME']\n",
    "DATALAKE_BUCKET_NAME = keychain['DATALAKE_BUCKET_NAME']\n",
    "S3_PATH_REUTER = keychain['S3_PATH_REUTER']\n",
    "S3_PATH_WWO = keychain['S3_PATH_WWO']\n",
    "S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "S3_PATH_FORECAST = keychain['S3_PATH_PREDICTION']\n",
    "\n",
    "boto3_session = boto3.Session(aws_access_key_id = ACCESS_KEY_ID,\n",
    "                              aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                              region_name = 'ap-northeast-2')\n",
    "\n",
    "s3_client = boto3_session.client('s3')\n",
    "sm_client = boto3_session.client('sagemaker')\n",
    "qs_client = boto3_session.client('quicksight')\n",
    "\n",
    "sts_client = boto3_session.client(\"sts\")\n",
    "user_account_id = sts_client.get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231675a-a0e4-49ad-bb2c-3bf40ccff01d",
   "metadata": {},
   "source": [
    "노트북에 저장된 변수를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed86952-8498-4d96-b359-3d23461042b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: src/v2.0/preprocessing.py to s3://crude-palm-oil-prices-forecast/src/preprocessing.py\n",
      "upload: src/v2.0/prediction-autogluon.py to s3://crude-palm-oil-prices-forecast/src/prediction-autogluon.py\n",
      "upload: src/v2.0/model_validation.py to s3://crude-palm-oil-prices-forecast/src/model_validation.py\n",
      "upload: src/v2.0/train.py to s3://crude-palm-oil-prices-forecast/src/train.py\n",
      "upload: src/v2.0/visualization.py to s3://crude-palm-oil-prices-forecast/src/visualization.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp 'src/v2.0' 's3://crude-palm-oil-prices-forecast/src' --recursive --exclude \".ipynb_checkpoints*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d15ee3-98aa-4c58-819b-5ea7479715aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_version: 2.0\n",
      "preprocessing_code: s3://crude-palm-oil-prices-forecast/src/preprocessing.py\n",
      "training_code: s3://crude-palm-oil-prices-forecast/src/train.py\n",
      "model_validation_code: s3://crude-palm-oil-prices-forecast/src/model_validation.py\n"
     ]
    }
   ],
   "source": [
    "preprocessing_code = 's3://crude-palm-oil-prices-forecast/src/preprocessing.py'\n",
    "training_code = 's3://crude-palm-oil-prices-forecast/src/train.py'\n",
    "model_validation_code = 's3://crude-palm-oil-prices-forecast/src/model_validation.py'\n",
    "print('code_version:',code_version)\n",
    "print('preprocessing_code:',preprocessing_code)\n",
    "print('training_code:',training_code)\n",
    "print('model_validation_code:',model_validation_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba1eda-81d5-4df9-9e5c-b6682982b8fa",
   "metadata": {},
   "source": [
    "# 2. 모델 빌딩 파이프라인 의 스텝(Step) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9d664-d5a1-44a1-a249-cbd1aa294a7e",
   "metadata": {},
   "source": [
    "## 2.1 모델 빌딩 파이프라인 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3eaa7cf-ebeb-4438-9a18-d5fe1bf12971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2023-03-24 16:37:02.816433\n",
      "code verison: 2.0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "###################\n",
    "# 0) 변수 선언 ###\n",
    "##################\n",
    "# 한국 시간\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "KST_aday_before = KST - relativedelta(days=1) \n",
    "yyyy, mm, dd = str(KST_aday_before.year), str(KST_aday_before.month).zfill(2), str(KST_aday_before.day).zfill(2)\n",
    "\n",
    "split_start = '2018-01-01'\n",
    "split_end = KST.strftime('%Y-%m-%d')\n",
    "timestamp = time.mktime(KST.timetuple())\n",
    "\n",
    "# 프로젝트 변수\n",
    "project_prefix = BUCKET_NAME_USECASE\n",
    "usecase_base_path = os.path.join('s3://', BUCKET_NAME_USECASE)\n",
    "datalake_base_path = os.path.join('s3://', DATALAKE_BUCKET_NAME, BUCKET_NAME_USECASE)\n",
    "raw_data_path = os.path.join(datalake_base_path, 'EikonDataAPI')\n",
    "\n",
    "# S3 디렉토리 위치(data path)\n",
    "staged_data_dir = os.path.join(usecase_base_path, keychain['S3_PATH_STAGE'], KST_aday_before.strftime('%Y/%m/%d'),str(timestamp))\n",
    "golden_data_dir = os.path.join(usecase_base_path, keychain['S3_PATH_GOLDEN'], KST_aday_before.strftime('%Y/%m/%d'),str(timestamp))\n",
    "trained_model_dir = os.path.join(usecase_base_path, keychain['S3_PATH_TRAIN'], KST_aday_before.strftime('%Y/%m/%d'),str(timestamp))\n",
    "\n",
    "# 전처리 결과 데이터 위치(Golden data path)\n",
    "train_data_dir = os.path.join(golden_data_dir,'train')\n",
    "test_data_dir = os.path.join(golden_data_dir,'test')\n",
    "scaler_data_dir = os.path.join(golden_data_dir,'scaler-files')\n",
    "\n",
    "num_fold = '3'\n",
    "scaler_switch = '1' # '0': scaling, '1': scaling\n",
    "print(f\"Start job time: {KST}\")\n",
    "print(f\"code verison: {code_version}\")\n",
    "\n",
    "###################\n",
    "## 1) 데이터 전처리를 위한 파이프라인 변수  ####################################\n",
    "###################\n",
    "processing_instance_type = ParameterString(\n",
    "    name = \"ProcessingInstanceType\",\n",
    "    default_value = \"ml.c5.xlarge\" # cpu 성능이 더 중요하기 때문에 m5보다 비교적 가격이 저렴한 c5.xlarge를 선택하였다.\n",
    ")\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name = \"ProcessingInstanceCount\",\n",
    "    default_value = 1\n",
    ")\n",
    "###################\n",
    "## 2) 데이터 학습을 위한 파이프라인 변수  ####################################\n",
    "###################\n",
    "train_instance_type = ParameterString(\n",
    "    name = \"TrainingInstanceType\",\n",
    "    default_value = \"ml.m5.xlarge\"\n",
    ")\n",
    "train_instance_count = ParameterInteger(\n",
    "    name = \"TrainInstanceCount\",\n",
    "    default_value = 1\n",
    ")\n",
    "###################\n",
    "## 3) 모델 검증을 위한 파이프라인 변수  ####################################\n",
    "###################\n",
    "model_validation_instance_count = ParameterInteger(\n",
    "    name=\"ModelValidationInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "model_validation_instance_type = ParameterString(\n",
    "    name=\"ModelValidationInstanceType\",\n",
    "    default_value='ml.c5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa50b16-6c49-4b1f-b0d4-b2ebfb36683c",
   "metadata": {},
   "source": [
    "## 2.2 프로세서 단계 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6177b3b-24cb-4690-81e2-1aa8ee5747a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "skframework_version = \"1.0-1\"#\"0.23-1\"\n",
    "item = 'FCPOc3'\n",
    "target = 'y'\n",
    "metric = 'MAPE'\n",
    "quality = 'medium_quality'#'medium_quality'#'fast_training'\n",
    "###################\n",
    "# 1) 데이터 전처리 ###\n",
    "##################\n",
    "skprocessor_preprocessing = SKLearnProcessor(\n",
    "    framework_version = skframework_version,\n",
    "    instance_type = \"ml.c5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    base_job_name = f\"{BUCKET_NAME_USECASE}(Preprocessing)\",\n",
    "    role = role,\n",
    ")\n",
    "\n",
    "################\n",
    "# 2) 모델 학습 ###\n",
    "###############\n",
    "image_uri = retrieve(framework='mxnet',\n",
    "                     region='ap-northeast-2',\n",
    "                     version='1.9.0',\n",
    "                     py_version='py38',\n",
    "                     image_scope='training',\n",
    "                     instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "script_processor_training = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=image_uri,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    base_job_name = f\"{BUCKET_NAME_USECASE}(Train Model)\",\n",
    "    role = role,\n",
    ")\n",
    "################\n",
    "# 3) 모델 검증 ###\n",
    "###############\n",
    "skprocessor_model_validation = SKLearnProcessor(\n",
    "    framework_version = skframework_version,\n",
    "    instance_type = \"ml.c5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    base_job_name = f\"{BUCKET_NAME_USECASE}(Model Validation)\",\n",
    "    role = role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557e393-90b6-49a5-9a1e-89218ee04fbf",
   "metadata": {},
   "source": [
    "## 2.3 파이프라인 스텝 단계 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f3bb63-c9f0-4fe1-b676-34ed600d5414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# 1) 데이터 전처리 ###\n",
    "##################\n",
    "step_preprocessing = ProcessingStep(\n",
    "    name = f\"{BUCKET_NAME_USECASE}-Preprocessing\",\n",
    "    processor = skprocessor_preprocessing,\n",
    "    inputs = [\n",
    "        ProcessingInput(input_name = 'input_stage_data_path',\n",
    "                        source = raw_data_path,\n",
    "                        destination = '/opt/ml/processing/input'),\n",
    "    ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name = \"stage\",\n",
    "                         source = '/opt/ml/processing/output/stage',\n",
    "                         destination = staged_data_dir),\n",
    "        ProcessingOutput(output_name = \"scaler\",\n",
    "                         source = '/opt/ml/processing/output/scaler',\n",
    "                         destination = scaler_data_dir),        \n",
    "        ProcessingOutput(output_name = \"train\",\n",
    "                         source = '/opt/ml/processing/output/train',\n",
    "                         destination = train_data_dir),\n",
    "        ProcessingOutput(output_name = \"test\",\n",
    "                         source = '/opt/ml/processing/output/test',\n",
    "                         destination = test_data_dir),\n",
    "    ],\n",
    "    job_arguments = [\"--split_start\", split_start,\n",
    "                     \"--split_end\", split_end,\n",
    "                     \"--num_fold\", num_fold,\n",
    "                     \"--scaler_switch\", scaler_switch], \n",
    "    code = preprocessing_code,\n",
    ")\n",
    "################\n",
    "# 2) 모델 학습 ###\n",
    "###############\n",
    "\n",
    "step_train = ProcessingStep(\n",
    "    name = f\"{BUCKET_NAME_USECASE}-Training\",\n",
    "    processor = script_processor_training,\n",
    "    inputs=[\n",
    "        ProcessingInput(source = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                        destination = \"/opt/ml/processing/input/train\"),\n",
    "        ProcessingInput(source = step_preprocessing.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                        destination = \"/opt/ml/processing/input/test\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name = \"prediction_data\",\n",
    "                         source = \"/opt/ml/processing/output/prediction\",\n",
    "                         destination = f'{trained_model_dir}/prediction'),\n",
    "        ProcessingOutput(output_name = \"leaderboard_data\",\n",
    "                         source = \"/opt/ml/processing/output/leaderboard\",\n",
    "                         destination = f'{trained_model_dir}/leaderboard'),        \n",
    "        ProcessingOutput(output_name = \"model_data\",\n",
    "                         source = \"/opt/ml/processing/output/model\",\n",
    "                         destination = f'{trained_model_dir}/model'),        \n",
    "        ProcessingOutput(output_name = \"manifest_data\",\n",
    "                         source = \"/opt/ml/processing/output/manifest\",\n",
    "                         destination = f'{trained_model_dir}/manifest')\n",
    "        ],\n",
    "    job_arguments = [\"--item\", item,\n",
    "                     \"--target\", target,\n",
    "                     \"--metric\", metric,\n",
    "                     \"--quality\", quality],\n",
    "    code = training_code\n",
    ")\n",
    "\n",
    "################\n",
    "# 3) 모델 검증 ###\n",
    "###############\n",
    "# 참조:https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html#API_DescribeTrainingJob_ResponseSyntax\n",
    "step_model_validaion = ProcessingStep(\n",
    "    name = f\"{project_prefix}-Model_validation\",\n",
    "    processor = skprocessor_model_validation,\n",
    "    inputs=[\n",
    "        ProcessingInput(source = step_train.properties.ProcessingOutputConfig.Outputs[\"leaderboard_data\"].S3Output.S3Uri,\n",
    "                        destination = \"/opt/ml/processing/input/leaderboard\")\n",
    "    ],\n",
    "    job_arguments = [\"--algorithm_name\", 'Autogluon',\n",
    "                     \"--model_base_path\", step_train.properties.ProcessingOutputConfig.Outputs[\"model_data\"].S3Output.S3Uri,\n",
    "                     \"--manifest_base_path\", step_train.properties.ProcessingOutputConfig.Outputs[\"manifest_data\"].S3Output.S3Uri,\n",
    "                     \"--prediction_base_path\", step_train.properties.ProcessingOutputConfig.Outputs[\"prediction_data\"].S3Output.S3Uri,\n",
    "                     \"--threshold\", \"-100\",\n",
    "                     \"--model_package_group_name\", BUCKET_NAME_USECASE,\n",
    "                     \"--qs_data_name\", \"model_result\",\n",
    "              ],\n",
    "    code = model_validation_code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b079639-15f9-4a02-93eb-da899709bf5a",
   "metadata": {},
   "source": [
    "# 3. 최종 파이프라인 정의 및 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89624b5-bc17-4677-b1e6-660fd328c49e",
   "metadata": {},
   "source": [
    "### 3.1 최종 파이프라인 정의\n",
    "1. Processing\n",
    "   1. 데이터 추출: 적재된 DataLake에서 Use-Case에 맞는 Stage data를 추출 → S3(staged data 폴더) 적재\n",
    "   2. 데이터 정제: 추출된 Stage data를 불러옴 → 머신러닝(AI/ML) 입력값에 맞게 정제된 golden data로 변환 → S3(golden data폴더) 적재\n",
    "2. Training\n",
    "   1. 정제된 golden data를 불러옴 → 머신러닝(AI/ML) 학습 → 모델 파일과 모델 결과를 적재\n",
    "3. Model Validation\n",
    "   1. 1차 모델 검증 →  Model Registry 저장.\n",
    "   2. 비즈니스 담당자 검증을 위한 시각화 data source 업데이트.   \n",
    "   \n",
    "참조: [MLOps Pipeline](https://aws.amazon.com/ko/blogs/machine-learning/deploy-an-mlops-solution-that-hosts-your-model-endpoints-in-aws-lambda/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a1ce443-0600-42ad-b997-b3acf5bace2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = project_prefix\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        # 1) preprocessing's parameters \n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        # 2) training's parameters        \n",
    "        train_instance_type,        \n",
    "        train_instance_count,   \n",
    "        # 3) model validating's parameters\n",
    "        model_validation_instance_type,\n",
    "        model_validation_instance_count,\n",
    "    ],\n",
    "   steps=[step_preprocessing,\n",
    "          step_train,\n",
    "          step_model_validaion]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221059ed-17dc-475e-a866-1f4f5ff627f9",
   "metadata": {},
   "source": [
    "### 3.2 파이프라인 정의 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f889ea-c4ba-4984-b2e6-8ed8139a1c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'TrainInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'ModelValidationInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'ModelValidationInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'crude-palm-oil-prices-forecast-Preprocessing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "     'ContainerArguments': ['--split_start',\n",
       "      '2018-01-01',\n",
       "      '--split_end',\n",
       "      '2023-03-24',\n",
       "      '--num_fold',\n",
       "      '3',\n",
       "      '--scaler_switch',\n",
       "      '1'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "    'ProcessingInputs': [{'InputName': 'input_stage_data_path',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://ai-data-lake/crude-palm-oil-prices-forecast/EikonDataAPI',\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'stage',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/staged-data/2023/03/23/1679675822.0',\n",
       "        'LocalPath': '/opt/ml/processing/output/stage',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'scaler',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/23/1679675822.0/scaler-files',\n",
       "        'LocalPath': '/opt/ml/processing/output/scaler',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/23/1679675822.0/train',\n",
       "        'LocalPath': '/opt/ml/processing/output/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/golden-data/2023/03/23/1679675822.0/test',\n",
       "        'LocalPath': '/opt/ml/processing/output/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'crude-palm-oil-prices-forecast-Training',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/mxnet-training:1.9.0-cpu-py38',\n",
       "     'ContainerArguments': ['--item',\n",
       "      'FCPOc3',\n",
       "      '--target',\n",
       "      'y',\n",
       "      '--metric',\n",
       "      'MAPE',\n",
       "      '--quality',\n",
       "      'medium_quality'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/train.py']},\n",
       "    'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.crude-palm-oil-prices-forecast-Preprocessing.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/train',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.crude-palm-oil-prices-forecast-Preprocessing.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/train.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'prediction_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/prediction',\n",
       "        'LocalPath': '/opt/ml/processing/output/prediction',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'leaderboard_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/leaderboard',\n",
       "        'LocalPath': '/opt/ml/processing/output/leaderboard',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'model_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/model',\n",
       "        'LocalPath': '/opt/ml/processing/output/model',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'manifest_data',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/manifest',\n",
       "        'LocalPath': '/opt/ml/processing/output/manifest',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'crude-palm-oil-prices-forecast-Model_validation',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.c5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "     'ContainerArguments': ['--algorithm_name',\n",
       "      'Autogluon',\n",
       "      '--model_base_path',\n",
       "      {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['model_data'].S3Output.S3Uri\"},\n",
       "      '--manifest_base_path',\n",
       "      {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['manifest_data'].S3Output.S3Uri\"},\n",
       "      '--prediction_base_path',\n",
       "      {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['prediction_data'].S3Output.S3Uri\"},\n",
       "      '--threshold',\n",
       "      '-100',\n",
       "      '--model_package_group_name',\n",
       "      'crude-palm-oil-prices-forecast',\n",
       "      '--qs_data_name',\n",
       "      'model_result'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/model_validation.py']},\n",
       "    'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.crude-palm-oil-prices-forecast-Training.ProcessingOutputConfig.Outputs['leaderboard_data'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/input/leaderboard',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/model_validation.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}]}}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f77f33-f76e-442d-aaef-88f5e3956b9e",
   "metadata": {},
   "source": [
    "### 3.3 파이프라인 정의를 제출하고 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41d22ff4-d0cb-4178-8d59-7b0e6838797d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "WaiterError",
     "evalue": "Waiter PipelineExecutionComplete failed: Max attempts exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaiterError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline.py:586\u001b[0m, in \u001b[0;36m_PipelineExecution.wait\u001b[0;34m(self, delay, max_attempts)\u001b[0m\n\u001b[1;32m    557\u001b[0m model \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mwaiter\u001b[38;5;241m.\u001b[39mWaiterModel(\n\u001b[1;32m    558\u001b[0m     {\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m     }\n\u001b[1;32m    582\u001b[0m )\n\u001b[1;32m    583\u001b[0m waiter \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mwaiter\u001b[38;5;241m.\u001b[39mcreate_waiter_with_client(\n\u001b[1;32m    584\u001b[0m     waiter_id, model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39msagemaker_client\n\u001b[1;32m    585\u001b[0m )\n\u001b[0;32m--> 586\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPipelineExecutionArn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/waiter.py:55\u001b[0m, in \u001b[0;36mcreate_waiter_with_client.<locals>.wait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mWaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/waiter.py:388\u001b[0m, in \u001b[0;36mWaiter.wait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m         reason \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    385\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMax attempts exceeded. Previously accepted state: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    386\u001b[0m             \u001b[38;5;241m%\u001b[39m (acceptor\u001b[38;5;241m.\u001b[39mexplanation)\n\u001b[1;32m    387\u001b[0m         )\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WaiterError(\n\u001b[1;32m    389\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    390\u001b[0m         reason\u001b[38;5;241m=\u001b[39mreason,\n\u001b[1;32m    391\u001b[0m         last_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep_amount)\n",
      "\u001b[0;31mWaiterError\u001b[0m: Waiter PipelineExecutionComplete failed: Max attempts exceeded"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "pipeline.upsert(role_arn = sagemaker.get_execution_role())\n",
    "execution = pipeline.start()\n",
    "#실행이 완료될 때까지 기다린다.\n",
    "execution.wait() \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ead1a434-b86c-4397-904a-2b3b37cb9ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터 전처리~모델 검증시간 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mend\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터 전처리~모델 검증시간 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((end\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'end' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"데이터 전처리~모델 검증시간 : {end - start:.1f} sec\")\n",
    "print(f\"데이터 전처리~모델 검증시간 : {((end - start)/60):.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9753aed-e16b-44c0-9178-e17467b5e710",
   "metadata": {},
   "source": [
    "[23년 3월 1일 테스트]    \n",
    "- 데이터 전처리 시간(초) = 20m22s\n",
    "- 모델링 시간(초) = 12m54s\n",
    "- 모델 검증 시간(초) = 4m22s\n",
    "- 총계: 37m42s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041cb8c9-62f1-41b1-909c-832285bdcb6c",
   "metadata": {},
   "source": [
    "### 3.4 Debug Model Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f1efc9-3f08-4d28-8d8c-9b8e0faf8581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:pipeline/crude-palm-oil-prices-forecast',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:pipeline/crude-palm-oil-prices-forecast/execution/o0i36853z9qt',\n",
       " 'PipelineExecutionDisplayName': 'execution-1679643425098',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'crude-palm-oil-prices-forecast',\n",
       "  'TrialName': 'o0i36853z9qt'},\n",
       " 'CreationTime': datetime.datetime(2023, 3, 24, 7, 37, 5, 28000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 3, 24, 8, 32, 30, 877000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': 'f3b18869-08b2-4d62-bc9a-890347be3de7',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f3b18869-08b2-4d62-bc9a-890347be3de7',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '541',\n",
       "   'date': 'Fri, 24 Mar 2023 08:35:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f174e9-f39a-4973-af7f-dbc398840e11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineSummaries': [{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:pipeline/crude-palm-oil-prices-forecast',\n",
       "   'PipelineName': 'crude-palm-oil-prices-forecast',\n",
       "   'PipelineDisplayName': 'crude-palm-oil-prices-forecast',\n",
       "   'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       "   'CreationTime': datetime.datetime(2023, 1, 5, 23, 2, 43, 874000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2023, 3, 24, 8, 32, 30, 881000, tzinfo=tzlocal())}],\n",
       " 'ResponseMetadata': {'RequestId': '870f4846-cd87-4229-9137-fe31d82dbfba',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '870f4846-cd87-4229-9137-fe31d82dbfba',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '400',\n",
       "   'date': 'Fri, 24 Mar 2023 08:35:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.list_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36c4b365-3adc-4c86-8146-64ca2d4ea645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'crude-palm-oil-prices-forecast-Model_validation',\n",
       "  'StartTime': datetime.datetime(2023, 3, 24, 8, 28, 2, 772000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2023, 3, 24, 8, 32, 30, 352000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-o0i36853z9qt-crude-palm-oil-price-vcuzd43ncw'}}},\n",
       " {'StepName': 'crude-palm-oil-prices-forecast-Training',\n",
       "  'StartTime': datetime.datetime(2023, 3, 24, 7, 57, 59, 474000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2023, 3, 24, 8, 28, 1, 796000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-o0i36853z9qt-crude-palm-oil-price-ebaejxc2jb'}}},\n",
       " {'StepName': 'crude-palm-oil-prices-forecast-Preprocessing',\n",
       "  'StartTime': datetime.datetime(2023, 3, 24, 7, 37, 6, 306000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2023, 3, 24, 7, 57, 58, 527000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-o0i36853z9qt-crude-palm-oil-price-kmqquxaamk'}}}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7ce10a3-1827-4f9d-8433-e4453e5f8185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'StepName': 'crude-palm-oil-prices-forecast-Preprocessing',\n",
       " 'StartTime': datetime.datetime(2023, 3, 24, 7, 37, 6, 306000, tzinfo=tzlocal()),\n",
       " 'EndTime': datetime.datetime(2023, 3, 24, 7, 57, 58, 527000, tzinfo=tzlocal()),\n",
       " 'StepStatus': 'Succeeded',\n",
       " 'AttemptCount': 0,\n",
       " 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-o0i36853z9qt-crude-palm-oil-price-kmqquxaamk'}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c52e7329-d8c9-4c85-a2ab-3d584299c3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'input-1',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/leaderboard',\n",
       "    'LocalPath': '/opt/ml/processing/input/leaderboard',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'code',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://crude-palm-oil-prices-forecast/src/model_validation.py',\n",
       "    'LocalPath': '/opt/ml/processing/input/code',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingJobName': 'pipelines-o0i36853z9qt-crude-palm-oil-price-VCUzd43ncw',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.c5.xlarge',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "  'ContainerEntrypoint': ['python3',\n",
       "   '/opt/ml/processing/input/code/model_validation.py'],\n",
       "  'ContainerArguments': ['--algorithm_name',\n",
       "   'Autogluon',\n",
       "   '--model_base_path',\n",
       "   's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/model',\n",
       "   '--manifest_base_path',\n",
       "   's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/manifest',\n",
       "   '--prediction_base_path',\n",
       "   's3://crude-palm-oil-prices-forecast/trained-model/2023/03/23/1679675822.0/prediction',\n",
       "   '--threshold',\n",
       "   '-100',\n",
       "   '--model_package_group_name',\n",
       "   'crude-palm-oil-prices-forecast',\n",
       "   '--qs_data_name',\n",
       "   'model_result']},\n",
       " 'RoleArn': 'arn:aws:iam::108594546720:role/service-role/AmazonSageMaker-ExecutionRole-20220901T154875',\n",
       " 'ExperimentConfig': {'ExperimentName': 'crude-palm-oil-prices-forecast',\n",
       "  'TrialName': 'o0i36853z9qt'},\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:ap-northeast-2:108594546720:processing-job/pipelines-o0i36853z9qt-crude-palm-oil-price-vcuzd43ncw',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2023, 3, 24, 8, 32, 29, 163000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2023, 3, 24, 8, 31, 57, 911000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 3, 24, 8, 32, 29, 724000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2023, 3, 24, 8, 28, 3, 162000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '3d972769-26dc-4067-9a1a-033af5941a44',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3d972769-26dc-4067-9a1a-033af5941a44',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2070',\n",
       "   'date': 'Fri, 24 Mar 2023 08:38:23 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_response = execution.list_steps()[0]\n",
    "processing_arn = processing_response['Metadata']['ProcessingJob']['Arn'] # index -1은 가장 처음 실행 step\n",
    "processing_job_name = processing_arn.split('/')[-1] # Processing job name만 추출\n",
    "processing_response = sm_client.describe_processing_job(ProcessingJobName = processing_job_name)\n",
    "processing_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba68fc-eb52-4c11-8faf-289c3aed50f3",
   "metadata": {},
   "source": [
    "```\n",
    "WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6ec5e-1baf-4c1e-b058-07408327f6a8",
   "metadata": {},
   "source": [
    "# 4. Future Works\n",
    "참고사항: 앞으로 얼마나 예측을 할것인지에 대해서 split_date를 설정하여 앞으로 몇일을 예측할 수 있다.(현재 분기점은 '2022-10-31' 기준으로 되어있다.)\n",
    "\n",
    "- Multi-model을 적용하는 multi pipeline\n",
    "- CodeCommit을 통한 소스코드 관리\n",
    "- sagemaker pipeline clarify 사용하여 data drift monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
