{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9efc2f-b20b-44eb-a54c-5a33e80e22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BUCKET = False\n",
    "LOCAL_MODE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710b146-a1f9-439a-8511-ed4db6406282",
   "metadata": {},
   "source": [
    "# 0. 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c7ec85-958f-4ef8-94b3-d19c1b7241f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip list | grep sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f127cd75-1fdc-497d-ada3-c19c61621331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7605b6c-12a1-4d8f-8c85-76c87e8ed096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72da0ad-857f-45c7-93c1-cc2e62a3c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "import time\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0c4f76-bae9-4028-acac-8b32bfe1553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "keychain = json.loads(get_secret())\n",
    "ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "S3_PATH_FORECAST = keychain['S3_PATH_FORECAST']\n",
    "\n",
    "boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "sm_session = sagemaker.Session(boto_session = boto3_session)\n",
    "region = boto3_session.region_name\n",
    "\n",
    "s3_resource = boto3_session.resource('s3')\n",
    "bucket = s3_resource.Bucket(BUCKET_NAME_USECASE)\n",
    "s3_client = boto3_session.client('s3')\n",
    "sm_client = boto3.client('sagemaker',\n",
    "                         aws_access_key_id = ACCESS_KEY_ID,\n",
    "                         aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                         region_name = 'ap-northeast-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f77e4f-f8bb-4683-bc58-9ebd2a1cd16c",
   "metadata": {},
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44712718-06a6-40c6-becc-5170e87db860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket:  palm-oil-price-forecast\n"
     ]
    }
   ],
   "source": [
    "if DEFAULT_BUCKET:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "else:\n",
    "    bucket = keychain['BUCKET_NAME_USECASE_ent']\n",
    "print(\"bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e4ecf-6d63-4f0c-a2cd-c64eaec089ec",
   "metadata": {},
   "source": [
    "## 1) 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9e26ae-2b24-42f4-b750-5d50730ea248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2022-12-23 17:38:58.078718\n"
     ]
    }
   ],
   "source": [
    "# 한국 시간\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "print(f\"Start job time: {KST}\")\n",
    "# 프로젝트 변수\n",
    "project_prefix = bucket\n",
    "base = f\"s3://{bucket}\"\n",
    "\n",
    "# 전처리 결과 데이터 위치(Golden data path)\n",
    "preproc_data_dir = f\"{base}/{keychain['S3_PATH_GOLDEN']}/{KST.strftime('%Y/%m/%d')}\"\n",
    "\n",
    "# stage_data_uri= f\"{preproc_data_dir}/stage.csv\"\n",
    "stage_data_uri = f\"{base}/{keychain['S3_PATH_STAGE']}\"\n",
    "train_data_uri = f\"{preproc_data_dir}/train.csv\"\n",
    "test_data_uri = f\"{preproc_data_dir}/test.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd2dfe-602b-47b6-9464-1617a4f3349a",
   "metadata": {},
   "source": [
    "## 2) 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3528151d-20cf-4da3-bfc1-bd6f7f144560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cec506e-61e9-4b67-965d-2b0d2222e342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'project_prefix' (str)\n",
      "Stored 'preproc_data_dir' (str)\n",
      "Stored 'stage_data_uri' (str)\n",
      "Stored 'train_data_uri' (str)\n",
      "Stored 'test_data_uri' (str)\n",
      "Stored 'bucket' (str)\n"
     ]
    }
   ],
   "source": [
    "%store project_prefix\n",
    "%store preproc_data_dir\n",
    "\n",
    "%store stage_data_uri\n",
    "%store train_data_uri\n",
    "%store test_data_uri\n",
    "\n",
    "%store bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e919d-2505-4b7a-9f01-e4b79003a28b",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7a9ff-4996-4a31-a1a1-23312c83e1e9",
   "metadata": {},
   "source": [
    "## 1) 전처리에 사용할 데이터를 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a001d1-1c2f-4000-bc94-c6fcc3c88048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be194f8-c729-4ab5-b3a6-20497cde9bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket                       -> 'palm-oil-price-forecast'\n",
      "preproc_data_dir             -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n",
      "project_prefix               -> 'palm-oil-price-forecast'\n",
      "stage_data_uri               -> 's3://palm-oil-price-forecast/staged-data'\n",
      "test_data_uri                -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n",
      "train_data_uri               -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d2744-471d-42ad-9c97-1cd733845314",
   "metadata": {},
   "source": [
    "### (1) 전처리용 python script\n",
    "- 인자값: https://engineer-mole.tistory.com/213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f7dc759-b3bf-46b0-b3ca-3ab2fcdcc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/v1.1/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/v1.1/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY=','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCHI','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'FKLIc1', 'FKLIc2', 'FKLIc3','FTSE','GCc1', 'GCc2', 'GCc3','GDAXI','GENMKL','HSI','IOIBKL','IXIC','JNIc1','JNIc2','JNIc3','KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2', 'MCCc3','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL','SSEC', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  ric,\n",
    "                  s3_resource,\n",
    "                  BUCKET_NAME_USECASE,\n",
    "                  S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(scaler, fp)\n",
    "        fp.seek(0)\n",
    "        s3_resource.put_object(Body = fp.read(),\n",
    "                               Bucket = BUCKET_NAME_USECASE,\n",
    "                               Key = f\"{S3_PATH_GOLDEN}/{KST.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--split_date', type=str, default=KST.strftime('%Y-%m-%d'))\n",
    "    parser.add_argument('--num_fold', type=str, default='5')       \n",
    "\n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = '1', help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "        \n",
    "    # parse arguments\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_date: {args.split_date}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")\n",
    "    logger.info(f\"args.num_fold: {args.num_fold}\")\n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_date = args.split_date\n",
    "    num_fold = int(args.num_fold)\n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    \n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    stage_dir = f'{base_output_dir}/stage'\n",
    "    logger.info(f\"\\n### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    for file in path_list:\n",
    "        df_tmp= pd.read_csv(file, encoding='utf-8') \n",
    "        df_sum = pd.concat([df_sum, df_tmp])\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{stage_dir}/stage_integrated.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    \n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    \n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= '2014-07-02'].reset_index(drop = True)\n",
    "        \n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if scaler_switch == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value, col, name, s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "        \n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    df_train_fold0 = df_golden[df_golden['ds'] < split_date]\n",
    "\n",
    "    # train 데이터 나누기\n",
    "    for cnt in range(num_fold):\n",
    "        split_date = (dt.strptime(split_date, '%Y-%m-%d') - relativedelta(days=30)).strftime('%Y-%m-%d')\n",
    "    \n",
    "        logger.info(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < {split_date}]\")\n",
    "        exec(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < split_date]\")\n",
    "        exec(f\"df_train_fold{cnt+1}.to_csv('{base_output_dir}/train/train_fold{cnt+1}.csv', index = False)\")\n",
    "\n",
    "        logger.info(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= {split_date}]\")\n",
    "        exec(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= split_date]\")\n",
    "        exec(f\"df_test_fold{cnt+1}.to_csv('{base_output_dir}/test/test_fold{cnt+1}.csv', index = False)\")\n",
    "    \n",
    "#     logger.info(f\"\\n ### Final result for train dataset \")\n",
    "#     eval(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= split_date]\")\n",
    "\n",
    "#     logger.info(f\"\\n ####preprocessed train shape \\n {df_train.shape}\")        \n",
    "#     logger.info(f\"preprocessed train sample: head(2) \\n df_test_fold{cnt+1}\")\n",
    "#     logger.info(f\"preprocessed train sample: tail(2) \\n {df_train.tail(2)}\")\n",
    "    \n",
    "#     logger.info(f\"\\n ####preprocessed test shape \\n {df_test.shape}\")            \n",
    "#     logger.info(f\"preprocessed test sample: head(2) \\n {df_test_fold{cnt+1}.head(2)}\")\n",
    "#     logger.info(f\"preprocessed test sample: tail(2) \\n {df_test_fold{cnt+1}.tail(2)}\")\n",
    "\n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70bbdea-cbdb-4df6-b57d-29afde3c386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기점: 2022-12-23\n",
      "학습에 사용되는 구간: 2022-11-23\n",
      "실제 예측에 사용되는 구간: 2023-01-22\n"
     ]
    }
   ],
   "source": [
    "# split_date = '2022-10-31'\n",
    "print(f\"기점: {KST.strftime('%Y-%m-%d')}\") \n",
    "print(f\"학습에 사용되는 구간: {(KST - relativedelta(days=30)).strftime('%Y-%m-%d')}\") \n",
    "print(f\"실제 예측에 사용되는 구간: {(KST + relativedelta(days=30)).strftime('%Y-%m-%d')}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "397c3246-a7be-4843-a469-7de9c7158ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'preprocessing_code' (str)\n"
     ]
    }
   ],
   "source": [
    "preprocessing_code = 'src/v1.1/preprocessing.py'\n",
    "%store preprocessing_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0497db-0d18-4745-b130-058d8d19a355",
   "metadata": {},
   "source": [
    "## 2) 전처리 로직 로컬에서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f9681-748a-4ede-a0e7-4f8185e27696",
   "metadata": {},
   "source": [
    "### (1) SageMaker Processing의 Docker Container와 같은 환경 구성\n",
    "도커 컨테이너의 출력 폴더와 비슷한 환경 기술\n",
    "- 로컬 경로 : opt/ml/processing/output\n",
    "- 도커 경로 : /opt/ml/processing/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04ccd49-34ea-43a9-9eef-a72df0b56586",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_MODE:\n",
    "    # 도커 컨테이너 입력 폴더: staged data가 들어가는 부분\n",
    "    base_preproc_input_dir = 'opt/ml/processing/input'\n",
    "    os.makedirs(base_preproc_input_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 기본 출력 폴더\n",
    "    base_output_dir = 'opt/ml/processing/output'\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 출력 폴더: stage 데이터셋이 들어가는 부분\n",
    "    base_preproc_output_stage_dir = f'{base_output_dir}/stage'\n",
    "    os.makedirs(base_preproc_output_stage_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 출력 폴더: train 데이터셋이 들어가는 부분\n",
    "    base_preproc_output_train_dir = f'{base_output_dir}/train'\n",
    "    os.makedirs(base_preproc_output_train_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 출력 폴더: test 데이터셋이 들어가는 부분\n",
    "    base_preproc_output_test_dir =  f'{base_output_dir}/test'\n",
    "    os.makedirs(base_preproc_output_test_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3a5ebe-c11d-44c1-88eb-41b3618286ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !aws s3 cp 's3://palm-oil-price-forecast/staged-data/' 'opt/ml/processing/input2' --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8694e79d-ab59-4967-bb48-502833822e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !python src/v1.1/preprocessing.py --base_preproc_input_dir 'opt/ml/processing/input2' \\\n",
    "#                                   --base_output_dir 'opt/ml/processing/output' \\\n",
    "#                                   --split_date '2022-12-22' \\\n",
    "#                                   --num_fold '5' \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5b3ea-5e32-4a69-96c1-24530d10223b",
   "metadata": {},
   "source": [
    "### (2) 전처리된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d394b3a7-726c-4afa-8d8b-38799e5ca65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_MODE:\n",
    "    preprocessed_stage_path = os.path.join(base_preproc_output_stage_dir + '/stage.csv')\n",
    "    preprocessed_train_path = os.path.join(base_preproc_output_train_dir + '/train.csv')\n",
    "    preprocessed_test_path = os.path.join(base_preproc_output_test_dir + '/test.csv')\n",
    "\n",
    "    preprocessed_stage_df = pd.read_csv(preprocessed_stage_path)\n",
    "    preprocessed_train_df = pd.read_csv(preprocessed_train_path)\n",
    "    preprocessed_test_df = pd.read_csv(preprocessed_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f8ac38-6400-4a4d-8562-7534bf9f077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_MODE:\n",
    "    print(\"##Stage Data Set: ##\")\n",
    "    print(preprocessed_stage_df[['RIC']].value_counts())\n",
    "\n",
    "    print(\"\\n##Train Data Set: ##\")\n",
    "    print(preprocessed_train_df[['ric']].value_counts())\n",
    "\n",
    "    print(\"\\n##Test Data Set: ##\")\n",
    "    print(preprocessed_test_df[['ric']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61997838-0504-4f18-b07f-5072591e0698",
   "metadata": {},
   "source": [
    "## 3) 모델 빌딩 파이프라인 의 스텝(Step) 생성\n",
    "### 3.1) 모델 빌딩 파이프라인 변수 생성\n",
    "파이프라인에서 사용할 파이프라인 파라미터를 정의합니다. 파이프라인을 스케줄하고 실행할 때 파라미터를 이용하여 실행조건을 커스마이징할 수 있습니다. 파라미터를 이용하면 파이프라인 실행시마다 매번 파이프라인 정의를 수정하지 않아도 됩니다.\n",
    "\n",
    "지원되는 파라미터 타입은 다음과 같습니다:\n",
    "\n",
    "- ParameterString - 파이썬 타입에서 str\n",
    "- ParameterInteger - 파이썬 타입에서 int\n",
    "- ParameterFloat - 파이썬 타입에서 float\n",
    "이들 파라미터를 정의할 때 디폴트 값을 지정할 수 있으며 파이프라인 실행시 재지정할 수도 있습니다. 지정하는 디폴트 값은 파라미터 타입과 일치하여야 합니다.\n",
    "\n",
    "파이프라인의 각 스텝에서 사용할 변수를 파라미터 변수로서 정의 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "639db5a6-5095-4892-b937-df8ebb3094e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name = \"ProcessingInstanceCount\",\n",
    "    default_value = 1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name = \"ProcessingInstanceType\",\n",
    "    default_value = \"ml.m5.xlarge\"\n",
    ")\n",
    "input_stage_data = ParameterString(\n",
    "    name = \"InputStageData\",\n",
    "    default_value = stage_data_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852c37d-bb51-44fd-8773-bb96ec3c8af1",
   "metadata": {},
   "source": [
    "### 3.2) 전처리 스텝 프로세서 정의\n",
    "전처리의 내장 SKLearnProcessor 를 통해서 sklearn_processor 오브젝트를 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0bacbb3-08d8-4988-ab34-2f41302781de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version = framework_version,\n",
    "    instance_type = processing_instance_type,\n",
    "    instance_count = processing_instance_count,\n",
    "    base_job_name = \"Palm_oil_forecast-Data_transform\",\n",
    "    role = sagemaker.get_execution_role(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d79514-a6a3-43be-ac1f-95e26d555d62",
   "metadata": {},
   "source": [
    "### 3.3) 전처리 스텝 단계 정의\n",
    "처리 단계에서는 아래와 같은 주요 인자가 있습니다.\n",
    "단계 이름\n",
    "- processor 기술: 위에서 생성한 processor 오브젝트를 제공\n",
    "- inputs: S3의 경로를 기술하고, 다커안에서의 다운로드 폴더(destination)을 기술 합니다.\n",
    "- outputs: 처리 결과가 저장될 다커안에서의 폴더 경로를 기술합니다.\n",
    "\n",
    "도커안의 결과 파일이 저장 후에 자동으로 S3로 업로딩을 합니다.\n",
    "- job_arguments: 사용자 정의의 인자를 기술 합니다.\n",
    "- code: 전처리 코드의 경로를 기술 합니다.\n",
    "처리 단계의 상세한 사항은 여기를 보세요. --> 처리 단계, Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d525f7c-f109-4e62-9eb3-571c12307094",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = KST.strftime('%Y-%m-%d')\n",
    "scaler_switch = '1' # 0: scaling, 1: scaling\n",
    "num_fold = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79c775cd-3015-446d-b50a-3a813ee80b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "step_process = ProcessingStep(\n",
    "    name = \"Palm_oil_forecast-Processing\",\n",
    "    processor = sklearn_processor,\n",
    "    inputs = [\n",
    "        ProcessingInput(source = stage_data_uri,\n",
    "                        destination = '/opt/ml/processing/input'),\n",
    "    ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name = \"stage\",\n",
    "                         source = '/opt/ml/processing/output/stage',\n",
    "                         destination = preproc_data_dir),\n",
    "        ProcessingOutput(output_name = \"train\",\n",
    "                         source = '/opt/ml/processing/output/train',\n",
    "                         destination = preproc_data_dir),\n",
    "        ProcessingOutput(output_name = \"test\",\n",
    "                         source = '/opt/ml/processing/output/test',\n",
    "                         destination = preproc_data_dir),\n",
    "    ],\n",
    "    job_arguments = [\"--split_date\", split_date,\n",
    "                     \"--num_fold\", num_fold,\n",
    "                     \"--scaler_switch\", scaler_switch], \n",
    "    code = preprocessing_code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b0251-deba-4e08-8f1e-2e9471b7f5ee",
   "metadata": {},
   "source": [
    "## 4) 파리마터, 단계, 조건을 조합하여 최종 파이프라인 정의 및 실행\n",
    "이제 지금까지 생성한 단계들을 하나의 파이프라인으로 조합하고 실행하도록 하겠습니다.\n",
    "\n",
    "파이프라인은 name, parameters, steps 속성이 필수적으로 필요합니다. 여기서 파이프라인의 이름은 (account, region) 조합에 대하여 유일(unique))해야 합니다.\n",
    "\n",
    "주의:\n",
    "\n",
    "- 정의에 사용한 모든 파라미터가 존재해야 합니다.\n",
    "- 파이프라인으로 전달된 단계(step)들은 실행순서와는 무관합니다. SageMaker Pipeline은 단계가 실행되고 완료될 수 있도록 의존관계를를 해석합니다.\n",
    "- [알림] 정의한 stpes 이 복수개이면 복수개를 기술합니다. 만약에 step 간에 의존성이 있으면, 명시적으로 기술하지 않아도 같이 실행 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "018d6940-1468-4aae-b116-02a92ad82c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = project_prefix\n",
    "pipeline = Pipeline(name = pipeline_name,\n",
    "                    parameters = [\n",
    "                        processing_instance_type, \n",
    "                        processing_instance_count,\n",
    "                        input_stage_data,\n",
    "                    ],\n",
    "                    steps = [step_process],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2205be8-13bf-439d-acbc-d4b15d0fbfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'InputStageData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://palm-oil-price-forecast/staged-data'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'Palm_oil_forecast-Processing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--split_date',\n",
       "      '2022-12-23',\n",
       "      '--num_fold',\n",
       "      '5',\n",
       "      '--scaler_switch',\n",
       "      '1'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::276114397529:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://palm-oil-price-forecast/staged-data',\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-276114397529/Palm_oil_forecast-Processing-c54213135ceee3d3ec4dbbca60f7334d/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'stage',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/23',\n",
       "        'LocalPath': '/opt/ml/processing/output/stage',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/23',\n",
       "        'LocalPath': '/opt/ml/processing/output/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/23',\n",
       "        'LocalPath': '/opt/ml/processing/output/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b1a5d-b571-4309-a8a0-17502b611842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "\n",
    "pipeline.upsert(role_arn = sagemaker.get_execution_role())\n",
    "execution = pipeline.start()\n",
    "execution.wait() \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c20e1ee-49c8-4371-b529-afb56e4caa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing 시간 : 965.7 sec\n",
      "preprocessing 시간 : 16.1 min\n"
     ]
    }
   ],
   "source": [
    "print(f\"preprocessing 시간 : {end - start:.1f} sec\")\n",
    "print(f\"preprocessing 시간 : {((end - start)/60):.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f60c8c-f31b-44b0-bd45-f34266eaab90",
   "metadata": {},
   "source": [
    "[2022-12-22(목) 1차]   \n",
    "- preprocessing 시간 : 995.5 sec   \n",
    "- preprocessing 시간 : 16.6 min \n",
    "\n",
    "[2022-12-22(목) 2차]     \n",
    "- preprocessing 시간 : 965.6 sec\n",
    "- preprocessing 시간 : 16.1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b25250c7-6936-4bdb-ae6e-8f7fa88da6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:276114397529:pipeline/palm-oil-price-forecast',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-northeast-2:276114397529:pipeline/palm-oil-price-forecast/execution/jv4fio0cjv8t',\n",
       " 'PipelineExecutionDisplayName': 'execution-1671784837935',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'palm-oil-price-forecast',\n",
       "  'TrialName': 'jv4fio0cjv8t'},\n",
       " 'CreationTime': datetime.datetime(2022, 12, 23, 8, 40, 37, 869000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 12, 23, 8, 56, 21, 148000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '10af1e31-c93c-4d72-9305-dda040f9aca1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '10af1e31-c93c-4d72-9305-dda040f9aca1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '520',\n",
       "   'date': 'Fri, 23 Dec 2022 08:56:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df10ebf-17da-49f2-8ab9-df0a92dc004c",
   "metadata": {},
   "source": [
    "# 3. 전처리 결과파일 경로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3bc4f9d-15f0-4a8a-b886-d064ac5ada28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proc_artifact(execution, client, kind=0):\n",
    "    '''\n",
    "    kind: 0 --> stage\n",
    "    kind: 1 --> train\n",
    "    kind: 2 --> test\n",
    "    '''\n",
    "    response = execution.list_steps()\n",
    "    proc_arn = response[-1]['Metadata']['ProcessingJob']['Arn'] # index -1은 가장 처음 실행 step\n",
    "    proc_job_name = proc_arn.split('/')[-1] # Processing job name만 추출\n",
    "    response = client.describe_processing_job(ProcessingJobName = proc_job_name)\n",
    "    file_uri = response['ProcessingOutputConfig']['Outputs'][kind]['S3Output']['S3Uri']\n",
    "    return file_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ff3d648-51cd-4e07-a2a7-9772610cb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_train_uri = get_proc_artifact(execution, sm_client, kind=1) + '/train.csv'\n",
    "# preprocessed_test_uri = get_proc_artifact(execution, sm_client, kind=2) + '/test.csv'\n",
    "\n",
    "# exec(f\"df_train_fold{cnt+1}.to_csv('{base_output_dir}/train/train_fold{cnt+1}.csv', index = False)\")\n",
    "\n",
    "# print(\"\\ntrain_preproc_dir_artifact: \", preprocessed_train_uri)\n",
    "# print(\"\\ntest_preproc__dir_artifact: \", preprocessed_test_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73c295d1-606f-41d3-adff-faec961e7722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_preproc_dir_artifact1 = preprocessed_base_uri + '/train_fold1.csv'\n",
      "test_preproc_dir_artifact1 = preprocessed_base_uri + '/test_fold1.csv'\n",
      "train_preproc_dir_artifact2 = preprocessed_base_uri + '/train_fold2.csv'\n",
      "test_preproc_dir_artifact2 = preprocessed_base_uri + '/test_fold2.csv'\n",
      "train_preproc_dir_artifact3 = preprocessed_base_uri + '/train_fold3.csv'\n",
      "test_preproc_dir_artifact3 = preprocessed_base_uri + '/test_fold3.csv'\n",
      "train_preproc_dir_artifact4 = preprocessed_base_uri + '/train_fold4.csv'\n",
      "test_preproc_dir_artifact4 = preprocessed_base_uri + '/test_fold4.csv'\n",
      "train_preproc_dir_artifact5 = preprocessed_base_uri + '/train_fold5.csv'\n",
      "test_preproc_dir_artifact5 = preprocessed_base_uri + '/test_fold5.csv'\n"
     ]
    }
   ],
   "source": [
    "preprocessed_base_uri = get_proc_artifact(execution, sm_client, kind=1)\n",
    "\n",
    "for cnt in range(1, int(num_fold)+1):\n",
    "    print(f\"train_preproc_dir_artifact{cnt} = preprocessed_base_uri + '/train_fold{cnt}.csv'\")\n",
    "    exec(f\"train_preproc_dir_artifact{cnt} = preprocessed_base_uri + '/train_fold{cnt}.csv'\")\n",
    "    \n",
    "    print(f\"test_preproc_dir_artifact{cnt} = preprocessed_base_uri + '/test_fold{cnt}.csv'\")\n",
    "    exec(f\"test_preproc_dir_artifact{cnt} = preprocessed_base_uri + '/test_fold{cnt}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "070629ac-2226-47b3-a135-be5a8807c945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>y</th>\n",
       "      <th>ric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-07-02</td>\n",
       "      <td>38.959999</td>\n",
       "      <td>38.480000</td>\n",
       "      <td>38.869999</td>\n",
       "      <td>38.470001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-03</td>\n",
       "      <td>38.660000</td>\n",
       "      <td>38.340000</td>\n",
       "      <td>38.470001</td>\n",
       "      <td>38.560001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-04</td>\n",
       "      <td>38.590000</td>\n",
       "      <td>38.219999</td>\n",
       "      <td>38.495001</td>\n",
       "      <td>38.460001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>38.520000</td>\n",
       "      <td>38.099998</td>\n",
       "      <td>38.520000</td>\n",
       "      <td>38.360001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-08</td>\n",
       "      <td>38.410000</td>\n",
       "      <td>37.770000</td>\n",
       "      <td>38.410000</td>\n",
       "      <td>37.799999</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221120</th>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>858.750000</td>\n",
       "      <td>833.750000</td>\n",
       "      <td>858.750000</td>\n",
       "      <td>846.500000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221121</th>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>845.750000</td>\n",
       "      <td>825.500000</td>\n",
       "      <td>844.250000</td>\n",
       "      <td>834.750000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221122</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>844.250000</td>\n",
       "      <td>829.000000</td>\n",
       "      <td>835.000000</td>\n",
       "      <td>830.750000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221123</th>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>838.750000</td>\n",
       "      <td>823.750000</td>\n",
       "      <td>831.000000</td>\n",
       "      <td>825.875000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221124</th>\n",
       "      <td>2022-11-22</td>\n",
       "      <td>833.250000</td>\n",
       "      <td>818.500000</td>\n",
       "      <td>827.000000</td>\n",
       "      <td>821.000000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221125 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ds        high         low        open           y   ric\n",
       "0       2014-07-02   38.959999   38.480000   38.869999   38.470001  BOc1\n",
       "1       2014-07-03   38.660000   38.340000   38.470001   38.560001  BOc1\n",
       "2       2014-07-04   38.590000   38.219999   38.495001   38.460001  BOc1\n",
       "3       2014-07-07   38.520000   38.099998   38.520000   38.360001  BOc1\n",
       "4       2014-07-08   38.410000   37.770000   38.410000   37.799999  BOc1\n",
       "...            ...         ...         ...         ...         ...   ...\n",
       "221120  2022-11-16  858.750000  833.750000  858.750000  846.500000   Wc3\n",
       "221121  2022-11-17  845.750000  825.500000  844.250000  834.750000   Wc3\n",
       "221122  2022-11-18  844.250000  829.000000  835.000000  830.750000   Wc3\n",
       "221123  2022-11-21  838.750000  823.750000  831.000000  825.875000   Wc3\n",
       "221124  2022-11-22  833.250000  818.500000  827.000000  821.000000   Wc3\n",
       "\n",
       "[221125 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(train_preproc_dir_artifact1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "575baade-e135-4ffc-b973-69df7b7f6ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_preproc_dir_artifact1' (str)\n",
      "Stored 'test_preproc_dir_artifact1' (str)\n",
      "Stored 'train_preproc_dir_artifact2' (str)\n",
      "Stored 'test_preproc_dir_artifact2' (str)\n",
      "Stored 'train_preproc_dir_artifact3' (str)\n",
      "Stored 'test_preproc_dir_artifact3' (str)\n",
      "Stored 'train_preproc_dir_artifact4' (str)\n",
      "Stored 'test_preproc_dir_artifact4' (str)\n",
      "Stored 'train_preproc_dir_artifact5' (str)\n",
      "Stored 'test_preproc_dir_artifact5' (str)\n"
     ]
    }
   ],
   "source": [
    "%store train_preproc_dir_artifact1\n",
    "%store test_preproc_dir_artifact1\n",
    "\n",
    "%store train_preproc_dir_artifact2\n",
    "%store test_preproc_dir_artifact2\n",
    "\n",
    "%store train_preproc_dir_artifact3\n",
    "%store test_preproc_dir_artifact3\n",
    "\n",
    "%store train_preproc_dir_artifact4\n",
    "%store test_preproc_dir_artifact4\n",
    "\n",
    "%store train_preproc_dir_artifact5\n",
    "%store test_preproc_dir_artifact5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2986f7-6151-442f-8251-faa8ca84f7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
