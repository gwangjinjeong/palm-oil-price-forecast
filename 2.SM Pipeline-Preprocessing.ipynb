{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9efc2f-b20b-44eb-a54c-5a33e80e22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BUCKET = False\n",
    "LOCAL_MODE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710b146-a1f9-439a-8511-ed4db6406282",
   "metadata": {},
   "source": [
    "# 0. 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c7ec85-958f-4ef8-94b3-d19c1b7241f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip list | grep sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f127cd75-1fdc-497d-ada3-c19c61621331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7605b6c-12a1-4d8f-8c85-76c87e8ed096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72da0ad-857f-45c7-93c1-cc2e62a3c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import *\n",
    "import time\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0c4f76-bae9-4028-acac-8b32bfe1553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "keychain = json.loads(get_secret())\n",
    "ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "S3_PATH_FORECAST = keychain['S3_PATH_FORECAST']\n",
    "\n",
    "boto3_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "sm_session = sagemaker.Session(boto_session = boto3_session)\n",
    "region = boto3_session.region_name\n",
    "\n",
    "s3_resource = boto3_session.resource('s3')\n",
    "bucket = s3_resource.Bucket(BUCKET_NAME_USECASE)\n",
    "s3_client = boto3_session.client('s3')\n",
    "sm_client = boto3.client('sagemaker',\n",
    "                         aws_access_key_id = ACCESS_KEY_ID,\n",
    "                         aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                         region_name = 'ap-northeast-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f77e4f-f8bb-4683-bc58-9ebd2a1cd16c",
   "metadata": {},
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44712718-06a6-40c6-becc-5170e87db860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket:  palm-oil-price-forecast\n"
     ]
    }
   ],
   "source": [
    "if DEFAULT_BUCKET:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "else:\n",
    "    bucket = keychain['BUCKET_NAME_USECASE_ent']\n",
    "print(\"bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e4ecf-6d63-4f0c-a2cd-c64eaec089ec",
   "metadata": {},
   "source": [
    "## 1) 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9e26ae-2b24-42f4-b750-5d50730ea248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job time: 2022-12-09 07:49:03.790647\n"
     ]
    }
   ],
   "source": [
    "# 한국 시간\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "print(f\"Start job time: {KST}\")\n",
    "# 프로젝트 변수\n",
    "project_prefix = bucket\n",
    "base = f\"s3://{bucket}\"\n",
    "\n",
    "# 전처리 결과 데이터 위치(Golden data path)\n",
    "preproc_data_dir = f\"{base}/{keychain['S3_PATH_GOLDEN']}/{KST.strftime('%Y/%m/%d')}\"\n",
    "\n",
    "# stage_data_uri= f\"{preproc_data_dir}/stage.csv\"\n",
    "stage_data_uri = f\"{base}/{keychain['S3_PATH_STAGE']}\"\n",
    "train_data_uri = f\"{preproc_data_dir}/train.csv\"\n",
    "test_data_uri = f\"{preproc_data_dir}/test.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd2dfe-602b-47b6-9464-1617a4f3349a",
   "metadata": {},
   "source": [
    "## 2) 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3528151d-20cf-4da3-bfc1-bd6f7f144560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cec506e-61e9-4b67-965d-2b0d2222e342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'project_prefix' (str)\n",
      "Stored 'preproc_data_dir' (str)\n",
      "Stored 'stage_data_uri' (str)\n",
      "Stored 'train_data_uri' (str)\n",
      "Stored 'test_data_uri' (str)\n",
      "Stored 'bucket' (str)\n"
     ]
    }
   ],
   "source": [
    "%store project_prefix\n",
    "%store preproc_data_dir\n",
    "\n",
    "%store stage_data_uri\n",
    "%store train_data_uri\n",
    "%store test_data_uri\n",
    "\n",
    "%store bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e919d-2505-4b7a-9f01-e4b79003a28b",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7a9ff-4996-4a31-a1a1-23312c83e1e9",
   "metadata": {},
   "source": [
    "## 1) 전처리에 사용할 데이터를 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a001d1-1c2f-4000-bc94-c6fcc3c88048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be194f8-c729-4ab5-b3a6-20497cde9bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket                       -> 'palm-oil-price-forecast'\n",
      "preproc_data_dir             -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n",
      "project_prefix               -> 'palm-oil-price-forecast'\n",
      "stage_data_uri               -> 's3://palm-oil-price-forecast/staged-data'\n",
      "test_data_uri                -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n",
      "train_data_uri               -> 's3://palm-oil-price-forecast/golden-data/2022/12/\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d2744-471d-42ad-9c97-1cd733845314",
   "metadata": {},
   "source": [
    "### (1) 전처리용 python script\n",
    "- 인자값: https://engineer-mole.tistory.com/213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb855db-4a16-4cc6-93c7-abfac42ae313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY=','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCHI','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'FKLIc1', 'FKLIc2', 'FKLIc3','FTSE','GCc1', 'GCc2', 'GCc3','GDAXI','GENMKL','HSI','IOIBKL','IXIC','JNIc1','JNIc2','JNIc3','KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2', 'MCCc3','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL','SSEC', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  ric,\n",
    "                  s3_resource,\n",
    "                  BUCKET_NAME_USECASE,\n",
    "                  S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(scaler, fp)\n",
    "        fp.seek(0)\n",
    "        s3_resource.put_object(Body = fp.read(),\n",
    "                               Bucket = BUCKET_NAME_USECASE,\n",
    "                               Key = f\"{S3_PATH_GOLDEN}/{KST.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    split_date_default = dt.today() + relativedelta(hours = 9) - relativedelta(months=1)\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--split_date', type=str, default=split_date_default.strftime('%Y-%m-%d'))       \n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = 1, help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "        \n",
    "    # parse arguments\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_date: {args.split_date}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")   \n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_date = args.split_date    \n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    stage_dir = f'{base_output_dir}/stage'\n",
    "    logger.info(f\"\\n### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    for file in path_list:\n",
    "        df_tmp= pd.read_csv(file, encoding='utf-8') \n",
    "        df_sum = pd.concat([df_sum, df_tmp])\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{stage_dir}/stage_integrated.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    \n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= '2014-07-02'].reset_index(drop = True)\n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if int(scaler_switch) == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value, col, name, s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    \n",
    "    # train 데이터 나누기\n",
    "    df_train = df_golden[df_golden['ds'] < split_date]\n",
    "    df_train.to_csv(f\"{base_output_dir}/train/train.csv\", index = False)\n",
    "    \n",
    "    df_test = df_golden[df_golden['ds'] >= split_date]\n",
    "    df_test.to_csv(f\"{base_output_dir}/test/test.csv\", index = False)\n",
    "    \n",
    "    logger.info(f\"\\n ### Final result for train dataset \")\n",
    "    logger.info(f\"\\n ####preprocessed train shape \\n {df_train.shape}\")        \n",
    "    logger.info(f\"preprocessed train sample: head(2) \\n {df_train.head(2)}\")\n",
    "    logger.info(f\"preprocessed train sample: tail(2) \\n {df_train.tail(2)}\")\n",
    "    \n",
    "    logger.info(f\"\\n ####preprocessed test shape \\n {df_test.shape}\")            \n",
    "    logger.info(f\"preprocessed test sample: head(2) \\n {df_test.head(2)}\")\n",
    "    logger.info(f\"preprocessed test sample: tail(2) \\n {df_test.tail(2)}\")\n",
    "\n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f7dc759-b3bf-46b0-b3ca-3ab2fcdcc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/preprocessing_fold.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/preprocessing_fold.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "###############################\n",
    "######### 전역변수 설정 ##########\n",
    "###############################\n",
    "KST = dt.today() + relativedelta(hours=9)\n",
    "ric_list = ['BOc1', 'BOc2', 'BOc3','BOPLKL','BRRTSc1', 'BRRTSc2', 'BRRTSc3', 'CAD=', 'EUR=', 'JPY=', 'KRW=', 'MYR=', 'GBP=', 'INR=','Cc1', 'Cc2', 'Cc3','CCMc1', 'CCMc2', 'CCMc3',\n",
    "            'CLc1', 'CLc2', 'CLc3','CNY=','COMc1', 'COMc2','COMc3','CTc1', 'CTc2', 'CTc3', 'DJCI', 'DJCIBR', 'DJCICL', 'DJCICN', 'DJCIEN', 'DJCIGR', 'DJCIIA', 'DJCING', \n",
    "            'DJCISO', 'DJCIWH', 'DJT','FCHI','FCPOc1', 'FCPOc2', 'FCPOc3','FGVHKL',\n",
    "            'FKLIc1', 'FKLIc2', 'FKLIc3','FTSE','GCc1', 'GCc2', 'GCc3','GDAXI','GENMKL','HSI','IOIBKL','IXIC','JNIc1','JNIc2','JNIc3','KCc1', 'KCc2', 'KCc3','KLKKKL','KLSE','KQ11', 'KS11',\n",
    "            'KWc1', 'KWc2', 'KWc3','LCOc1', 'LCOc2', 'LCOc3','LWBc1', 'LWBc2', 'LWBc3','MCCc1', 'MCCc2', 'MCCc3','MXSCKL','Oc1', 'Oc2', 'Oc3','PEPTKL','RRc1', 'RRc2', 'RRc3','RSc1', 'RSc2', 'RSc3',\n",
    "            'Sc1', 'Sc2', 'Sc3','SIMEKL','SOPSKL','SSEC', 'THPBKL', 'Wc1', 'Wc2', 'Wc3'\n",
    "           ]\n",
    "\n",
    "col_names_asis = ['ds','high','low','open','ric']\n",
    "col_names_tobe = ['ds','high','low','open','y']\n",
    "\n",
    "###############################\n",
    "######### util 함수 설정 ##########\n",
    "###############################\n",
    "def _get_logger():\n",
    "    '''\n",
    "    로깅을 위해 파이썬 로거를 사용\n",
    "    # https://stackoverflow.com/questions/17745914/python-logging-module-is-printing-lines-multiple-times\n",
    "    '''\n",
    "    loglevel = logging.DEBUG\n",
    "    l = logging.getLogger(__name__)\n",
    "    if not l.hasHandlers():\n",
    "        l.setLevel(loglevel)\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))        \n",
    "        l.handler_set = True\n",
    "    return l  \n",
    "logger = _get_logger()\n",
    "\n",
    "def download_object(file_name):\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        download_path = Path('test') / file_name.replace('/','_')\n",
    "        s3_client.download_file(\n",
    "            BUCKET_NAME_USECASE,\n",
    "            file_name,\n",
    "            str(download_path)\n",
    "        )\n",
    "        return \"Success\"\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "def download_parallel_multiprocessing(path_list):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        future_to_key = {executor.submit(download_object, key): key for key in path_list}\n",
    "        for future in futures.as_completed(future_to_key):\n",
    "            key = future_to_key[future]\n",
    "            exception = future.exception()\n",
    "            if not exception:\n",
    "                yield key, future.result()\n",
    "            else:\n",
    "                yield key, exception\n",
    "                                \n",
    "def get_list_in_s3(key_id : str,\n",
    "                   secret_key_id : str,\n",
    "                   bucket_name : str,\n",
    "                   s3_path : str) -> list:\n",
    "    \n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id = ACCESS_KEY_ID,\n",
    "                      aws_secret_access_key = ACCESS_SECRET_KEY,\n",
    "                      region_name = 'ap-northeast-2')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket = bucket_name,\n",
    "                               Prefix = s3_path)  # 원하는 bucket 과 하위경로에 있는 object list # dict type\n",
    "    contents_list = [] # object list의 Contents를 가져옴\n",
    "    for page in pages:\n",
    "        for obj in page['Contents']:\n",
    "            contents_list.append(obj)\n",
    "    return contents_list\n",
    "\n",
    "def get_file_folders(s3_client, bucket_name, prefix=\"\"):\n",
    "    file_names = []\n",
    "    folders = []\n",
    "\n",
    "    default_kwargs = {\n",
    "        \"Bucket\": bucket_name,\n",
    "        \"Prefix\": prefix\n",
    "    }\n",
    "    next_token = \"\"\n",
    "\n",
    "    while next_token is not None:\n",
    "        updated_kwargs = default_kwargs.copy()\n",
    "        if next_token != \"\":\n",
    "            updated_kwargs[\"ContinuationToken\"] = next_token\n",
    "\n",
    "        response = s3_client.list_objects_v2(**default_kwargs)\n",
    "        contents = response.get(\"Contents\")\n",
    "\n",
    "        for result in contents:\n",
    "            key = result.get(\"Key\")\n",
    "            if key[-1] == \"/\":\n",
    "                folders.append(key)\n",
    "            else:\n",
    "                file_names.append(key)\n",
    "\n",
    "        next_token = response.get(\"NextContinuationToken\")\n",
    "\n",
    "    return file_names, folders\n",
    "\n",
    "\n",
    "def download_files(s3_client, bucket_name, local_path, file_names, folders):\n",
    "\n",
    "    local_path = Path(local_path)\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = Path.joinpath(local_path, folder)\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = Path.joinpath(local_path, file_name)\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3_client.download_file(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            str(file_path)\n",
    "        )\n",
    "        \n",
    "def get_dataframe(base_preproc_input_dir, file_name_prefix ):    \n",
    "    '''\n",
    "    파일 이름이 들어가 있는 csv 파일을 모두 저장하여 데이터 프레임을 리턴\n",
    "    '''\n",
    "    \n",
    "    input_files = glob('{}/{}*.csv'.format(base_preproc_input_dir, file_name_prefix))\n",
    "    #claim_input_files = glob('{}/dataset*.csv'.format(base_preproc_input_dir))    \n",
    "    logger.info(f\"input_files: \\n {input_files}\")    \n",
    "    \n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(base_preproc_input_dir, \"train\"))\n",
    "        \n",
    "    raw_data = [ pd.read_csv(file, index_col=0) for file in input_files ]\n",
    "    df = pd.concat(raw_data)\n",
    "   \n",
    "    logger.info(f\"dataframe shape \\n {df.shape}\")    \n",
    "    logger.info(f\"dataset sample \\n {df.head(2)}\")        \n",
    "    #logger.info(f\"df columns \\n {df.columns}\")    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_secret():\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    secret_name = \"prod/sagemaker\"\n",
    "    region_name = \"ap-northeast-2\"\n",
    "    \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId='prod/sagemaker',\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'DecryptionFailureException': # Secrets Manager can't decrypt the protected secret text using the provided KMS key.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InternalServiceErrorException': # An error occurred on the server side.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidParameterException': # You provided an invalid value for a parameter.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'InvalidRequestException': # You provided a parameter value that is not valid for the current state of the resource.\n",
    "            raise e\n",
    "        elif e.response['Error']['Code'] == 'ResourceNotFoundException': # We can't find the resource that you asked for.\n",
    "            raise e\n",
    "    else:\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "        \n",
    "def fill_missing_dates(df_in : pd.DataFrame,\n",
    "                       freq : str\n",
    "                      ) -> pd.DataFrame : \n",
    "    df = df_in.copy()\n",
    "    if df[\"ds\"].dtype == np.int64:\n",
    "            df.loc[:, \"ds\"] = df.loc[:, \"ds\"].astype(str)\n",
    "    df.loc[:, \"ds\"] = pd.to_datetime(df.loc[:, \"ds\"])\n",
    "    r = pd.date_range(start = df[\"ds\"].min(),\n",
    "                      end = df[\"ds\"].max(),\n",
    "                      freq = freq)\n",
    "    df = df.set_index(\"ds\").reindex(r).rename_axis(\"ds\").reset_index()\n",
    "    return df\n",
    "\n",
    "def fill_missing_price_value(df: pd.DataFrame, col: str, limit_linear : int = 20 ) -> pd.DataFrame :\n",
    "    initial_is_na = sum(df[col].isnull())\n",
    "    series = df.loc[:, col].astype(float)\n",
    "    series = series.interpolate(method=\"linear\", limit=limit_linear, limit_direction=\"both\")\n",
    "    series = [0 if v < 0 else v for v in series]\n",
    "    df[col] = series\n",
    "    return df\n",
    "\n",
    "def scaling_value(df : pd.DataFrame,\n",
    "                  col_name : str,\n",
    "                  ric,\n",
    "                  s3_resource,\n",
    "                  BUCKET_NAME_USECASE,\n",
    "                  S3_PATH_GOLDEN) -> tuple:\n",
    "\n",
    "    series = df[col_name].values\n",
    "    scaler = MinMaxScaler()\n",
    "    series = series.reshape(-1,1)\n",
    "    scaler.fit(series)\n",
    "    series = scaler.transform(series)\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        joblib.dump(scaler, fp)\n",
    "        fp.seek(0)\n",
    "        s3_resource.put_object(Body = fp.read(),\n",
    "                               Bucket = BUCKET_NAME_USECASE,\n",
    "                               Key = f\"{S3_PATH_GOLDEN}/{KST.strftime('%Y/%m/%d')}/scaler-files/{ric}_{col_name}_scaler.pkl\")\n",
    "    return series\n",
    "\n",
    "def convert_type(raw, cols, type_target):\n",
    "    '''\n",
    "    해당 데이터 타입으로 변경\n",
    "    '''\n",
    "    df = raw.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_target)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__=='__main__':\n",
    "    ################################\n",
    "    ###### 커맨드 인자 파싱   ##########\n",
    "    ################################\n",
    "    split_date_default = dt.today() + relativedelta(hours = 9) - relativedelta(months=1)\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--base_output_dir', type=str, default=\"/opt/ml/processing/output\")\n",
    "    parser.add_argument('--base_preproc_input_dir', type=str, default=\"/opt/ml/processing/input\")   \n",
    "    parser.add_argument('--split_date', type=str, default=split_date_default.strftime('%Y-%m-%d'))       \n",
    "    parser.add_argument('--label_column', type=str, default=\"ric\") \n",
    "    parser.add_argument(\"--scaler_switch\", type = str, default = 1, help = '1이면 Scaling ON, 0이면 Scaling OFF')\n",
    "        \n",
    "    # parse arguments\n",
    "    args = parser.parse_args()     \n",
    "\n",
    "    logger.info(\"######### Argument Info ####################################\")\n",
    "    logger.info(f\"args.base_output_dir: {args.base_output_dir}\")\n",
    "    logger.info(f\"args.base_preproc_input_dir: {args.base_preproc_input_dir}\")    \n",
    "    logger.info(f\"args.label_column: {args.label_column}\")        \n",
    "    logger.info(f\"args.split_date: {args.split_date}\")   \n",
    "    logger.info(f\"args.scaler_switch: {args.scaler_switch}\")   \n",
    "    \n",
    "    base_output_dir = args.base_output_dir\n",
    "    base_preproc_input_dir = args.base_preproc_input_dir\n",
    "    label_column = args.label_column\n",
    "    split_date = args.split_date    \n",
    "    scaler_switch = int(args.scaler_switch)\n",
    "    ############################################\n",
    "    ###### Secret Manager에서 키값 가져오기  #######\n",
    "    ########################################### \n",
    "    logger.info(f\"\\n### Loading the key value using Secret Manager\")\n",
    "\n",
    "    keychain = json.loads(get_secret())\n",
    "    ACCESS_KEY_ID = keychain['ACCESS_KEY_ID_ent']\n",
    "    ACCESS_SECRET_KEY = keychain['ACCESS_SECRET_KEY_ent']\n",
    "\n",
    "    BUCKET_NAME_USECASE = keychain['BUCKET_NAME_USECASE_ent']\n",
    "    S3_PATH_STAGE = keychain['S3_PATH_STAGE']\n",
    "    S3_PATH_GOLDEN = keychain['S3_PATH_GOLDEN']\n",
    "    S3_PATH_TRAIN = keychain['S3_PATH_TRAIN']\n",
    "    S3_PATH_log = keychain['S3_PATH_LOG']\n",
    "\n",
    "    boto_session = boto3.Session(ACCESS_KEY_ID, ACCESS_SECRET_KEY)\n",
    "    region = boto_session.region_name\n",
    "    s3_resource = boto_session.resource('s3')\n",
    "    s3_client = boto_session.client('s3')\n",
    "    ############################################\n",
    "    ###### 1. 데이터 Integration  #######\n",
    "    ########################################### \n",
    "    total_start = time.time()\n",
    "    start = time.time()\n",
    "    stage_dir = f'{base_output_dir}/stage'\n",
    "    logger.info(f\"\\n### Data Integration\")\n",
    "    path_list = []\n",
    "    df_sum = pd.DataFrame()\n",
    "\n",
    "    for (path, dir, files) in os.walk(base_preproc_input_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == '.csv':\n",
    "                path_list.append(\"%s/%s\" % (path, filename))\n",
    "                \n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    for file in path_list:\n",
    "        df_tmp= pd.read_csv(file, encoding='utf-8') \n",
    "        df_sum = pd.concat([df_sum, df_tmp])\n",
    "    df_sum = df_sum.sort_values(by='Date').reset_index(drop=True)\n",
    "    df_sum.to_csv(f\"{stage_dir}/stage_integrated.csv\", index = False)\n",
    "    end = time.time()\n",
    "    \n",
    "    logger.info(f\"Data Integration is done\")\n",
    "    logger.info(f\"Runtime : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "    logger.info(f\"The number for data : {len(path_list)}\")\n",
    "    logger.info(f\"Integrated data sample: head(2) \\n {df_sum.head(2)}\")\n",
    "    logger.info(f\"Integrated data sample: tail(2) \\n {df_sum.tail(2)}\")\n",
    "    \n",
    "    #################################\n",
    "    ####   2. 첫번쨰 전처리 단계     ####\n",
    "    ####   품목선별, 열 삭제, 형변환  ####\n",
    "    ################################    \n",
    "    start = time.time()\n",
    "    logger.info(f\"\\n ### RIC Item selection\")    \n",
    "    df_sum = df_sum[df_sum['RIC'].isin(ric_list)].reset_index()\n",
    "    logger.info(f\"The number for data after RIC Item selection : {df_sum.shape}\")\n",
    "\n",
    "    logger.info(f\"\\n ### Column selection\")    \n",
    "    df_sum = df_sum[['Date','HIGH', 'LOW', 'OPEN', 'CLOSE','RIC']]\n",
    "    logger.info(f\"The number for data after Column selection : {df_sum.shape}\")\n",
    "    logger.info(f\"\\n ### type conversion\")    \n",
    "    df_sum.loc[:, \"Date\"] = pd.to_datetime(df_sum.loc[:, \"Date\"])\n",
    "    df_sum.loc[:, \"HIGH\"] = df_sum.loc[:, \"HIGH\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"LOW\"] = df_sum.loc[:, \"LOW\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"OPEN\"] = df_sum.loc[:, \"OPEN\"].astype(np.float32)\n",
    "    df_sum.loc[:, \"CLOSE\"] = df_sum.loc[:, \"CLOSE\"].astype(np.float32)\n",
    "    ####################################################\n",
    "    ####   3. Autogluon timeseries 데이터 셋으로 만들기  ####\n",
    "    ####################################################\n",
    "    logger.info(f\"\\n ### Autogluon timeseriesdataframe Conversion\")        \n",
    "    df_list = OrderedDict()\n",
    "    for name in ric_list:\n",
    "        df_tmp = df_sum[df_sum['RIC'] == name]\n",
    "        df_tmp = df_tmp.drop('RIC', axis=1)\n",
    "        df_list[name] = df_tmp[df_tmp['Date'] >= '2014-07-02'].reset_index(drop = True)\n",
    "    ####################################################\n",
    "    ############   4. 열 이름 변경, 결측치 처리  ############\n",
    "    ###################################################\n",
    "    logger.info(f\"\\n ### Rename columns\")        \n",
    "    col_names = ['ds','high','low','open','y']\n",
    "    for name, value in df_list.items():\n",
    "        df_list[name].columns = col_names\n",
    "\n",
    "    logger.info(f\"\\n ### Fill missing value (Date)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_list[name]  = fill_missing_dates(value, 'B')\n",
    "        num_added = len(df_list[name]) - len(value)\n",
    "        is_na = sum(df_list[name]['y'].isnull())\n",
    "    \n",
    "    logger.info(f\"\\n ### Fill missing value (Price)\")        \n",
    "    for name, value in df_list.items():\n",
    "        df_proc1 = fill_missing_price_value(value, 'y')\n",
    "        df_proc1 = fill_missing_price_value(value, 'high')\n",
    "        df_proc1 = fill_missing_price_value(value, 'low')\n",
    "        df_proc1 = fill_missing_price_value(value, 'open')\n",
    "        df_list[name] = df_proc1\n",
    "        \n",
    "    ####################################################\n",
    "    #################   5. Scaling  ###################\n",
    "    ###################################################\n",
    "    if int(scaler_switch) == 1:\n",
    "        logger.info(f\"\\n ### Scaling\")            \n",
    "        scale_dir = f\"{base_output_dir}/scaler-files\"\n",
    "        os.makedirs(scale_dir, exist_ok=True)\n",
    "        for name, value in df_list.items():\n",
    "            for col in ['y','high','open','low']:\n",
    "                value.loc[:, col] = scaling_value(value, col, name, s3_client, BUCKET_NAME_USECASE, S3_PATH_GOLDEN)\n",
    "            df_list[name] = value\n",
    "    else:\n",
    "        logger.info(f\"\\n ### No Scaling\")\n",
    "    end = time.time()\n",
    "    logger.info(f\"\\n### All Date Transform is done\")\n",
    "    print(f\"All Date Transform Run time : {end - start:.1f} sec({((end - start)/60):.1f} min)\")\n",
    "\n",
    "    #################################################\n",
    "    #####   6. 훈련, 테스트 데이터 세트로 분리 및 저장  ######\n",
    "    #################################################\n",
    "    logger.info(f\"\\n ### Split train, test dataset\")            \n",
    "    df_golden = pd.DataFrame()\n",
    "    for name, value in df_list.items():\n",
    "        value = value.assign(ric = name)\n",
    "        df_golden = pd.concat([df_golden, value])\n",
    "    df_golden = df_golden.reset_index(drop = True)\n",
    "    \n",
    "    df_train_fold1 = df_golden[df_golden['ds'] < split_date]\n",
    "    df_train_fold1.to_csv(f\"{base_output_dir}/train/train_fold1.csv\", index = False)\n",
    "    df_test_fold1 = df_golden[df_golden['ds'] >= split_date]\n",
    "    df_test_fold1.to_csv(f\"{base_output_dir}/test/test_fold2.csv\", index = False)\n",
    "\n",
    "    # train 데이터 나누기\n",
    "    for cnt in range(1,5):\n",
    "        logger.info(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < {split_date}]\")\n",
    "        exec(f\"df_train_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] < split_date]\")\n",
    "        exec(f\"df_train_fold{cnt+1}.to_csv('{base_output_dir}/train/train_fold{cnt+1}.csv', index = False)\")\n",
    "\n",
    "        logger.info(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= {split_date}]\")\n",
    "        exec(f\"df_test_fold{cnt+1} = df_train_fold{cnt}[df_train_fold{cnt}['ds'] >= split_date]\")\n",
    "        exec(f\"df_test_fold{cnt+1}.to_csv('{base_output_dir}/test/test_fold{cnt+1}.csv', index = False)\")\n",
    "    \n",
    "    logger.info(f\"\\n ### Final result for train dataset \")\n",
    "    logger.info(f\"\\n ####preprocessed train shape \\n {df_train.shape}\")        \n",
    "    logger.info(f\"preprocessed train sample: head(2) \\n {df_train.head(2)}\")\n",
    "    logger.info(f\"preprocessed train sample: tail(2) \\n {df_train.tail(2)}\")\n",
    "    \n",
    "    logger.info(f\"\\n ####preprocessed test shape \\n {df_test.shape}\")            \n",
    "    logger.info(f\"preprocessed test sample: head(2) \\n {df_test.head(2)}\")\n",
    "    logger.info(f\"preprocessed test sample: tail(2) \\n {df_test.tail(2)}\")\n",
    "\n",
    "    logger.info(f\"\\n### End All of data preprocessing\")\n",
    "    total_end = time.time()\n",
    "    print(f\"Run time 시간 : {total_end - total_start:.1f} sec({((total_end - total_start)/60):.1f} min)\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cc57e4-dc0f-4d64-af06-3ddb54fe9c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-10-09'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dt.now() - relativedelta(days=60)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23894e1b-355f-421a-8f25-90e5b9b2b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f70bbdea-cbdb-4df6-b57d-29afde3c386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_date = '2022-10-31'\n",
    "split_date = (dt.now() - relativedelta(days=30)).strftime('%Y-%m-%d') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "397c3246-a7be-4843-a469-7de9c7158ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'preprocessing_code' (str)\n"
     ]
    }
   ],
   "source": [
    "preprocessing_code = 'src/preprocessing.py'\n",
    "%store preprocessing_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0497db-0d18-4745-b130-058d8d19a355",
   "metadata": {},
   "source": [
    "## 2) 전처리 로직 로컬에서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f9681-748a-4ede-a0e7-4f8185e27696",
   "metadata": {},
   "source": [
    "### (1) SageMaker Processing의 Docker Container와 같은 환경 구성\n",
    "도커 컨테이너의 출력 폴더와 비슷한 환경 기술\n",
    "- 로컬 경로 : opt/ml/processing/output\n",
    "- 도커 경로 : /opt/ml/processing/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d04ccd49-34ea-43a9-9eef-a72df0b56586",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_MODE:\n",
    "    # 도커 컨테이너 입력 폴더: staged data가 들어가는 부분\n",
    "    base_preproc_input_dir = 'opt/ml/processing/input'\n",
    "    os.makedirs(base_preproc_input_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 기본 출력 폴더\n",
    "    base_output_dir = 'opt/ml/processing/output'\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 출력 폴더: stage 데이터셋이 들어가는 부분\n",
    "    base_preproc_output_stage_dir = f'{base_output_dir}/stage'\n",
    "    os.makedirs(base_preproc_output_stage_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 출력 폴더: train 데이터셋이 들어가는 부분\n",
    "    base_preproc_output_train_dir = f'{base_output_dir}/train'\n",
    "    os.makedirs(base_preproc_output_train_dir, exist_ok=True)\n",
    "\n",
    "    # 도커 컨테이너 출력 폴더: test 데이터셋이 들어가는 부분\n",
    "    base_preproc_output_test_dir =  f'{base_output_dir}/test'\n",
    "    os.makedirs(base_preproc_output_test_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea3a5ebe-c11d-44c1-88eb-41b3618286ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 535 ms, sys: 131 ms, total: 666 ms\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!aws s3 cp 's3://palm-oil-price-forecast/staged-data/' 'opt/ml/processing/input2' --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8694e79d-ab59-4967-bb48-502833822e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Argument Info ####################################\n",
      "args.base_output_dir: opt/ml/processing/output\n",
      "args.base_preproc_input_dir: opt/ml/processing/input2\n",
      "args.label_column: ric\n",
      "args.split_date: 2022-10-31\n",
      "args.scaler_switch: 0\n",
      "\n",
      "### Loading the key value using Secret Manager\n",
      "\n",
      "### Data Integration\n",
      "The number for data : 12531\n",
      "Data Integration is done\n",
      "Runtime : 236.7 sec(3.9 min)\n",
      "The number for data : 12531\n",
      "Integrated data sample: head(2) \n",
      "          Date    HIGH     LOW    OPEN   CLOSE  VOLUME  RIC\n",
      "0  1980-01-01  107.94  107.94  107.94  107.94     NaN  SPX\n",
      "1  1980-01-01     NaN     NaN     NaN  879.38     NaN  HSI\n",
      "Integrated data sample: tail(2) \n",
      "               Date      HIGH        LOW  ...      CLOSE        VOLUME   RIC\n",
      "896249  2022-12-06  1473.520  1467.1300  ...  1471.5500  2.252947e+08  KLSE\n",
      "896250  2022-12-06  3224.822  3195.0788  ...  3212.5334  3.735903e+10  SSEC\n",
      "\n",
      "[2 rows x 7 columns]\n",
      "\n",
      " ### RIC Item selection\n",
      "The number for data after RIC Item selection : (877162, 8)\n",
      "\n",
      " ### Column selection\n",
      "The number for data after Column selection : (877162, 6)\n",
      "\n",
      " ### type conversion\n",
      "\n",
      " ### Autogluon timeseriesdataframe Conversion\n",
      "\n",
      " ### Rename columns\n",
      "\n",
      " ### Fill missing value (Date)\n",
      "\n",
      " ### Fill missing value (Price)\n",
      "\n",
      " ### No Scaling\n",
      "\n",
      "### All Date Transform is done\n",
      "All Date Transform Run time : 8.1 sec(0.1 min)\n",
      "\n",
      " ### Split train, test dataset\n",
      "\n",
      " ### Final result for train dataset \n",
      "\n",
      " ####preprocessed train shape \n",
      " (219408, 6)\n",
      "preprocessed train sample: head(2) \n",
      "           ds       high    low       open          y   ric\n",
      "0 2014-07-02  38.959999  38.48  38.869999  38.470001  BOc1\n",
      "1 2014-07-03  38.660000  38.34  38.470001  38.560001  BOc1\n",
      "preprocessed train sample: tail(2) \n",
      "                ds    high     low    open       y  ric\n",
      "222106 2022-10-27  885.75  863.25  872.50  868.00  Wc3\n",
      "222107 2022-10-28  869.25  853.25  869.25  861.25  Wc3\n",
      "\n",
      " ####preprocessed test shape \n",
      " (2727, 6)\n",
      "preprocessed test sample: head(2) \n",
      "              ds       high        low   open          y   ric\n",
      "2173 2022-10-31  74.279999  72.500000  72.50  73.239998  BOc1\n",
      "2174 2022-11-01  74.570000  72.739998  73.18  73.290001  BOc1\n",
      "preprocessed test sample: tail(2) \n",
      "                ds    high     low   open       y  ric\n",
      "222133 2022-12-05  779.75  745.75  775.0  751.75  Wc3\n",
      "222134 2022-12-06  754.50  735.00  751.0  741.75  Wc3\n",
      "\n",
      "### End All of data preprocessing\n",
      "Run time 시간 : 246.9 sec(4.1 min)\n",
      "\n",
      "CPU times: user 1.81 s, sys: 599 ms, total: 2.41 s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python src/preprocessing.py --base_preproc_input_dir 'opt/ml/processing/input2' \\\n",
    "                                 --base_output_dir 'opt/ml/processing/output' \\\n",
    "                                 --split_date '2022-10-31' \\\n",
    "                                 # --scaler_switch 0 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5b3ea-5e32-4a69-96c1-24530d10223b",
   "metadata": {},
   "source": [
    "### (2) 전처리된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d394b3a7-726c-4afa-8d8b-38799e5ca65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:#LOCAL_MODE:\n",
    "    preprocessed_stage_path = os.path.join(base_preproc_output_stage_dir + '/stage.csv')\n",
    "    preprocessed_train_path = os.path.join(base_preproc_output_train_dir + '/train.csv')\n",
    "    preprocessed_test_path = os.path.join(base_preproc_output_test_dir + '/test.csv')\n",
    "\n",
    "    preprocessed_stage_df = pd.read_csv(preprocessed_stage_path)\n",
    "    preprocessed_train_df = pd.read_csv(preprocessed_train_path)\n",
    "    preprocessed_test_df = pd.read_csv(preprocessed_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "36f8ac38-6400-4a4d-8562-7534bf9f077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_MODE:\n",
    "    print(\"##Stage Data Set: ##\")\n",
    "    print(preprocessed_stage_df[['RIC']].value_counts())\n",
    "\n",
    "    print(\"\\n##Train Data Set: ##\")\n",
    "    print(preprocessed_train_df[['ric']].value_counts())\n",
    "\n",
    "    print(\"\\n##Test Data Set: ##\")\n",
    "    print(preprocessed_test_df[['ric']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78703b74-20b2-4d20-b1ec-faa4abb0891d",
   "metadata": {},
   "source": [
    "### (3) 실험: Fold 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "09460437-09e5-4508-874b-a9780f0e66a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>y</th>\n",
       "      <th>ric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-07-02</td>\n",
       "      <td>38.959999</td>\n",
       "      <td>38.480000</td>\n",
       "      <td>38.869999</td>\n",
       "      <td>38.470001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-03</td>\n",
       "      <td>38.660000</td>\n",
       "      <td>38.340000</td>\n",
       "      <td>38.470001</td>\n",
       "      <td>38.560001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-04</td>\n",
       "      <td>38.590000</td>\n",
       "      <td>38.219999</td>\n",
       "      <td>38.495001</td>\n",
       "      <td>38.460001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>38.520000</td>\n",
       "      <td>38.099998</td>\n",
       "      <td>38.520000</td>\n",
       "      <td>38.360001</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-08</td>\n",
       "      <td>38.410000</td>\n",
       "      <td>37.770000</td>\n",
       "      <td>38.410000</td>\n",
       "      <td>37.799999</td>\n",
       "      <td>BOc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219403</th>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>885.250000</td>\n",
       "      <td>863.750000</td>\n",
       "      <td>882.500000</td>\n",
       "      <td>868.500000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219404</th>\n",
       "      <td>2022-10-25</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>857.750000</td>\n",
       "      <td>871.500000</td>\n",
       "      <td>863.250000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219405</th>\n",
       "      <td>2022-10-26</td>\n",
       "      <td>874.750000</td>\n",
       "      <td>854.750000</td>\n",
       "      <td>867.250000</td>\n",
       "      <td>871.500000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219406</th>\n",
       "      <td>2022-10-27</td>\n",
       "      <td>885.750000</td>\n",
       "      <td>863.250000</td>\n",
       "      <td>872.500000</td>\n",
       "      <td>868.000000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219407</th>\n",
       "      <td>2022-10-28</td>\n",
       "      <td>869.250000</td>\n",
       "      <td>853.250000</td>\n",
       "      <td>869.250000</td>\n",
       "      <td>861.250000</td>\n",
       "      <td>Wc3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219408 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ds        high         low        open           y   ric\n",
       "0       2014-07-02   38.959999   38.480000   38.869999   38.470001  BOc1\n",
       "1       2014-07-03   38.660000   38.340000   38.470001   38.560001  BOc1\n",
       "2       2014-07-04   38.590000   38.219999   38.495001   38.460001  BOc1\n",
       "3       2014-07-07   38.520000   38.099998   38.520000   38.360001  BOc1\n",
       "4       2014-07-08   38.410000   37.770000   38.410000   37.799999  BOc1\n",
       "...            ...         ...         ...         ...         ...   ...\n",
       "219403  2022-10-24  885.250000  863.750000  882.500000  868.500000   Wc3\n",
       "219404  2022-10-25  874.000000  857.750000  871.500000  863.250000   Wc3\n",
       "219405  2022-10-26  874.750000  854.750000  867.250000  871.500000   Wc3\n",
       "219406  2022-10-27  885.750000  863.250000  872.500000  868.000000   Wc3\n",
       "219407  2022-10-28  869.250000  853.250000  869.250000  861.250000   Wc3\n",
       "\n",
       "[219408 rows x 6 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ccaf0d94-d1a1-4320-9b95-e0dc735d0004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-09\n",
      "2022-09-09\n",
      "df_train01 = df_train00[df_train00['ds'] < 'split_date]\n",
      "df_valid01 = df_train00[df_train00['ds'] >= split_date]\n",
      "\n",
      "2022-08-10\n",
      "df_train02 = df_train01[df_train01['ds'] < 'split_date]\n",
      "df_valid02 = df_train01[df_train01['ds'] >= split_date]\n",
      "\n",
      "2022-07-11\n",
      "df_train03 = df_train02[df_train02['ds'] < 'split_date]\n",
      "df_valid03 = df_train02[df_train02['ds'] >= split_date]\n",
      "\n",
      "2022-06-11\n",
      "df_train04 = df_train03[df_train03['ds'] < 'split_date]\n",
      "df_valid04 = df_train03[df_train03['ds'] >= split_date]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_date = (dt.now() - relativedelta(days=60)).strftime('%Y-%m-%d') \n",
    "print(split_date)\n",
    "\n",
    "df_train00 = preprocessed_train_df[preprocessed_train_df['ds'] < split_date]\n",
    "df_valid00 = preprocessed_train_df[preprocessed_train_df['ds'] >= split_date]\n",
    "\n",
    "for cnt in range(4):\n",
    "    split_date = (dt.strptime(split_date, '%Y-%m-%d') - relativedelta(days=30)).strftime('%Y-%m-%d')\n",
    "    print(split_date)\n",
    "    print(f\"df_train{str(cnt+1).zfill(2)} = df_train{str(cnt).zfill(2)}[df_train{str(cnt).zfill(2)}['ds'] < 'split_date]\")\n",
    "    exec(f\"df_train{str(cnt+1).zfill(2)} = df_train{str(cnt).zfill(2)}[df_train{str(cnt).zfill(2)}['ds'] < split_date]\")\n",
    "    print(f\"df_valid{str(cnt+1).zfill(2)} = df_train{str(cnt).zfill(2)}[df_train{str(cnt).zfill(2)}['ds'] >= split_date]\")\n",
    "    exec(f\"df_valid{str(cnt+1).zfill(2)} = df_train{str(cnt).zfill(2)}[df_train{str(cnt).zfill(2)}['ds'] >= split_date]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "10d2b6b3-0884-4605-bcc0-3ac3b26d048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ds       high    low       open          y   ric\n",
      "0  2014-07-02  38.959999  38.48  38.869999  38.470001  BOc1\n",
      "1  2014-07-03  38.660000  38.34  38.470001  38.560001  BOc1\n",
      "                ds    high     low    open       y  ric\n",
      "219370  2022-09-07  885.25  824.25  831.25  857.50  Wc3\n",
      "219371  2022-09-08  869.50  833.25  850.75  843.75  Wc3\n",
      "\n",
      "215772\n"
     ]
    }
   ],
   "source": [
    "print(df_train01.head(2))\n",
    "print(df_train01.tail(2))\n",
    "print()\n",
    "print(len(df_train01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5a40c4a6-948b-441e-bd98-e3cc50f978cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ds   high     low    open       y  ric\n",
      "219392  2022-10-07  918.0  895.75  904.75  903.25  Wc3\n",
      "                ds   high     low    open       y  ric\n",
      "219371  2022-09-08  869.5  833.25  850.75  843.75  Wc3\n",
      "                ds   high    low   open      y  ric\n",
      "219349  2022-08-09  838.0  810.5  822.0  816.5  Wc3\n",
      "                ds   high    low   open      y  ric\n",
      "219327  2022-07-08  909.5  854.0  854.0  906.5  Wc3\n",
      "                ds     high      low     open       y  ric\n",
      "219307  2022-06-10  1107.25  1078.75  1099.75  1098.0  Wc3\n",
      "\n",
      "217893\n",
      "215772\n",
      "213550\n",
      "211328\n",
      "209308\n"
     ]
    }
   ],
   "source": [
    "print(df_train00.tail(1))\n",
    "print(df_train01.tail(1))\n",
    "print(df_train02.tail(1))\n",
    "print(df_train03.tail(1))\n",
    "print(df_train04.tail(1))\n",
    "print()\n",
    "print(len(df_train00))\n",
    "print(len(df_train01))\n",
    "print(len(df_train02))\n",
    "print(len(df_train03))\n",
    "print(len(df_train04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c7ebcbff-3d62-4d72-8da1-6f5618b87b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ds       high        low       open          y   ric\n",
      "2158  2022-10-10  69.910004  69.800003  69.800003  69.910004  BOc1\n",
      "              ds  high   low  open      y   ric\n",
      "2137  2022-09-09  70.0  70.0  70.0  70.25  BOc1\n",
      "              ds       high        low       open          y   ric\n",
      "2115  2022-08-10  71.440002  69.809998  69.809998  70.559998  BOc1\n",
      "              ds       high   low       open          y   ric\n",
      "2093  2022-07-11  66.510002  65.0  65.910004  65.209999  BOc1\n",
      "              ds   high        low       open          y   ric\n",
      "2073  2022-06-13  81.07  78.720001  80.919998  79.510002  BOc1\n",
      "\n",
      "1515\n",
      "2121\n",
      "2222\n",
      "2222\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "print(df_valid00.head(1))\n",
    "print(df_valid01.head(1))\n",
    "print(df_valid02.head(1))\n",
    "print(df_valid03.head(1))\n",
    "print(df_valid04.head(1))\n",
    "print()\n",
    "print(len(df_valid00))\n",
    "print(len(df_valid01))\n",
    "print(len(df_valid02))\n",
    "print(len(df_valid03))\n",
    "print(len(df_valid04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "63ade5af-2d3d-455d-8639-2adbb54bbeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOc1      22\n",
       "KLKKKL    22\n",
       "LWBc1     22\n",
       "LCOc3     22\n",
       "LCOc2     22\n",
       "          ..\n",
       "DJCIBR    22\n",
       "DJCI      22\n",
       "CTc3      22\n",
       "CTc2      22\n",
       "Wc3       22\n",
       "Name: ric, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid01['ric'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "94322893-2381-4284-872b-345ffc95a20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOc1      15\n",
       "KLKKKL    15\n",
       "LWBc1     15\n",
       "LCOc3     15\n",
       "LCOc2     15\n",
       "          ..\n",
       "DJCIBR    15\n",
       "DJCI      15\n",
       "CTc3      15\n",
       "CTc2      15\n",
       "Wc3       15\n",
       "Name: ric, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid00['ric'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0b443afe-9bfd-4362-bd14-3880f4f6deb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ds       high        low   open          y   ric\n",
      "0  2022-10-31  74.279999  72.500000  72.50  73.239998  BOc1\n",
      "1  2022-11-01  74.570000  72.739998  73.18  73.290001  BOc1\n",
      "              ds    high     low   open       y  ric\n",
      "2725  2022-12-05  779.75  745.75  775.0  751.75  Wc3\n",
      "2726  2022-12-06  754.50  735.00  751.0  741.75  Wc3\n",
      "\n",
      "2727\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_test_df.head(2))\n",
    "print(preprocessed_test_df.tail(2))\n",
    "print()\n",
    "print(len(preprocessed_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61997838-0504-4f18-b07f-5072591e0698",
   "metadata": {},
   "source": [
    "## 3) 모델 빌딩 파이프라인 의 스텝(Step) 생성\n",
    "### 3.1) 모델 빌딩 파이프라인 변수 생성\n",
    "파이프라인에서 사용할 파이프라인 파라미터를 정의합니다. 파이프라인을 스케줄하고 실행할 때 파라미터를 이용하여 실행조건을 커스마이징할 수 있습니다. 파라미터를 이용하면 파이프라인 실행시마다 매번 파이프라인 정의를 수정하지 않아도 됩니다.\n",
    "\n",
    "지원되는 파라미터 타입은 다음과 같습니다:\n",
    "\n",
    "- ParameterString - 파이썬 타입에서 str\n",
    "- ParameterInteger - 파이썬 타입에서 int\n",
    "- ParameterFloat - 파이썬 타입에서 float\n",
    "이들 파라미터를 정의할 때 디폴트 값을 지정할 수 있으며 파이프라인 실행시 재지정할 수도 있습니다. 지정하는 디폴트 값은 파라미터 타입과 일치하여야 합니다.\n",
    "\n",
    "파이프라인의 각 스텝에서 사용할 변수를 파라미터 변수로서 정의 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639db5a6-5095-4892-b937-df8ebb3094e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name = \"ProcessingInstanceCount\",\n",
    "    default_value = 1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name = \"ProcessingInstanceType\",\n",
    "    default_value = \"ml.m5.xlarge\"\n",
    ")\n",
    "input_stage_data = ParameterString(\n",
    "    name = \"InputStageData\",\n",
    "    default_value = stage_data_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852c37d-bb51-44fd-8773-bb96ec3c8af1",
   "metadata": {},
   "source": [
    "### 3.2) 전처리 스텝 프로세서 정의\n",
    "전처리의 내장 SKLearnProcessor 를 통해서 sklearn_processor 오브젝트를 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bacbb3-08d8-4988-ab34-2f41302781de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version = framework_version,\n",
    "    instance_type = processing_instance_type,\n",
    "    instance_count = processing_instance_count,\n",
    "    base_job_name = \"Palm_oil_forecast-Data_transform\",\n",
    "    role = sagemaker.get_execution_role(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d79514-a6a3-43be-ac1f-95e26d555d62",
   "metadata": {},
   "source": [
    "### 3.3) 전처리 스텝 단계 정의\n",
    "처리 단계에서는 아래와 같은 주요 인자가 있습니다.\n",
    "단계 이름\n",
    "- processor 기술: 위에서 생성한 processor 오브젝트를 제공\n",
    "- inputs: S3의 경로를 기술하고, 다커안에서의 다운로드 폴더(destination)을 기술 합니다.\n",
    "- outputs: 처리 결과가 저장될 다커안에서의 폴더 경로를 기술합니다.\n",
    "\n",
    "도커안의 결과 파일이 저장 후에 자동으로 S3로 업로딩을 합니다.\n",
    "- job_arguments: 사용자 정의의 인자를 기술 합니다.\n",
    "- code: 전처리 코드의 경로를 기술 합니다.\n",
    "처리 단계의 상세한 사항은 여기를 보세요. --> 처리 단계, Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79c775cd-3015-446d-b50a-3a813ee80b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "step_process = ProcessingStep(\n",
    "    name = \"Palm_oil_forecast-Processing\",\n",
    "    processor = sklearn_processor,\n",
    "    inputs = [\n",
    "        ProcessingInput(source = stage_data_uri,\n",
    "                        destination = '/opt/ml/processing/input'),\n",
    "    ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name = \"stage\",\n",
    "                         source = '/opt/ml/processing/output/stage',\n",
    "                         destination = preproc_data_dir),\n",
    "        ProcessingOutput(output_name = \"train\",\n",
    "                         source = '/opt/ml/processing/output/train',\n",
    "                         destination = preproc_data_dir),\n",
    "        ProcessingOutput(output_name = \"test\",\n",
    "                         source = '/opt/ml/processing/output/test',\n",
    "                         destination = preproc_data_dir),\n",
    "    ],\n",
    "    job_arguments=[\"--split_date\", split_date],    \n",
    "    code = preprocessing_code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b0251-deba-4e08-8f1e-2e9471b7f5ee",
   "metadata": {},
   "source": [
    "## 4) 파리마터, 단계, 조건을 조합하여 최종 파이프라인 정의 및 실행\n",
    "이제 지금까지 생성한 단계들을 하나의 파이프라인으로 조합하고 실행하도록 하겠습니다.\n",
    "\n",
    "파이프라인은 name, parameters, steps 속성이 필수적으로 필요합니다. 여기서 파이프라인의 이름은 (account, region) 조합에 대하여 유일(unique))해야 합니다.\n",
    "\n",
    "주의:\n",
    "\n",
    "- 정의에 사용한 모든 파라미터가 존재해야 합니다.\n",
    "- 파이프라인으로 전달된 단계(step)들은 실행순서와는 무관합니다. SageMaker Pipeline은 단계가 실행되고 완료될 수 있도록 의존관계를를 해석합니다.\n",
    "- [알림] 정의한 stpes 이 복수개이면 복수개를 기술합니다. 만약에 step 간에 의존성이 있으면, 명시적으로 기술하지 않아도 같이 실행 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d6940-1468-4aae-b116-02a92ad82c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = project_prefix\n",
    "pipeline = Pipeline(name = pipeline_name,\n",
    "                    parameters = [\n",
    "                        processing_instance_type, \n",
    "                        processing_instance_count,\n",
    "                        input_stage_data,\n",
    "                    ],\n",
    "                    steps = [step_process],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2205be8-13bf-439d-acbc-d4b15d0fbfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'InputStageData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://palm-oil-price-forecast/staged-data'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'Palm_oil_forecast-Processing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--split_date',\n",
       "      '2022-10-31',\n",
       "      'scaler_switch',\n",
       "      '1'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::276114397529:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://palm-oil-price-forecast/staged-data',\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-276114397529/Palm_oil_forecast-Processing-d7269674adfab91d0a2056a6dd5218e6/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'stage',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/07',\n",
       "        'LocalPath': '/opt/ml/processing/output/stage',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/07',\n",
       "        'LocalPath': '/opt/ml/processing/output/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://palm-oil-price-forecast/golden-data/2022/12/07',\n",
       "        'LocalPath': '/opt/ml/processing/output/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b1a5d-b571-4309-a8a0-17502b611842",
   "metadata": {},
   "outputs": [
    {
     "ename": "WaiterError",
     "evalue": "Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaiterError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, delay, max_attempts)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mwaiter_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         )\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPipelineExecutionArn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mWaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     wait.__doc__ = WaiterDocstring(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0macceptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 )\n\u001b[0;32m--> 375\u001b[0;31m                 raise WaiterError(\n\u001b[0m\u001b[1;32m    376\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                     \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWaiterError\u001b[0m: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "\n",
    "pipeline.upsert(role_arn = sagemaker.get_execution_role())\n",
    "execution = pipeline.start()\n",
    "execution.wait() \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c20e1ee-49c8-4371-b529-afb56e4caa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing 시간 : 935.4 sec\n",
      "preprocessing 시간 : 15.6 min\n"
     ]
    }
   ],
   "source": [
    "print(f\"preprocessing 시간 : {end - start:.1f} sec\")\n",
    "print(f\"preprocessing 시간 : {((end - start)/60):.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b25250c7-6936-4bdb-ae6e-8f7fa88da6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-northeast-2:276114397529:pipeline/palm-oil-price-forecast',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-northeast-2:276114397529:pipeline/palm-oil-price-forecast/execution/otrsacpxffbz',\n",
       " 'PipelineExecutionDisplayName': 'execution-1670401588923',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'palm-oil-price-forecast',\n",
       "  'TrialName': 'otrsacpxffbz'},\n",
       " 'CreationTime': datetime.datetime(2022, 12, 7, 8, 26, 28, 861000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 12, 7, 8, 41, 52, 799000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '89db4cad-ed61-4aa1-aa8c-7229a8a20028',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '89db4cad-ed61-4aa1-aa8c-7229a8a20028',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '520',\n",
       "   'date': 'Wed, 07 Dec 2022 08:42:02 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df10ebf-17da-49f2-8ab9-df0a92dc004c",
   "metadata": {},
   "source": [
    "# 3. 전처리 결과파일 경로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3bc4f9d-15f0-4a8a-b886-d064ac5ada28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proc_artifact(execution, client, kind=0):\n",
    "    '''\n",
    "    kind: 0 --> stage\n",
    "    kind: 1 --> train\n",
    "    kind: 2 --> test\n",
    "    '''\n",
    "    response = execution.list_steps()\n",
    "    proc_arn = response[-1]['Metadata']['ProcessingJob']['Arn'] # index -1은 가장 처음 실행 step\n",
    "    proc_job_name = proc_arn.split('/')[-1] # Processing job name만 추출\n",
    "    response = client.describe_processing_job(ProcessingJobName = proc_job_name)\n",
    "    file_uri = response['ProcessingOutputConfig']['Outputs'][kind]['S3Output']['S3Uri']\n",
    "    return file_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ff3d648-51cd-4e07-a2a7-9772610cb87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_preproc_dir_artifact:  s3://palm-oil-price-forecast/golden-data/2022/12/07/stage.csv\n",
      "\n",
      "train_preproc_dir_artifact:  s3://palm-oil-price-forecast/golden-data/2022/12/07/train.csv\n",
      "\n",
      "test_preproc__dir_artifact:  s3://palm-oil-price-forecast/golden-data/2022/12/07/test.csv\n"
     ]
    }
   ],
   "source": [
    "preprocessed_stage_uri = get_proc_artifact(execution, sm_client, kind=0) + '/stage.csv'\n",
    "preprocessed_train_uri = get_proc_artifact(execution, sm_client, kind=1) + '/train.csv'\n",
    "preprocessed_test_uri = get_proc_artifact(execution, sm_client, kind=2) + '/test.csv'\n",
    "\n",
    "print(\"train_preproc_dir_artifact: \", preprocessed_stage_uri)\n",
    "print(\"\\ntrain_preproc_dir_artifact: \", preprocessed_train_uri)\n",
    "print(\"\\ntest_preproc__dir_artifact: \", preprocessed_test_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "575baade-e135-4ffc-b973-69df7b7f6ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'preprocessed_stage_uri' (str)\n",
      "Stored 'preprocessed_train_uri' (str)\n",
      "Stored 'preprocessed_test_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store preprocessed_stage_uri\n",
    "%store preprocessed_train_uri\n",
    "%store preprocessed_test_uri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
